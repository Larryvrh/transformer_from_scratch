{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import TRIETokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "from typing import *\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from flash_attn import flash_attn_func\n",
    "from flash_attn_triton import flash_attn_func as flash_attn_func_triton\n",
    "from dataloader import DatasetReader, DatasetIter, SingleDatasetReader, MultiDatasetsReader\n",
    "from math import ceil\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import bitsandbytes as bnb\n",
    "from threading import Lock, Thread\n",
    "import traceback\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from enum import Enum\n",
    "from modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Disabled due to no obvious speed up\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d09b9a4246f3aae2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33702a8e6613f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_tokenizer = TRIETokenizer('llama_vocab_pruned_32k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb151752ec125fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network definition for 370M network\n",
    "# C_SEQ_LEN = 1024\n",
    "# C_HIDDEN_SIZE = 1024\n",
    "# C_NUM_HEADS = 16\n",
    "# C_NUM_LAYERS = 24\n",
    "\n",
    "# Network definition for 135M network\n",
    "C_SEQ_LEN = 1024\n",
    "C_HIDDEN_SIZE = 768\n",
    "C_NUM_HEADS = 12\n",
    "C_NUM_LAYERS = 12\n",
    "\n",
    "C_DEVICE = torch.device('cuda')\n",
    "C_DTYPE = torch.float32\n",
    "\n",
    "C_DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b78921dbff1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not C_DEBUG:\n",
    "    # g_train_data = MultiDatasetsReader([\n",
    "    #     SingleDatasetReader('datasets/minipile_train_masked_1024.bin'),\n",
    "    #     SingleDatasetReader('datasets/enwiki_train_masked_1024.bin'),\n",
    "    #     SingleDatasetReader('datasets/tinytextbooks_train_masked_1024.bin'),\n",
    "    # ], seed=0)\n",
    "\n",
    "    # g_train_data = MultiDatasetsReader([\n",
    "    #     SingleDatasetReader('datasets/tinystories_train_masked.bin'),\n",
    "    # ], seed=0)\n",
    "\n",
    "    # g_train_data = MultiDatasetsReader([\n",
    "    #     SingleDatasetReader('datasets/slimpajamas/slimpajama_train_masked_1024.bin'),\n",
    "    # ], seed=0)\n",
    "\n",
    "    g_train_data = MultiDatasetsReader([\n",
    "        SingleDatasetReader('datasets/sft/alpaca_gpt4.bin'),\n",
    "        SingleDatasetReader('datasets/sft/wizardlm_evol_2.bin'),\n",
    "        SingleDatasetReader('datasets/sft/airoboros_2.2.1.bin')\n",
    "    ], seed=0)\n",
    "else:\n",
    "    g_train_data = MultiDatasetsReader([\n",
    "        SingleDatasetReader('datasets/debug_data_masked.bin'),\n",
    "        SingleDatasetReader('datasets/debug_data_masked.bin'),\n",
    "    ], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d770a2be845a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train samples:', len(g_train_data))\n",
    "print('Sample length:', len(next(iter(g_train_data))['token_ids']))\n",
    "print('Train tokens:', len(g_train_data) * len(next(iter(g_train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8dbb96cab10318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample 1:', g_tokenizer.decode(next(iter(g_train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124589733f9ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    g_model = ToyTransformer(g_tokenizer.get_vocab_size(), 2, 2, 256, 1024, C_DEVICE, C_DTYPE)\n",
    "else:\n",
    "    g_model = ToyTransformer(g_tokenizer.get_vocab_size(), C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN, C_DEVICE, C_DTYPE)\n",
    "    # g_model = ToyTransformer(g_tokenizer.get_vocab_size(), 4, 8, 512, C_SEQ_LEN, C_DEVICE, C_DTYPE) # 46M model for tiny stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Total parameters:', sum([t.numel() for t in g_model.parameters()]))\n",
    "print(g_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f906aed5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_memory = 0\n",
    "for p in g_model.parameters():\n",
    "    total_memory += (p.numel() * p.element_size())\n",
    "print(f'Model memory usage: {total_memory / 1024 / 1024:.2f}MB')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73a1c1c4b301c9cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dca6004e9b65892c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dataset_collate(dataset_iter: DatasetIter, batch_size: int,\n",
    "                    transform: Optional[Callable[[Dict[str, List[np.ndarray]]], Dict[str, torch.Tensor]]] = None,\n",
    "                    drop_last: bool = False):\n",
    "    cur_batch, cur_batch_size = {}, 0\n",
    "    for entry in dataset_iter:\n",
    "        for k, v in entry.items():\n",
    "            cur_batch.setdefault(k, [])\n",
    "            cur_batch[k].append(v)\n",
    "        cur_batch_size += 1\n",
    "        if cur_batch_size == batch_size:\n",
    "            yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)\n",
    "            cur_batch = {}\n",
    "            cur_batch_size = 0\n",
    "    if not drop_last and len(cur_batch) > 0:\n",
    "        yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "674478c0ccb206af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainArguments:\n",
    "    num_epochs: int\n",
    "    batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "\n",
    "    optimizer: Type[torch.optim.Optimizer]\n",
    "    optimizer_args: Optional[Dict[str, Any]]\n",
    "    mixed_precision_dtype: torch.dtype\n",
    "\n",
    "    start_lr: float\n",
    "    max_lr: float\n",
    "    end_lr: float\n",
    "    warmup_ratio: float\n",
    "\n",
    "    gradient_clip_norm: Optional[float]\n",
    "\n",
    "    train_data: DatasetReader\n",
    "    ignore_attn_mask: bool\n",
    "    ignore_loss_mask: bool\n",
    "\n",
    "    # eval_data: Optional[DatasetReader]\n",
    "    # eval_steps: int\n",
    "    # \n",
    "    # eval_generate_prompt: Optional[str]\n",
    "    # eval_generate_steps: int\n",
    "\n",
    "    save_steps: int\n",
    "    save_on_interrupt: bool\n",
    "\n",
    "\n",
    "# type cast for handling int16/uint16 columns\n",
    "def train_transform(batch: Dict[str, List[np.ndarray]]):\n",
    "    return {k: torch.tensor(np.stack(v, dtype=np.int32 if v[0].dtype in [np.int16, np.uint16] else v[0].dtype)) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, model: ToyTransformer,\n",
    "                    optimizer: torch.optim.Optimizer, lr_scheduler: torch.optim.lr_scheduler.LRScheduler, grad_scaler: Optional[torch.cuda.amp.GradScaler],\n",
    "                    train_args: TrainArguments,\n",
    "                    dataset: DatasetReader, dataset_iter: DatasetIter, train_logs: List, misc: Dict):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path + '/model.pt')\n",
    "    torch.save(optimizer.state_dict(), path + '/optimizer.pt')\n",
    "    torch.save(lr_scheduler.state_dict(), path + '/lr_scheduler.pt')\n",
    "    if grad_scaler is not None:\n",
    "        torch.save(lr_scheduler.state_dict(), path + '/grad_scaler.pt')\n",
    "    torch.save(torch.get_rng_state(), path + '/rng_state.pt')\n",
    "    torch.save(train_args, path + '/train_args.pt')\n",
    "    dataset.save_iterator(dataset_iter, path + '/dataset_iter.pt')\n",
    "    torch.save(train_logs, path + '/train_logs.pt')\n",
    "    torch.save(misc, path + '/misc.pt')\n",
    "    with open(path + '/config.txt', 'w') as file:\n",
    "        file.write(f'Dataset: {str(dataset)}')\n",
    "        file.write('\\n\\n')\n",
    "        file.write(f'Model Config: {str(model.config)}')\n",
    "        file.write('\\n\\n')\n",
    "        file.write(f'Training Arguments: {str(train_args)}')\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str, model: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer, lr_scheduler: torch.optim.lr_scheduler.LRScheduler, grad_scaler: Optional[torch.cuda.amp.GradScaler],\n",
    "                    train_args: TrainArguments,\n",
    "                    dataset: DatasetReader, dataset_iter: DatasetIter, train_logs: List, misc: Dict):\n",
    "    model.load_state_dict(torch.load(path + '/model.pt'))\n",
    "    optimizer.load_state_dict(torch.load(path + '/optimizer.pt'))\n",
    "    lr_scheduler.load_state_dict(torch.load(path + '/lr_scheduler.pt'))\n",
    "    if grad_scaler is not None:\n",
    "        grad_scaler.load_state_dict(torch.load(path + '/grad_scaler.pt'))\n",
    "    torch.set_rng_state(torch.load(path + '/rng_state.pt'))\n",
    "    # assert torch.load(path + '/train_args.pt') == train_args\n",
    "    dataset_iter.set_state(dataset.load_iterator(path + '/dataset_iter.pt').get_state())\n",
    "    train_logs.clear()\n",
    "    train_logs += torch.load(path + '/train_logs.pt')\n",
    "    misc.update(torch.load(path + '/misc.pt'))\n",
    "\n",
    "\n",
    "def train_model(model: ToyTransformer, train_args: TrainArguments,\n",
    "                resume_from: Optional[str] = None,\n",
    "                show_progress: bool = True,\n",
    "                output_dir: str = 'checkpoints', interrupt_lock: Optional[Lock] = None):\n",
    "    output_dir = output_dir.rstrip('/')\n",
    "\n",
    "    interrupted = False\n",
    "    train_logs = []\n",
    "    misc = {'epochs': 0, 'steps': 0, 'last_batch_idx': -1}\n",
    "\n",
    "    total_samples = len(train_args.train_data)\n",
    "    epoch_steps = ceil(total_samples / train_args.batch_size)\n",
    "    assert epoch_steps >= train_args.gradient_accumulation_steps, \\\n",
    "        f'per-epoch steps {epoch_steps} is less than gradient accumulation steps {train_args.gradient_accumulation_steps}'\n",
    "\n",
    "    schedule_steps = ceil(total_samples / train_args.batch_size / train_args.gradient_accumulation_steps)\n",
    "    total_steps = schedule_steps * train_args.num_epochs\n",
    "\n",
    "    optimizer = train_args.optimizer(model.parameters(), **(train_args.optimizer_args if train_args.optimizer_args is not None else {}))\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=train_args.max_lr, div_factor=train_args.max_lr / train_args.start_lr,\n",
    "                                                    total_steps=total_steps,\n",
    "                                                    final_div_factor=train_args.start_lr / train_args.end_lr, pct_start=train_args.warmup_ratio)\n",
    "\n",
    "    if train_args.mixed_precision_dtype == torch.float16:\n",
    "        grad_scaler = torch.cuda.amp.GradScaler()\n",
    "    else:\n",
    "        grad_scaler = None\n",
    "\n",
    "    dataset_iter = iter(train_args.train_data)\n",
    "    if resume_from is not None:\n",
    "        load_checkpoint(resume_from, model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "\n",
    "    bar = tqdm.tqdm(total=total_steps, smoothing=1.0, disable=not show_progress)\n",
    "    bar.update(misc['steps'])\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch_num in range(train_args.num_epochs):\n",
    "        if epoch_num < misc['epochs']:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, batch in enumerate(dataset_collate(dataset_iter, train_args.batch_size, train_transform), start=misc['last_batch_idx'] + 1):\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            tokens = batch['token_ids'].to(model.device)\n",
    "            inputs = tokens[:, :-1]\n",
    "            labels = tokens[:, 1:]\n",
    "\n",
    "            attn_mask = batch['attn_mask'][:, :-1].to(model.device) if 'attn_mask' in batch and not train_args.ignore_attn_mask else None\n",
    "            loss_mask = batch['loss_mask'][:, 1:].to(model.device) if 'loss_mask' in batch and not train_args.ignore_loss_mask else None\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=train_args.mixed_precision_dtype, enabled=train_args.mixed_precision_dtype is not None):\n",
    "                logits, kv_state = model.forward(inputs, attn_mask=attn_mask)\n",
    "\n",
    "                probs = torch.softmax(logits, dim=2).view(-1, logits.shape[-1])\n",
    "\n",
    "                loss = (-torch.log(probs[torch.arange(probs.shape[0]), labels.reshape(-1)]))\n",
    "                if loss_mask is not None:\n",
    "                    loss = (loss * loss_mask.reshape(-1)).mean() / train_args.gradient_accumulation_steps\n",
    "                else:\n",
    "                    loss = loss.mean() / train_args.gradient_accumulation_steps\n",
    "\n",
    "            # brutally clear nan, give up the whole batch\n",
    "            if torch.isnan(loss):\n",
    "                print(f'encountered nan loss at epoch {epoch_num + 1}, batch {batch_idx}')\n",
    "            else:\n",
    "                if grad_scaler is not None:\n",
    "                    grad_scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % train_args.gradient_accumulation_steps == 0 or (batch_idx + 1) == epoch_steps:\n",
    "                if grad_scaler is not None:\n",
    "                    grad_scaler.unscale_(optimizer)\n",
    "\n",
    "                if train_args.gradient_clip_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), train_args.gradient_clip_norm)\n",
    "\n",
    "                if grad_scaler is not None:\n",
    "                    grad_scaler.step(optimizer)\n",
    "                    grad_scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                step_time_cost = time.time() - step_start_time\n",
    "                throughput = round(probs.shape[0] / step_time_cost / 1000, 2)\n",
    "\n",
    "                step_stat = {'Loss': f'{loss.item() * train_args.gradient_accumulation_steps:.3f}',\n",
    "                             'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                             'Throughput': f'{throughput} kt/s'}\n",
    "\n",
    "                if show_progress:\n",
    "                    bar.set_description(f'Epoch {epoch_num + 1}')\n",
    "                    bar.set_postfix(step_stat)\n",
    "                else:\n",
    "                    print(', '.join(f'{s[0]}:{s[1]}' for s in step_stat.items()))\n",
    "\n",
    "                scheduler.step()\n",
    "                bar.update(1)\n",
    "                train_logs.append((epoch_num, batch_idx, step_stat))\n",
    "\n",
    "                misc['steps'] += 1\n",
    "                misc['last_batch_idx'] = batch_idx\n",
    "                if train_args.save_steps > 0 and (misc['steps'] % train_args.save_steps) == 0:\n",
    "                    save_checkpoint(output_dir + f'/checkpoint-{misc[\"steps\"]}',\n",
    "                                    model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "                if interrupt_lock is not None and not interrupt_lock.locked():\n",
    "                    if train_args.save_on_interrupt:\n",
    "                        save_checkpoint(output_dir + f'/checkpoint-{misc[\"steps\"]}',\n",
    "                                        model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "                    interrupted = True\n",
    "                    break\n",
    "        if interrupted:\n",
    "            break\n",
    "        misc['epochs'] += 1\n",
    "        misc['last_batch_idx'] = -1\n",
    "        dataset_iter = iter(train_args.train_data)\n",
    "    bar.close()\n",
    "\n",
    "    if not interrupted:\n",
    "        save_checkpoint(output_dir + f'/checkpoint-done',\n",
    "                        model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "\n",
    "    return train_logs\n",
    "\n",
    "\n",
    "def train_model_interruptable(model: nn.Module, train_args: TrainArguments,\n",
    "                              resume_from: Optional[str] = None,\n",
    "                              show_progress: bool = True,\n",
    "                              output_dir: str = 'checkpoints'):\n",
    "    return_value, run_finish = None, False\n",
    "\n",
    "    def return_value_wrapper(func, *args, **kwargs):\n",
    "        nonlocal return_value, run_finish\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            return_value = func(*args, **kwargs)\n",
    "        except Exception as _:\n",
    "            traceback.print_exc()\n",
    "        run_finish = True\n",
    "\n",
    "    interrupt_lock = Lock()\n",
    "    interrupt_lock.acquire()\n",
    "    thread = Thread(target=return_value_wrapper, args=(train_model, model, train_args),\n",
    "                    kwargs={'resume_from': resume_from, 'show_progress': show_progress, 'output_dir': output_dir, 'interrupt_lock': interrupt_lock})\n",
    "    thread.start()\n",
    "    while not run_finish:\n",
    "        try:\n",
    "            time.sleep(0.1)\n",
    "        except KeyboardInterrupt as _:\n",
    "            interrupt_lock.release()\n",
    "            break\n",
    "    thread.join()\n",
    "\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e39ed7414acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    g_train_args = TrainArguments(\n",
    "        num_epochs=1000, batch_size=8, gradient_accumulation_steps=1,\n",
    "        optimizer=torch.optim.AdamW, optimizer_args=None,\n",
    "        mixed_precision_dtype=torch.bfloat16,\n",
    "        start_lr=1e-5, max_lr=1e-3, end_lr=1e-6, warmup_ratio=0.1,\n",
    "        gradient_clip_norm=1.0,\n",
    "        train_data=g_train_data, ignore_attn_mask=False, ignore_loss_mask=False,\n",
    "        # eval_data=None, eval_steps=-1,\n",
    "        # eval_generate_prompt=None, eval_generate_steps=-1,\n",
    "        save_steps=-1,\n",
    "        save_on_interrupt=False,\n",
    "    )\n",
    "else:\n",
    "    g_train_args = TrainArguments(\n",
    "        num_epochs=2, batch_size=12, gradient_accumulation_steps=8,\n",
    "        optimizer=torch.optim.AdamW, optimizer_args=None,\n",
    "        mixed_precision_dtype=torch.bfloat16,\n",
    "        start_lr=5e-5, max_lr=1e-3, end_lr=1e-4, warmup_ratio=0.1,\n",
    "        gradient_clip_norm=0.7,\n",
    "        train_data=g_train_data, ignore_attn_mask=True, ignore_loss_mask=True,\n",
    "        # eval_data=None, eval_steps=-1,\n",
    "        # eval_generate_prompt=None, eval_generate_steps=-1,\n",
    "        save_steps=1000,\n",
    "        save_on_interrupt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673871354eec2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    global_config['attn_backend'] = AttentionBackend.FlashAttentionTriton\n",
    "    g_train_args.ignore_attn_mask = True\n",
    "    g_train_logs = train_model_interruptable(g_model, g_train_args, resume_from=None,\n",
    "                                             show_progress=True, output_dir='checkpoints/debug_output')\n",
    "else:\n",
    "    g_model.load_state_dict(torch.load('checkpoints/train-round1-135M/checkpoint-19000/model.pt'))\n",
    "    global_config['attn_backend'] = AttentionBackend.FlashAttentionTriton\n",
    "    g_train_args.ignore_attn_mask = False\n",
    "    g_train_args.ignore_loss_mask = False\n",
    "    g_train_args.num_epochs = 3\n",
    "    g_train_args.save_steps = 500\n",
    "    g_train_args.warmup_ratio = 0.1\n",
    "    g_train_args.batch_size = 10\n",
    "    g_train_args.gradient_accumulation_steps = 4\n",
    "    g_train_logs = train_model_interruptable(g_model, g_train_args, resume_from='checkpoints/train-round1-135M-sft/checkpoint-232',\n",
    "                                             show_progress=True, output_dir='checkpoints/train-round1-135M-sft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ca07cd31cd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_logs(train_logs_list):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    for i, train_logs in enumerate(train_logs_list):\n",
    "        axes[0].plot([float(l[2]['Loss']) for l in train_logs])\n",
    "        axes[1].plot([float(l[2]['LR']) for l in train_logs])\n",
    "        axes[2].plot([float(l[2]['Throughput'][:-5]) for l in train_logs])\n",
    "\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[1].set_title('Learning Rate')\n",
    "    axes[2].set_title('Throughput (kt/s)')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.autoscale()\n",
    "\n",
    "    #plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_list = [\n",
    "    'checkpoints/train-round1/checkpoint-19000/train_logs.pt',\n",
    "    'checkpoints/train-round1-masked/checkpoint-1199/train_logs.pt'\n",
    "]\n",
    "logs_list = [torch.load(c) for c in checkpoint_list]\n",
    "\n",
    "plot_train_logs(logs_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "471f560981b69d48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e800403549c98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, temperature, top_p, rep_penalty,\n",
    "             max_new_tokens=20, total_tokens=None,\n",
    "             end_tokens=None,\n",
    "             enable_kv_cache=True):\n",
    "    model.eval()\n",
    "\n",
    "    feed_tokens = tokenizer.encode(prompt) if isinstance(prompt, str) else prompt\n",
    "    all_tokens = feed_tokens.copy()\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(feed_tokens))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, kv_cache = model.forward(\n",
    "                torch.tensor([feed_tokens if enable_kv_cache else all_tokens]).to(model.device),\n",
    "                kv_cache=kv_cache)\n",
    "            logits = logits[0][-1].cpu()\n",
    "            if not enable_kv_cache:\n",
    "                kv_cache = None\n",
    "\n",
    "            # apply repetition penalty\n",
    "            logits_rep = torch.gather(logits, 0, torch.tensor(all_tokens))\n",
    "            logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "            logits.scatter_(0, torch.tensor(all_tokens), logits_rep)\n",
    "\n",
    "            # apply temperature\n",
    "            logits /= max(temperature, 1e-6)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # apply top-p\n",
    "            ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "            cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "            top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "            ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "            sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "            all_tokens.append(sampled_index)\n",
    "            feed_tokens = [sampled_index]\n",
    "\n",
    "            if end_tokens is not None and sampled_index in end_tokens:\n",
    "                break\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def modeling_sanity_check(gen_length: int, enable_kv_cache: bool):\n",
    "    assert C_DEBUG == True, 'sanity check can only be performed under debug settings'\n",
    "    train_token_ids = next(iter(g_train_data))['token_ids'].tolist()\n",
    "    train_texts = [l.strip() for l in g_tokenizer.decode(train_token_ids).split('</s>')]\n",
    "    start_time = time.time()\n",
    "    gen_token_ids = generate(g_model, g_tokenizer, train_token_ids[:10],\n",
    "                             temperature=1.0, top_p=0.01, rep_penalty=1.0,\n",
    "                             total_tokens=gen_length,\n",
    "                             end_tokens=g_tokenizer.encode('<reserved_0>'),\n",
    "                             enable_kv_cache=enable_kv_cache)\n",
    "    cost_time = time.time() - start_time\n",
    "    print(f'Generation finished in {cost_time:.2f} sec(s), throughput: {len(gen_token_ids) / cost_time:.1f} tokens/sec')\n",
    "    # Complete check\n",
    "    cmp_length = min(len(train_token_ids), len(gen_token_ids))\n",
    "    print('Complete Identical:', train_token_ids[:cmp_length] == gen_token_ids[:cmp_length])\n",
    "    # Segment check\n",
    "    gen_texts = [l.strip() for l in g_tokenizer.decode(gen_token_ids).split('</s>')]\n",
    "    for i in range(min(len(train_texts), len(gen_texts))):\n",
    "        ref, real = train_texts[i], gen_texts[i]\n",
    "        cmp_length = min(len(ref), len(real))\n",
    "        print(f'Segment {i}: Ref Len: {len(ref)}, Gen Len: {len(real)} Identical: {ref[:cmp_length] == real[:cmp_length]}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93a3628aacbd1f80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for backend_option in [AttentionBackend.Naive, AttentionBackend.FlashAttentionTriton, AttentionBackend.FlashAttentionCuda]:\n",
    "    for enable_kv_cache_option in [True, False]:\n",
    "        print(f'Backend = {backend_option}, KVCache Enable = {enable_kv_cache_option}')\n",
    "        global_config['attn_backend'] = backend_option\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=True):\n",
    "            modeling_sanity_check(512, enable_kv_cache_option)\n",
    "        print('=' * 80)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ffe9294382f18c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c17f13926df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in g_tokenizer.decode(next(iter(g_train_data))['token_ids'].tolist()).split('</s>')[:3]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc924753b5acfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "global_config['attn_backend'] = AttentionBackend.FlashAttentionCuda\n",
    "result = generate(g_model, g_tokenizer, '<s>A chat between User and Assistant.\\nUser:Where is the capital of France?\\nAssistant:',\n",
    "                  temperature=1.0, top_p=0.01, rep_penalty=1.1,\n",
    "                  total_tokens=128,\n",
    "                  end_tokens=g_tokenizer.encode('</s>'),\n",
    "                  enable_kv_cache=True)\n",
    "time_cost = time.time() - time_start\n",
    "\n",
    "print(g_tokenizer.decode(result))\n",
    "print(f'{time_cost:.3f} sec(s), throughput {len(result) / time_cost:.1f} tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "625f35292e9bde9b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

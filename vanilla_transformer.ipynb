{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:20.178406372Z",
     "start_time": "2023-10-24T01:58:18.272391208Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import TRIETokenizerFast\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "from typing import *\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from flash_attn import flash_attn_func\n",
    "from dataloader import DatasetReader, MultiDatasetsReader\n",
    "from math import ceil\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33702a8e6613f742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:28.366032542Z",
     "start_time": "2023-10-24T01:58:20.179144814Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = TRIETokenizerFast('llama_vocab_pruned_32k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb151752ec125fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:28.381939546Z",
     "start_time": "2023-10-24T01:58:28.366704559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network definition\n",
    "C_SEQ_LEN = 2048\n",
    "C_HIDDEN_SIZE = 768\n",
    "C_NUM_HEADS = 12\n",
    "C_NUM_LAYERS = 12\n",
    "\n",
    "C_DEVICE = torch.device('cuda')\n",
    "C_DTYPE = torch.bfloat16\n",
    "\n",
    "C_DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799b78921dbff1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:28.410112999Z",
     "start_time": "2023-10-24T01:58:28.368993801Z"
    }
   },
   "outputs": [],
   "source": [
    "if not C_DEBUG:\n",
    "    train_data = DatasetReader('datasets/tinystories_train_masked.bin')\n",
    "else:\n",
    "    train_data = MultiDatasetsReader([\n",
    "        DatasetReader('datasets/debug_data_masked.bin'),\n",
    "        DatasetReader('datasets/debug_data_masked.bin'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d770a2be845a70d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:28.410337544Z",
     "start_time": "2023-10-24T01:58:28.388939535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 287582\n",
      "Sample length: 2048\n",
      "Train tokens: 588967936\n"
     ]
    }
   ],
   "source": [
    "print('Train samples:', len(train_data))\n",
    "print('Sample length:', len(next(iter(train_data))['token_ids']))\n",
    "print('Train tokens:', len(train_data) * len(next(iter(train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8dbb96cab10318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:28.480622393Z",
     "start_time": "2023-10-24T01:58:28.398195112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: <s>Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
      "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
      "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
      "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
      "And that's how Ben found an amazing vase in the store!</s><s>Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
      "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his friend, the duck. \"Hi, Ollie!\" said the duck. \"Hi, duck!\" said Ollie. \"I need to hurry and catch fish for my family.\"\n",
      "While Ollie was catching fish, he found a big shiny stone. He thought, \"This is not a fish, but it is so pretty!\" Ollie took the shiny stone home to show his family. They all looked at the shiny stone and smiled. The shiny stone made everyone happy, and they forgot about the fish for dinner.</s><s>One day, a little boy named Tim went to the park. He saw a big tiger. The tiger was not mean, but very easy to play with. Tim and the tiger played all day. They had lots of fun.\n",
      "Then, something unexpected happened. The tiger started to shake. Tim was scared. He did not know what was going on. But then, the tiger turned into a nice dog. Tim was very surprised.\n",
      "Tim and the dog played together now. They were very happy. The dog was easy to play with too. At the end of the day, Tim went home with his new friend.</s><s>Once upon a time there was a friendly little boy called Bob. Bob loved to pick flowers and look for birds. One day he decided to go outside with his friends to pick some more flowers. \n",
      "He suddenly noticed something weird on the ground. It was a big, green thumb! It was so big, Bob had never seen one before. Bob curiously leaned in to take a better look. He told his friends: \"look everyone, I picked up this big thumb! What do we do with it?\"\n",
      "His friends were very excited. They told him to pick it up and take it home to show his family. So Bob carefully picked up the friendly thumb and carried it back home. When he arrived, Bob happily showed the thumb to his family. His dad was amazed and hugged Bob to show his appreciation.\n",
      "From that day on Bob always kept the big, friendly thumb with him as a reminder that special things can be found anywhere.</s><s>Once upon a time, in a small house, there lived a little girl named Lucy. Lucy loved the color orange. She had an orange dress, an orange ball, and even an orange cat. One day, Lucy met a new friend. This friend was not like other friends. It was a spirit. The spirit was very nice and liked to play with Lucy.\n",
      "One day, Lucy and the spirit were playing with her orange ball. They were having so much fun. Then, Lucy's mom called her for dinner. Lucy said to the spirit, \"I have to go eat now. Will you play with me later?\" The spirit nodded and smiled.\n",
      "At dinner, Lucy told her mom about the spirit. But her mom did not believe her. She said, \"Spirits are not real, Lucy. You have a big imagination.\" Lucy felt sad that her mom did not believe her. After dinner, she went back to play with the spirit. They played with the orange ball and had lots of fun. Lucy knew that even if others ignore her friend, the spirit was real and they could play together.</s><s>One day, a boy named Tim went to the park to play. He saw his friend, Sam, playing with a toy car. Tim wanted to join and play with Sam. They both played with the toy car, and it went fast. The car had a battery inside that made it go.\n",
      "Tim said, \"Sam, the battery is tight in the car. It will not fall out.\" Sam smiled and they kept playing. They raced the car around the park, laughing and having fun.\n",
      "But then, something unexpected happened. A big dog came and took the toy car in its mouth! Tim and Sam were scared, but the dog just wanted to play too. They all played together, and the dog was very gentle with the car. In the end, Tim, Sam, and the dog became good friends.</s><s>Once upon a time, there was a sailor named Tom. Tom had a big boat. He liked to sail on the sea. One day, Tom saw a little fish. The fish was sad. It was lost and wanted to go home.\n",
      "Tom said, \"I can help you, little fish. You can fit in my boat, and I will take you home.\" The little fish was happy. It jumped into Tom's boat. They sailed together on the sea.\n",
      "The sea was safe and calm. Tom and the little fish talked and laughed. They became good friends. At last, they found the fish's home. The little fish said, \"Thank you, Tom, for helping me.\" Tom smiled and waved goodbye. He sailed away, knowing he had a new friend.</s><s>Sara and Ben wanted to decorate a bowl for their mom. They found a big bowl in the kitchen and some paint and brushes. They took the bowl and the paint to the backyard and put them on a table.\n",
      "\"Let's make the bowl pretty with colors,\" Sara said.\n",
      "\"OK, I will paint a flower,\" Ben said.\n",
      "They started to paint the bowl with different colors. Sara painted a red heart and Ben painted a yellow flower. They were having fun.\n",
      "But then, it started to rain. The rain was wet and cold. It made the paint run and drip. The bowl looked messy and ugly.\n",
      "\"Oh no, the rain ruined our bowl!\" Sara cried.\n",
      "\"Mom will not like it,\" Ben said.\n",
      "They ran inside the house with the bowl. They were sad and wet.\n",
      "They showed the bowl to their mom. They said they were sorry.\n",
      "But mom smiled and hugged them. She said she loved the bowl and them.\n",
      "\"It's a beautiful bowl,\" she said. \"You made it with love and creativity. The rain made it special. It's like a rainbow bowl.\"\n",
      "Sara and Ben felt happy. They gave mom a kiss and thanked her. They learned that sometimes, things can be good even when they seem bad.</s><s>Once upon a time, in a small town, there was a deaf boy named Tom. Tom could not hear, but he was very good at seeing and feeling things. He had many friends who loved to play with him. They would play games like tag and hide-and-seek. Tom was very fast and good at finding his friends.\n",
      "One day, the town bell would ring at noon. Everyone was excited to hear the bell, but Tom knew he could not hear it. He felt sad, but his friends had a plan. They decided to help Tom feel the bell ring too. They took Tom to the big bell and let him touch it while it rang. When the bell rang, Tom could feel the strong vibrations and he smiled.\n",
      "The moral of the story is that even if someone is different or has a problem, like Tom being deaf, we can still find ways to help them and make them happy. We should always be kind and caring to everyone, no matter how different they are.</s><s>Tom and Lily were twins who liked to play in the park. One day, they saw a large maze made of green bushes. They wanted to try it and find the way out.\n",
      "They ran inside the maze and followed the paths. Sometimes they turned left, sometimes they turned right. They saw many flowers and birds, but they did not see the exit. They started to feel lost and scared.\n",
      "They heard a loud roar behind them. It was a big lion who lived in the maze. He was hungry and angry. He wanted to catch Tom and Lily and eat them.\n",
      "Tom and Lily ran away from the lion. They looked for a place to hide. They saw a small hole in the bushes. They crawled inside and hoped the lion would not find them.\n",
      "The lion came closer and closer. He smelled Tom and Lily in the hole. He reached his paw inside and tried to grab them. He\n"
     ]
    }
   ],
   "source": [
    "print('Sample 1:', tokenizer.decode(next(iter(train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d4a7cb7e49d5f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:28.500203376Z",
     "start_time": "2023-10-24T01:58:28.456409648Z"
    }
   },
   "outputs": [],
   "source": [
    "global_config = {\n",
    "    'enable_torch_attn': False,\n",
    "    'enable_flash_attn': False,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int = -1,\n",
    "    num_layers: int = -1,\n",
    "    num_heads: int = -1,\n",
    "    hidden_size: int = -1,\n",
    "    max_seq_len: int = -1,\n",
    "    root_model: 'ToyTransformer' = None\n",
    "    device: torch.device = torch.device('cpu')\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    enable_rel_pos: bool = False\n",
    "\n",
    "\n",
    "def expand_attn_mask(custom_attn_mask: torch.Tensor):\n",
    "    B, T = custom_attn_mask.shape\n",
    "    mask = custom_attn_mask.unsqueeze(1).repeat((1, T, 1))\n",
    "    seq_index_mask = (mask == custom_attn_mask[:, torch.arange(T)].view(B, T, 1))\n",
    "    return seq_index_mask & (torch.tril(mask) > 0)\n",
    "\n",
    "\n",
    "# expand attn mask to cu_seqlens for flash attn\n",
    "def expand_attn_mask_to_seq_lengths(attn_mask: torch.Tensor):\n",
    "    attn_mask = attn_mask.to('cpu')\n",
    "    seq_len = attn_mask.shape[0] * attn_mask.shape[1]\n",
    "    disjoint_point = torch.cat([torch.tensor([[True]] * attn_mask.shape[0]), attn_mask[:, 1:] != attn_mask[:, :-1]], dim=1)\n",
    "    return torch.cat([torch.nonzero(disjoint_point.view((-1,))), torch.tensor([[seq_len]])]).to(dtype=torch.int32)\n",
    "\n",
    "\n",
    "# naive RoPE implementation following https://arxiv.org/pdf/2104.09864.pdf\n",
    "def get_rope_cache_slow(seq_len: int, dim: int, theta: int, device: torch.device, dtype: torch.dtype):\n",
    "    assert dim % 2 == 0\n",
    "    freqs = theta ** (-2 * torch.arange(0, dim // 2, 1.) / dim)\n",
    "    freqs = torch.repeat_interleave(freqs, 2)\n",
    "    v1 = torch.cos(torch.arange(seq_len, dtype=torch.float).view((seq_len, 1)) * freqs)\n",
    "    v2 = torch.sin(torch.arange(seq_len, dtype=torch.float).view((seq_len, 1)) * freqs)\n",
    "    v2 = v2 * torch.tensor([1, -1] * (dim // 2))\n",
    "    indices = torch.tensor([j for i in range(0, dim, 2) for j in (i + 1, i)])\n",
    "    return v1.to(device, dtype=dtype), v2.to(device, dtype=dtype), indices.to(device)\n",
    "\n",
    "\n",
    "def apply_rope_slow(x, rope_cache, positions: Optional[torch.Tensor] = None):\n",
    "    v1, v2, indices = rope_cache\n",
    "    seq_len, dim = x.shape[1:]\n",
    "    if positions is None:\n",
    "        v1 = v1[:seq_len, :]\n",
    "        v2 = v2[:seq_len, :]\n",
    "    else:\n",
    "        v1 = v1[positions, torch.arange(dim)].view((-1, dim))\n",
    "        v2 = v2[positions, torch.arange(dim)].view((-1, dim))\n",
    "    applied_x = x * v1 + (x * v2)[:, :, indices]\n",
    "    return applied_x\n",
    "\n",
    "\n",
    "# Optimized RoPE implementation adapted from https://github.com/facebookresearch/llama/blob/main/llama/model.py\n",
    "def get_rope_cache_fast(seq_len: int, dim: int, theta: int, device: torch.device, dtype: torch.dtype):\n",
    "    freqs = (1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)))\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "\n",
    "def apply_rope_fast(x, rope_cache, positions: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    if positions is None and x.shape[1] < rope_cache.shape[0]:\n",
    "        freqs_cis = rope_cache[:x.shape[1], :]\n",
    "    elif positions is not None:\n",
    "        freqs_cis = rope_cache[positions, :]\n",
    "    else:\n",
    "        freqs_cis = rope_cache\n",
    "    freqs_cis = freqs_cis.view([d if i == 1 or i == x_.ndim - 1 else 1 for i, d in enumerate(x_.shape)])\n",
    "\n",
    "    applied_x = torch.view_as_real(x_ * freqs_cis).flatten(2)\n",
    "    return applied_x.type_as(x)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.dtype = config.dtype\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        use_flash_attn = global_config['enable_flash_attn'] and kv_cache is None and attn_mask is None\n",
    "\n",
    "        if use_flash_attn:\n",
    "            apply_mask = None\n",
    "        elif kv_cache is None and attn_mask is not None:\n",
    "            apply_mask = expand_attn_mask(attn_mask)\n",
    "        elif kv_cache is None and attn_mask is None:\n",
    "            apply_mask = expand_attn_mask(torch.ones(x.shape[:2]))\n",
    "        elif kv_cache is not None:\n",
    "            apply_mask = torch.ones((B, T, T), dtype=torch.bool)\n",
    "        else:\n",
    "            apply_mask = None\n",
    "        # fill mask into attn bias\n",
    "        mask_zero = torch.tensor(0, dtype=self.dtype)\n",
    "        mask_val = torch.tensor(torch.finfo(self.dtype).min / 2, dtype=self.dtype)\n",
    "        if apply_mask is not None and not global_config['enable_torch_attn']:\n",
    "            apply_mask = torch.where(apply_mask, mask_zero, mask_val)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        if self.config.enable_rel_pos:\n",
    "            positions = torch.tensor([kv_cache[0].shape[1]]).to(q.device) if kv_cache is not None else None\n",
    "            q = apply_rope_fast(q, self.config.root_model.rope_cache, positions)\n",
    "            k = apply_rope_fast(k, self.config.root_model.rope_cache, positions)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k = torch.concat([kv_cache[0], k], dim=1)\n",
    "            v = torch.concat([kv_cache[1], v], dim=1)\n",
    "\n",
    "        if use_flash_attn:\n",
    "            q, k, v, = q.unsqueeze(2), k.unsqueeze(2), v.unsqueeze(2)\n",
    "            attn_result = flash_attn_func(q, k, v, causal=True)\n",
    "            q, k, v, attn_result = q.squeeze(2), k.squeeze(2), v.squeeze(2), attn_result.squeeze(2)\n",
    "        elif global_config['enable_torch_attn']:\n",
    "            with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n",
    "                attn_result = nn.functional.scaled_dot_product_attention(q, k, v,\n",
    "                                                                         attn_mask=apply_mask.to(q.device) if apply_mask is not None else None,\n",
    "                                                                         is_causal=True if apply_mask is None else False)\n",
    "        else:\n",
    "            attn_score = (q @ k.permute(0, 2, 1) / (self.hidden_size ** 0.5)) + apply_mask.to(q.device)\n",
    "            attn_result = torch.softmax(attn_score, dim=2) @ v\n",
    "\n",
    "        return attn_result, [k, v]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(config) for _ in range(config.num_heads)])\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, dtype=config.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        head_outputs = [head(x, attn_mask, kv_cache[idx] if kv_cache is not None else None) for idx, head in\n",
    "                        enumerate(self.attn_heads)]\n",
    "        return self.o_proj(torch.concat([o[0] for o in head_outputs], dim=2)), [o[1] for o in head_outputs]\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.hidden_size * 4, dtype=config.dtype)\n",
    "        self.down_proj = nn.Linear(config.hidden_size * 4, config.hidden_size, dtype=config.dtype)\n",
    "        self.ln_mha = nn.LayerNorm(config.hidden_size, dtype=config.dtype)\n",
    "        self.ln_ffn = nn.LayerNorm(config.hidden_size, dtype=config.dtype)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        mha_output, new_kv_cache = self.mha(self.ln_mha(x), attn_mask, kv_cache)\n",
    "        mha_output = x + mha_output\n",
    "        ffn_output = self.down_proj(self.act(self.up_proj(self.ln_ffn(mha_output))))\n",
    "        return mha_output + ffn_output, new_kv_cache\n",
    "\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, hidden_size: int, max_seq_len: int,\n",
    "                 device: torch.device = torch.device('cpu'), dtype: torch.dtype = torch.float32,\n",
    "                 enable_rel_pos: bool = False):\n",
    "        super().__init__()\n",
    "        self.config = TransformerConfig(vocab_size, num_layers, num_heads, hidden_size, max_seq_len, self, device,\n",
    "                                        dtype, enable_rel_pos)\n",
    "\n",
    "        self.sem_embed = nn.Embedding(vocab_size, hidden_size, dtype=dtype)\n",
    "\n",
    "        if not self.config.enable_rel_pos:\n",
    "            self.pos_embed = nn.Embedding(max_seq_len, hidden_size, dtype=dtype)\n",
    "        else:\n",
    "            # self.rope_cache = get_rope_cache(max_seq_len, hidden_size // num_heads, 10000, device, dtype)\n",
    "            self.rope_cache = get_rope_cache_fast(max_seq_len, hidden_size // num_heads, 10000, device, dtype)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(num_layers)])\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, dtype=dtype)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, seq: torch.Tensor,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                kv_cache: Optional[List[torch.Tensor]] = None) -> Tuple[torch.Tensor, List[List[List[torch.Tensor]]]]:\n",
    "\n",
    "        if self.config.enable_rel_pos:\n",
    "            hidden = self.sem_embed(seq)\n",
    "        elif position_ids is not None:\n",
    "            hidden = self.sem_embed(seq) + self.pos_embed(position_ids)\n",
    "        else:\n",
    "            hidden = self.sem_embed(seq) + self.pos_embed(torch.arange(0, seq.shape[1], 1).to(self.device))\n",
    "\n",
    "        new_kv_cache = []\n",
    "        for idx, decoder in enumerate(self.decoder_layers):\n",
    "            hidden, layer_kv_cache = decoder(hidden, attn_mask, kv_cache[idx] if kv_cache is not None else None)\n",
    "            new_kv_cache.append(layer_kv_cache)\n",
    "\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        return logits, new_kv_cache\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d124589733f9ed1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:29.753030424Z",
     "start_time": "2023-10-24T01:58:28.456578150Z"
    }
   },
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    model = ToyTransformer(tokenizer.get_vocab_size(), 2, 2, 256, 1024, C_DEVICE, C_DTYPE, enable_rel_pos=True)\n",
    "else:\n",
    "    model = ToyTransformer(tokenizer.get_vocab_size(), C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN, C_DEVICE, C_DTYPE, enable_rel_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f906aed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:29.800780219Z",
     "start_time": "2023-10-24T01:58:29.755513507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 135418880\n",
      "ToyTransformer(\n",
      "  (sem_embed): Embedding(32768, 768)\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-11): 12 x DecoderLayer(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (attn_heads): ModuleList(\n",
      "          (0-11): 12 x AttentionHead(\n",
      "            (q_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (k_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (ln_mha): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_ffn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('Total parameters:', sum([t.numel() for t in model.parameters()]))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca6004e9b65892c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:29.885843126Z",
     "start_time": "2023-10-24T01:58:29.800472747Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674478c0ccb206af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:29.928427427Z",
     "start_time": "2023-10-24T01:58:29.886937572Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_collate(dataset: DatasetReader, batch_size: int,\n",
    "                    transform: Optional[Callable[[Dict[str, List[np.ndarray]]], Dict[str, torch.Tensor]]] = None,\n",
    "                    drop_last: bool = False):\n",
    "    cur_batch, cur_batch_size = {}, 0\n",
    "    for entry in iter(dataset):\n",
    "        for k, v in entry.items():\n",
    "            cur_batch.setdefault(k, [])\n",
    "            cur_batch[k].append(v)\n",
    "        cur_batch_size += 1\n",
    "        if cur_batch_size == batch_size:\n",
    "            yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)\n",
    "            cur_batch = {}\n",
    "            cur_batch_size = 0\n",
    "    if not drop_last and len(cur_batch) > 0:\n",
    "        yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-10-24T01:58:29.928591101Z",
     "start_time": "2023-10-24T01:58:29.928364867Z"
    }
   },
   "outputs": [],
   "source": [
    "# type cast for handling int16/uint16 columns\n",
    "def train_transform(batch: Dict[str, List[np.ndarray]]):\n",
    "    return {k: torch.tensor(np.stack(v, dtype=np.int32 if v[0].dtype in [np.int16, np.uint16] else v[0].dtype)) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, num_epochs: int, batch_size: int, gradient_accumulation_steps: int,\n",
    "                start_lr, max_lr: float, end_lr: float, warmup_ratio: float,\n",
    "                dataset: DatasetReader, gradient_clip: float = 1.0, show_progress=True,\n",
    "                ignore_attn_mask: bool = False, ignore_loss_mask: bool = False,\n",
    "                train_logs: Optional[List] = None):\n",
    "    if train_logs is None:\n",
    "        train_logs = []\n",
    "\n",
    "    total_samples = len(dataset)\n",
    "    epoch_steps = ceil(total_samples / batch_size)\n",
    "    assert epoch_steps >= gradient_accumulation_steps, f'per-epoch steps {epoch_steps} is less than gradient accumulation steps {gradient_accumulation_steps}'\n",
    "\n",
    "    schedule_steps = ceil(total_samples / batch_size / gradient_accumulation_steps)\n",
    "    total_steps = schedule_steps * num_epochs\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, div_factor=max_lr / start_lr,\n",
    "                                                    total_steps=total_steps,\n",
    "                                                    final_div_factor=start_lr / end_lr, pct_start=warmup_ratio)\n",
    "\n",
    "    bar = tqdm.tqdm(total=total_steps, disable=not show_progress)\n",
    "\n",
    "    model.train()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, batch in enumerate(dataset_collate(dataset, batch_size, train_transform)):\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            tokens = batch['token_ids'].to(model.device)\n",
    "            inputs = tokens[:, :-1]\n",
    "            labels = tokens[:, 1:]\n",
    "\n",
    "            positions = batch['position_ids'][:, :-1].to(model.device) if 'position_ids' in batch else None\n",
    "            attn_mask = batch['attn_mask'][:, :-1].to(model.device) if 'attn_mask' in batch and not ignore_attn_mask else None\n",
    "            loss_mask = batch['loss_mask'][:, :-1].to(model.device) if 'loss_mask' in batch and not ignore_loss_mask else None\n",
    "\n",
    "            logits, kv_state = model.forward(inputs, position_ids=positions, attn_mask=attn_mask)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=2).view(-1, logits.shape[-1])\n",
    "\n",
    "            loss = (-torch.log(probs[torch.arange(probs.shape[0]), labels.reshape(-1)]))\n",
    "            if loss_mask is not None:\n",
    "                loss = (loss * loss_mask.reshape(-1)).mean() / gradient_accumulation_steps\n",
    "            else:\n",
    "                loss = loss.mean() / gradient_accumulation_steps\n",
    "\n",
    "            # brutally clear nan, give up the whole batch\n",
    "            if torch.isnan(loss):\n",
    "                print(f'encountered nan loss at epoch {epoch_num + 1}, batch {batch_idx}')\n",
    "                # optimizer.zero_grad()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == epoch_steps:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                step_time_cost = time.time() - step_start_time\n",
    "                throughput = round(probs.shape[0] / step_time_cost / 1000, 2)\n",
    "\n",
    "                step_stat = {'Loss': f'{loss.item() * gradient_accumulation_steps:.3f}',\n",
    "                             'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                             'Throughput': f'{throughput} kt/s'}\n",
    "\n",
    "                if show_progress:\n",
    "                    bar.set_description(f'Epoch {epoch_num + 1}')\n",
    "                    bar.set_postfix(step_stat)\n",
    "                else:\n",
    "                    print(', '.join(f'{s[0]}:{s[1]}' for s in step_stat.items()))\n",
    "\n",
    "                scheduler.step()\n",
    "                bar.update(1)\n",
    "                train_logs.append((epoch_num, batch_idx, step_stat))\n",
    "\n",
    "    bar.close()\n",
    "\n",
    "    return train_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281e39ed7414acc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T02:01:17.264637570Z",
     "start_time": "2023-10-24T01:58:29.928504838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/7989 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8aa740319dfc4d5094f4f08868ad8b0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 11\u001B[0m\n\u001B[1;32m      5\u001B[0m     train_model(model, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, gradient_accumulation_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, start_lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-5\u001B[39m, max_lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m, end_lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-6\u001B[39m,\n\u001B[1;32m      6\u001B[0m                 warmup_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m,\n\u001B[1;32m      7\u001B[0m                 dataset\u001B[38;5;241m=\u001B[39mtrain_data, train_logs\u001B[38;5;241m=\u001B[39mlast_train_logs,\n\u001B[1;32m      8\u001B[0m                 ignore_attn_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, ignore_loss_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      9\u001B[0m                 show_progress\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 11\u001B[0m     \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_lr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m6e-6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_lr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m6e-4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_lr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m6e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m                \u001B[49m\u001B[43mwarmup_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m                \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m                \u001B[49m\u001B[43mgradient_clip\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_logs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlast_train_logs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m                \u001B[49m\u001B[43mignore_attn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_loss_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m                \u001B[49m\u001B[43mshow_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 61\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, num_epochs, batch_size, gradient_accumulation_steps, start_lr, max_lr, end_lr, warmup_ratio, dataset, gradient_clip, show_progress, ignore_attn_mask, ignore_loss_mask, train_logs)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (batch_idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m gradient_accumulation_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m (batch_idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m==\u001B[39m epoch_steps:\n\u001B[1;32m     60\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), gradient_clip)\n\u001B[0;32m---> 61\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     64\u001B[0m     step_time_cost \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m step_start_time\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001B[0m, in \u001B[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     66\u001B[0m instance\u001B[38;5;241m.\u001B[39m_step_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     67\u001B[0m wrapped \u001B[38;5;241m=\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__get__\u001B[39m(instance, \u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m---> 68\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    368\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    369\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    370\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    371\u001B[0m             )\n\u001B[0;32m--> 373\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    376\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/adamw.py:184\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    171\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    174\u001B[0m         group,\n\u001B[1;32m    175\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    181\u001B[0m         state_steps,\n\u001B[1;32m    182\u001B[0m     )\n\u001B[0;32m--> 184\u001B[0m     \u001B[43madamw\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/adamw.py:335\u001B[0m, in \u001B[0;36madamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    333\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[0;32m--> 335\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    336\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/adamw.py:543\u001B[0m, in \u001B[0;36m_multi_tensor_adamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    540\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[1;32m    542\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001B[0;32m--> 543\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_foreach_addcmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_grads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_grads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[38;5;66;03m# Delete the local intermediate since it won't be used anymore to save on peak memory\u001B[39;00m\n\u001B[1;32m    546\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m device_grads\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "global_config['enable_flash_attn'] = True\n",
    "global_config['enable_torch_attn'] = False\n",
    "last_train_logs = []\n",
    "if C_DEBUG:\n",
    "    train_model(model, num_epochs=1000, batch_size=8, gradient_accumulation_steps=1, start_lr=1e-5, max_lr=1e-3, end_lr=1e-6,\n",
    "                warmup_ratio=0.1,\n",
    "                dataset=train_data, train_logs=last_train_logs,\n",
    "                ignore_attn_mask=True, ignore_loss_mask=True,\n",
    "                show_progress=True)\n",
    "else:\n",
    "    train_model(model, num_epochs=1, batch_size=12, gradient_accumulation_steps=3, start_lr=6e-6, max_lr=6e-4, end_lr=6e-5,\n",
    "                warmup_ratio=0.1,\n",
    "                dataset=train_data,\n",
    "                gradient_clip=0.5, train_logs=last_train_logs,\n",
    "                ignore_attn_mask=True, ignore_loss_mask=True,\n",
    "                show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ca07cd31cd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].plot([float(l[2]['Loss']) for l in last_train_logs])\n",
    "axes[1].plot([float(l[2]['LR']) for l in last_train_logs])\n",
    "axes[2].plot([float(l[2]['Throughput'][:-5]) for l in last_train_logs])\n",
    "axes[0].set_title('Loss')\n",
    "axes[1].set_title('Learning Rate')\n",
    "axes[2].set_title('Throughput (kt/s)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.autoscale()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = 'checkpoints/train-231022-tinystories'\n",
    "# torch.save(model.state_dict(), f'{run_name}.pt')\n",
    "# torch.save(last_train_logs, f'{run_name}-logs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e800403549c98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, temperature, top_p, rep_penalty,\n",
    "             max_new_tokens=20, total_tokens=None,\n",
    "             end_tokens=None,\n",
    "             enable_kv_cache=True):\n",
    "    model.eval()\n",
    "\n",
    "    feed_tokens = tokenizer.encode(prompt)\n",
    "    all_tokens = feed_tokens.copy()\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(feed_tokens))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            position_ids = None if kv_cache is None else torch.tensor([[len(all_tokens) - 1]]).to(model.device)\n",
    "            logits, kv_cache = model.forward(\n",
    "                torch.tensor([feed_tokens if enable_kv_cache else all_tokens]).to(model.device),\n",
    "                position_ids=position_ids,\n",
    "                kv_cache=kv_cache)\n",
    "            logits = logits[0][-1].cpu()\n",
    "            if not enable_kv_cache:\n",
    "                kv_cache = None\n",
    "\n",
    "            # apply repetition penalty\n",
    "            logits_rep = torch.gather(logits, 0, torch.tensor(all_tokens))\n",
    "            logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "            logits.scatter_(0, torch.tensor(all_tokens), logits_rep)\n",
    "\n",
    "            # apply temperature\n",
    "            logits /= max(temperature, 1e-6)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # apply top-p\n",
    "            ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "            cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "            top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "            ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "            sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "            all_tokens.append(sampled_index)\n",
    "            feed_tokens = [sampled_index]\n",
    "\n",
    "            if end_tokens is not None and sampled_index in end_tokens:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c17f13926df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokenizer.decode(next(iter(train_data))['token_ids'].tolist()).split('</s>')[:3]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc924753b5acfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()\n",
    "global_config['enable_flash_attn'] = False\n",
    "global_config['enable_torch_attn'] = False\n",
    "result = generate(model, tokenizer, '<s>One day, a little boy named Tim went to the park',\n",
    "                  temperature=1.0, top_p=0.01, rep_penalty=1.0,\n",
    "                  total_tokens=350,\n",
    "                  end_tokens=tokenizer.encode('<reserved_0>'),\n",
    "                  enable_kv_cache=True)\n",
    "print(result)\n",
    "print(f'{time.time() - a:.3f} sec(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537635f6ba64280",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((1, 2047, 1, 128))\n",
    "nn.functional.pad(a, (0, 0, 0, 0, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ae8be2acbacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = next(iter(train_data))\n",
    "a, t = torch.tensor(e['attn_mask'].astype(np.int32)), e['token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214f52b8b0327d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fe032494a962c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

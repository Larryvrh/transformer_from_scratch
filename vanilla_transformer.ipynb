{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-10-27T02:47:51.572198415Z",
     "start_time": "2023-10-27T02:47:49.494130453Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import TRIETokenizerFast\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "from typing import *\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from flash_attn import flash_attn_func\n",
    "from flash_attn_triton import flash_attn_func as flash_attn_func_triton\n",
    "from dataloader import DatasetReader, DatasetIter, SingleDatasetReader, MultiDatasetsReader\n",
    "from math import ceil\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import bitsandbytes as bnb\n",
    "from threading import Lock, Thread\n",
    "import traceback\n",
    "import json\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33702a8e6613f742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:47:59.991673629Z",
     "start_time": "2023-10-27T02:47:51.573044135Z"
    }
   },
   "outputs": [],
   "source": [
    "g_tokenizer = TRIETokenizerFast('llama_vocab_pruned_32k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb151752ec125fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:00.007213538Z",
     "start_time": "2023-10-27T02:47:59.992457604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network definition\n",
    "C_SEQ_LEN = 2048\n",
    "C_HIDDEN_SIZE = 768\n",
    "C_NUM_HEADS = 12\n",
    "C_NUM_LAYERS = 12\n",
    "\n",
    "C_DEVICE = torch.device('cuda')\n",
    "C_DTYPE = torch.bfloat16\n",
    "\n",
    "C_DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799b78921dbff1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:00.007289319Z",
     "start_time": "2023-10-27T02:47:59.994582928Z"
    }
   },
   "outputs": [],
   "source": [
    "if not C_DEBUG:\n",
    "    g_train_data = MultiDatasetsReader([\n",
    "        SingleDatasetReader('datasets/minipile_train.bin'),\n",
    "        SingleDatasetReader('datasets/enwiki_train.bin'),\n",
    "        SingleDatasetReader('datasets/tinytextbooks_train.bin'),\n",
    "    ], seed=0)\n",
    "else:\n",
    "    g_train_data = MultiDatasetsReader([\n",
    "        SingleDatasetReader('datasets/debug_data_masked.bin'),\n",
    "        SingleDatasetReader('datasets/debug_data_masked.bin'),\n",
    "    ], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d770a2be845a70d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:00.036933867Z",
     "start_time": "2023-10-27T02:48:00.004387018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1085632\n",
      "Sample length: 2048\n",
      "Train tokens: 2223374336\n"
     ]
    }
   ],
   "source": [
    "print('Train samples:', len(g_train_data))\n",
    "print('Sample length:', len(next(iter(g_train_data))['token_ids']))\n",
    "print('Train tokens:', len(g_train_data) * len(next(iter(g_train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8dbb96cab10318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:00.098809305Z",
     "start_time": "2023-10-27T02:48:00.014472483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: <s>Attraction at Disneyland\n",
      "King Arthur Carrousel is a carousel attraction located in Fantasyland at Disneyland in Anaheim, California. The carousel was built in 1922 and operated at Sunnyside Beach Park in Toronto, Ontario, until the park closed. The ride was relocated to Disneyland in 1954, where it was refurbished and modified by Arrow Development, and opened with the park on July 17, 1955.\n",
      "\n",
      "History\n",
      "Inspired by the Griffith Park carousel, Walt Disney wanted something similar for his new theme park: a carousel consisting of all jumpers. A park model Menagerie Carousel was purchased and moved to Disneyland in 1954. The carousel was built by William Dentzel and had been operated at Sunnyside Beach Park in Toronto, Ontario, since 1922; it had three courses of horses and other animals on a platform 72 feet (22 m) in diameter.\n",
      "Preparation\n",
      "\n",
      "The attraction was refurbished and significantly altered by the Arrow Development Company of Mountain View, California in preparation for opening day. It was widened to four courses to increase guest capacity. Of the carousel's 71 horses and one mule, most were carved in the Dentzel factory. To add the outermost course, several carved wooden horses were acquired from a Stein and Goldstein carousel, others from Coney Island's Looff carousel, and more carved horses from various other carousels from around North America. Many horses arrived with crude repairs, such as newspaper-stuffed papier-mâché legs. Standers on the original three rows were converted to jumpers by removing the legs and carving new ones. Custom-built crankshafts were installed overhead to operate each horse as a jumper in motion. The original, ornately hand-carved, wooden chariot benches were removed, and the chariot woodwork was repurposed to decorate the \"calliope\" tenders and passenger cars of Casey Jr. Circus Train. A Wurlitzer #157 Band Organ facade decorates the carousel, but does not operate. Motifs from Sleeping Beauty were also added to the carousel.\n",
      "\n",
      "The carousel was placed in a prominent position in the middle of the castle courtyard, able to be viewed from Main Street through the castle gate, drawing guests into the realm of fantasy.\n",
      "\n",
      "Refurbishment\n",
      "\n",
      "There were two refurbishments to the carousel: one in 1983 and one in 2003. In 1983, to make room for other attractions, the carousel was moved slightly backwards and received a completely new roof. The carousel was also repainted in orange, red, and blue, and the princess and jester rounding boards were repainted into 18k gold. In preparation for Disneyland's 50th anniversary celebration, the Happiest Homecoming on Earth, King Arthur Carrousel closed for extensive renovations and reopened in February 2003. These renovations included an entirely rebuilt turntable platform, a new computerized operating console and system which halts the carousel each time at the same spot, removal of a row of four horses to accommodate a four-course-wide bench and wheelchair clamps with an access ramp for ADA compliance, which reduced the count of horses to 68. In January 2010, the stirrups of each outer-course horse were replaced to include additional lower loops, increasing accessibility.\n",
      "\n",
      "Horses\n",
      "Because of the overwhelming popularity of the carousel's single white horse, since 1975 all horses have been painted white.[citation needed] After a 2003 update, the carrousel was reduced to 68 horses and one chariot. Each horse on the carousel has a name; a complete list is available at City Hall on Main Street, U.S.A.\n",
      "\n",
      "Jingles is the lead horse, and Walt's favorite, named for her very ornate carvings which include straps of jingle bells hanging from her breast collar, decorative quarter sheet behind the saddle, and fastened on the cantle. For Disneyland's 50th anniversary in 2005, Jingles was repainted gold from nose to tail, trimmed in 18k gold leaf set apart as a photo opportunity near the queue for Dumbo the Flying Elephant. When Jingles was reinstalled as lead horse after the Year of a Million Dreams campaign, major portions of Jingles were painted over in a new pastel color theme, except where the gold bells and trim show through, with a translucent treatment of the rosettes on Jingles' head. Decorative detail was painted on the quarter sheet representing the talking-parrot-handled umbrella from Mary Poppins. On the saddle flap, a decorative crest was added, with the monogram \"JA\", a bird perched on high button shoes, a silhouette of Mary in flight, and the number 50, representing the 50th anniversary of this original Disneyland attraction. Jingles was then ceremoniously dedicated to Julie Andrews on April 8, 2008, as \"Honorary Ambassador\", the title painted beneath the Hidden Mickey on her cantle.\n",
      "\n",
      "Sword in the Stone Ceremony\n",
      "\n",
      "Inspired by the legend of Excalibur from The Sword in the Stone, Merlin used to host a ceremony nearby to determine which guest could pull the sword from the stone to become king for a day. The final ceremony was in 2006.\n",
      "\n",
      "\"By proclamation of Arthur, the right and true king, and lord of all the land, it is time to select a temporary ruler of the realm... to safeguard and protect the kingdom while good King Arthur is on vacation.\"\n",
      "\n",
      "The statue with the sword still stands at the front of the carrousel. In January, 2020, a visitor reportedly pulled out the sword.</s><s>Work settlement in Irkutsk Oblast, Russia\n",
      "Balagansk (Russian: Балага́нск) is an urban locality (a work settlement) and the administrative center of Balagansky District, Irkutsk Oblast, Russia. It is located on the left bank of the Angara River, downstream from Svirsk and 285 kilometres (177 mi) by road northwest of Irkutsk and to the southeast of Sayansk. Population: 4,109 (2010 Census); 4,307 (2002 Census); 4,136 (1989 Census).\n",
      "\n",
      "On three sides of Balagansk is the Bratsk Reservoir. The settlement is best known for Balagansk Prison, which was used as one of the Siberian exile camps during the Stalin era.\n",
      "\n",
      "History\n",
      "\n",
      "Balagansk was founded in 1654 on the left bank of the Angara River opposite to the mouth of the Unga River by the Cossack detachment led by Dmitry Firsov in the course of Russian colonization of Siberia. Its name is derived from the word \"Bulagat\", literally meaning sable hunters, a Buryat tribe. From 1655, mass settlement started in the area; eventually, a colony was built and iron mining developed. In 1658, Ivan Pokhabov, the administrator (prikazchik) of Balagansk, caused an outbreak and many Russians were killed by them. Balagansk Fortress (ostrog) was built in Balagansk; the Buryats were attached to it and paid tributes to the Russians. Buryats grew to dominate the area and on April 1, 1818, seventeen clans of the Balagansk Buryats met and adopted a memorandum to submit to the Russian authorities. In their memorandum they raised six issues and also provided action to be taken on each of them which related to courts, private law, criminal law, the rights and duties of the chief Taisha and heads of clans.\n",
      "\n",
      "In the course of the administrative reform carried out in 1708 by Peter the Great, the area was included in Siberia Governorate. In 1764, Irkutsk Governorate split off, and in 1775, Balagansk became a town and the seat of Balagansky Uyezd of Irkutsk Governorate. In 1924, the uyezds were abolished, the governorate was split into districts, and Balagansk became a part of Ziminsky District. In 1925, it lost town status and was downgraded to a selo. In 1926, Balagansky District was established, and\n"
     ]
    }
   ],
   "source": [
    "print('Sample 1:', g_tokenizer.decode(next(iter(g_train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d4a7cb7e49d5f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:00.098921513Z",
     "start_time": "2023-10-27T02:48:00.058155143Z"
    }
   },
   "outputs": [],
   "source": [
    "global_config = {\n",
    "    'enable_torch_attn': False,\n",
    "    'enable_flash_attn': False,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int = -1,\n",
    "    num_layers: int = -1,\n",
    "    num_heads: int = -1,\n",
    "    hidden_size: int = -1,\n",
    "    max_seq_len: int = -1,\n",
    "    root_model: 'ToyTransformer' = None\n",
    "    device: torch.device = torch.device('cpu')\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    enable_rel_pos: bool = False\n",
    "\n",
    "\n",
    "def expand_attn_mask(custom_attn_mask: torch.Tensor):\n",
    "    B, T = custom_attn_mask.shape\n",
    "    mask = custom_attn_mask.unsqueeze(1).repeat((1, T, 1))\n",
    "    seq_index_mask = (mask == custom_attn_mask[:, torch.arange(T)].view(B, T, 1))\n",
    "    return seq_index_mask & (torch.tril(mask) > 0)\n",
    "\n",
    "\n",
    "# expand attn mask to cu_seqlens for flash attn\n",
    "def expand_attn_mask_to_seq_lengths(attn_mask: torch.Tensor):\n",
    "    attn_mask = attn_mask.to('cpu')\n",
    "    seq_len = attn_mask.shape[0] * attn_mask.shape[1]\n",
    "    disjoint_point = torch.cat([torch.tensor([[True]] * attn_mask.shape[0]), attn_mask[:, 1:] != attn_mask[:, :-1]], dim=1)\n",
    "    return torch.cat([torch.nonzero(disjoint_point.view((-1,))), torch.tensor([[seq_len]])]).to(dtype=torch.int32)\n",
    "\n",
    "\n",
    "# naive RoPE implementation following https://arxiv.org/pdf/2104.09864.pdf\n",
    "def get_rope_cache_slow(seq_len: int, dim: int, theta: int, device: torch.device, dtype: torch.dtype):\n",
    "    assert dim % 2 == 0\n",
    "    freqs = theta ** (-2 * torch.arange(0, dim // 2, 1.) / dim)\n",
    "    freqs = torch.repeat_interleave(freqs, 2)\n",
    "    v1 = torch.cos(torch.arange(seq_len, dtype=torch.float).view((seq_len, 1)) * freqs)\n",
    "    v2 = torch.sin(torch.arange(seq_len, dtype=torch.float).view((seq_len, 1)) * freqs)\n",
    "    v2 = v2 * torch.tensor([1, -1] * (dim // 2))\n",
    "    indices = torch.tensor([j for i in range(0, dim, 2) for j in (i + 1, i)])\n",
    "    return v1.to(device, dtype=dtype), v2.to(device, dtype=dtype), indices.to(device)\n",
    "\n",
    "\n",
    "def apply_rope_slow(x, rope_cache, positions: Optional[torch.Tensor] = None):\n",
    "    v1, v2, indices = rope_cache\n",
    "    seq_len, dim = x.shape[1:]\n",
    "    if positions is None:\n",
    "        v1 = v1[:seq_len, :]\n",
    "        v2 = v2[:seq_len, :]\n",
    "    else:\n",
    "        v1 = v1[positions, torch.arange(dim)].view((-1, dim))\n",
    "        v2 = v2[positions, torch.arange(dim)].view((-1, dim))\n",
    "    applied_x = x * v1 + (x * v2)[:, :, indices]\n",
    "    return applied_x\n",
    "\n",
    "\n",
    "# Optimized RoPE implementation adapted from https://github.com/facebookresearch/llama/blob/main/llama/model.py\n",
    "def get_rope_cache_fast(seq_len: int, dim: int, theta: int, device: torch.device, dtype: torch.dtype):\n",
    "    freqs = (1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)))\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "\n",
    "def apply_rope_fast(x, rope_cache, positions: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    if positions is None and x.shape[1] < rope_cache.shape[0]:\n",
    "        freqs_cis = rope_cache[:x.shape[1], :]\n",
    "    elif positions is not None:\n",
    "        freqs_cis = rope_cache[positions, :]\n",
    "    else:\n",
    "        freqs_cis = rope_cache\n",
    "    freqs_cis = freqs_cis.view([d if i == 1 or i == x_.ndim - 1 else 1 for i, d in enumerate(x_.shape)])\n",
    "\n",
    "    applied_x = torch.view_as_real(x_ * freqs_cis).flatten(2)\n",
    "    return applied_x.type_as(x)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.dtype = config.dtype\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        use_flash_attn = global_config['enable_flash_attn'] and kv_cache is None\n",
    "\n",
    "        if kv_cache is None and attn_mask is not None:\n",
    "            apply_mask = expand_attn_mask(attn_mask)\n",
    "        elif kv_cache is None and attn_mask is None and not use_flash_attn:\n",
    "            apply_mask = expand_attn_mask(torch.ones(x.shape[:2]))\n",
    "        elif kv_cache is not None:\n",
    "            apply_mask = torch.ones((B, T, T), dtype=torch.bool)\n",
    "        else:\n",
    "            apply_mask = None\n",
    "\n",
    "        # fill mask into attn bias\n",
    "        mask_zero = torch.tensor(0, dtype=self.dtype)\n",
    "        mask_val = torch.tensor(torch.finfo(self.dtype).min / 2, dtype=self.dtype)\n",
    "        if apply_mask is not None and not global_config['enable_torch_attn']:\n",
    "            apply_mask = torch.where(apply_mask, mask_zero, mask_val)  # apply_mask is now attn_bias\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        if self.config.enable_rel_pos:\n",
    "            positions = torch.tensor([kv_cache[0].shape[1]]).to(q.device) if kv_cache is not None else None\n",
    "            q = apply_rope_fast(q, self.config.root_model.rope_cache, positions)\n",
    "            k = apply_rope_fast(k, self.config.root_model.rope_cache, positions)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k = torch.concat([kv_cache[0], k], dim=1)\n",
    "            v = torch.concat([kv_cache[1], v], dim=1)\n",
    "\n",
    "        if use_flash_attn and apply_mask is None:\n",
    "            q, k, v, = q.unsqueeze(2), k.unsqueeze(2), v.unsqueeze(2)\n",
    "            # attn_result = flash_attn_func(q, k, v, causal=True)\n",
    "            attn_result = flash_attn_func_triton(q, k, v, None, True)\n",
    "            q, k, v, attn_result = q.squeeze(2), k.squeeze(2), v.squeeze(2), attn_result.squeeze(2)\n",
    "        elif use_flash_attn and apply_mask is not None:\n",
    "            q, k, v, = q.unsqueeze(2), k.unsqueeze(2), v.unsqueeze(2)\n",
    "            attn_result = flash_attn_func_triton(q, k, v, apply_mask.unsqueeze(1).to(q.device), True)\n",
    "            q, k, v, attn_result = q.squeeze(2), k.squeeze(2), v.squeeze(2), attn_result.squeeze(2)\n",
    "        elif global_config['enable_torch_attn']:\n",
    "            with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n",
    "                attn_result = nn.functional.scaled_dot_product_attention(q, k, v,\n",
    "                                                                         attn_mask=apply_mask.to(q.device) if apply_mask is not None else None,\n",
    "                                                                         is_causal=True if apply_mask is None else False)\n",
    "        else:\n",
    "            attn_score = (q @ k.permute(0, 2, 1) / (self.hidden_size ** 0.5)) + apply_mask.to(q.device)\n",
    "            attn_result = torch.softmax(attn_score, dim=2) @ v\n",
    "\n",
    "        return attn_result, [k, v]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(config) for _ in range(config.num_heads)])\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, dtype=config.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        head_outputs = [head(x, attn_mask, kv_cache[idx] if kv_cache is not None else None) for idx, head in\n",
    "                        enumerate(self.attn_heads)]\n",
    "        return self.o_proj(torch.concat([o[0] for o in head_outputs], dim=2)), [o[1] for o in head_outputs]\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.hidden_size * 4, dtype=config.dtype)\n",
    "        self.down_proj = nn.Linear(config.hidden_size * 4, config.hidden_size, dtype=config.dtype)\n",
    "        self.ln_mha = nn.LayerNorm(config.hidden_size, dtype=config.dtype)\n",
    "        self.ln_ffn = nn.LayerNorm(config.hidden_size, dtype=config.dtype)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        mha_output, new_kv_cache = self.mha(self.ln_mha(x), attn_mask, kv_cache)\n",
    "        mha_output = x + mha_output\n",
    "        ffn_output = self.down_proj(self.act(self.up_proj(self.ln_ffn(mha_output))))\n",
    "        return mha_output + ffn_output, new_kv_cache\n",
    "\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, hidden_size: int, max_seq_len: int,\n",
    "                 device: torch.device = torch.device('cpu'), dtype: torch.dtype = torch.float32,\n",
    "                 enable_rel_pos: bool = False):\n",
    "        super().__init__()\n",
    "        self.config = TransformerConfig(vocab_size, num_layers, num_heads, hidden_size, max_seq_len, self, device,\n",
    "                                        dtype, enable_rel_pos)\n",
    "\n",
    "        self.sem_embed = nn.Embedding(vocab_size, hidden_size, dtype=dtype)\n",
    "\n",
    "        if not self.config.enable_rel_pos:\n",
    "            self.pos_embed = nn.Embedding(max_seq_len, hidden_size, dtype=dtype)\n",
    "        else:\n",
    "            # self.rope_cache = get_rope_cache(max_seq_len, hidden_size // num_heads, 10000, device, dtype)\n",
    "            self.rope_cache = get_rope_cache_fast(max_seq_len, hidden_size // num_heads, 10000, device, dtype)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(num_layers)])\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, dtype=dtype)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, seq: torch.Tensor,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                kv_cache: Optional[List[torch.Tensor]] = None) -> Tuple[torch.Tensor, List[List[List[torch.Tensor]]]]:\n",
    "\n",
    "        if self.config.enable_rel_pos:\n",
    "            hidden = self.sem_embed(seq)\n",
    "        elif position_ids is not None:\n",
    "            hidden = self.sem_embed(seq) + self.pos_embed(position_ids)\n",
    "        else:\n",
    "            hidden = self.sem_embed(seq) + self.pos_embed(torch.arange(0, seq.shape[1], 1).to(self.device))\n",
    "\n",
    "        new_kv_cache = []\n",
    "        for idx, decoder in enumerate(self.decoder_layers):\n",
    "            hidden, layer_kv_cache = decoder(hidden, attn_mask, kv_cache[idx] if kv_cache is not None else None)\n",
    "            new_kv_cache.append(layer_kv_cache)\n",
    "\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        return logits, new_kv_cache\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d124589733f9ed1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:01.356342057Z",
     "start_time": "2023-10-27T02:48:00.058246703Z"
    }
   },
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    g_model = ToyTransformer(g_tokenizer.get_vocab_size(), 2, 2, 256, 1024, C_DEVICE, C_DTYPE, enable_rel_pos=True)\n",
    "else:\n",
    "    g_model = ToyTransformer(g_tokenizer.get_vocab_size(), C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN, C_DEVICE, C_DTYPE, enable_rel_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f906aed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:01.362384859Z",
     "start_time": "2023-10-27T02:48:01.357911163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 135418880\n",
      "ToyTransformer(\n",
      "  (sem_embed): Embedding(32768, 768)\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-11): 12 x DecoderLayer(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (attn_heads): ModuleList(\n",
      "          (0-11): 12 x AttentionHead(\n",
      "            (q_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (k_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (ln_mha): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_ffn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('Total parameters:', sum([t.numel() for t in g_model.parameters()]))\n",
    "print(g_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca6004e9b65892c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:01.483995781Z",
     "start_time": "2023-10-27T02:48:01.361080315Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674478c0ccb206af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:01.526251853Z",
     "start_time": "2023-10-27T02:48:01.526142858Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_collate(dataset_iter: DatasetIter, batch_size: int,\n",
    "                    transform: Optional[Callable[[Dict[str, List[np.ndarray]]], Dict[str, torch.Tensor]]] = None,\n",
    "                    drop_last: bool = False):\n",
    "    cur_batch, cur_batch_size = {}, 0\n",
    "    for entry in dataset_iter:\n",
    "        for k, v in entry.items():\n",
    "            cur_batch.setdefault(k, [])\n",
    "            cur_batch[k].append(v)\n",
    "        cur_batch_size += 1\n",
    "        if cur_batch_size == batch_size:\n",
    "            yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)\n",
    "            cur_batch = {}\n",
    "            cur_batch_size = 0\n",
    "    if not drop_last and len(cur_batch) > 0:\n",
    "        yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:01.526342177Z",
     "start_time": "2023-10-27T02:48:01.526213440Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainArguments:\n",
    "    num_epochs: int\n",
    "    batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "\n",
    "    optimizer: Type[torch.optim.Optimizer]\n",
    "    optimizer_args: Optional[Dict[str, Any]]\n",
    "\n",
    "    start_lr: float\n",
    "    max_lr: float\n",
    "    end_lr: float\n",
    "    warmup_ratio: float\n",
    "\n",
    "    gradient_clip_norm: Optional[float]\n",
    "\n",
    "    train_data: DatasetReader\n",
    "    ignore_attn_mask: bool\n",
    "    ignore_loss_mask: bool\n",
    "\n",
    "    # eval_data: Optional[DatasetReader]\n",
    "    # eval_steps: int\n",
    "    # \n",
    "    # eval_generate_prompt: Optional[str]\n",
    "    # eval_generate_steps: int\n",
    "\n",
    "    save_steps: int\n",
    "    save_on_interrupt: bool\n",
    "\n",
    "\n",
    "# type cast for handling int16/uint16 columns\n",
    "def train_transform(batch: Dict[str, List[np.ndarray]]):\n",
    "    return {k: torch.tensor(np.stack(v, dtype=np.int32 if v[0].dtype in [np.int16, np.uint16] else v[0].dtype)) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, model: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer, lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "                    train_args: TrainArguments,\n",
    "                    dataset: DatasetReader, dataset_iter: DatasetIter, train_logs: List, misc: Dict):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    torch.save(model.state_dict(), path + '/model.pt')\n",
    "    torch.save(optimizer.state_dict(), path + '/optimizer.pt')\n",
    "    torch.save(lr_scheduler.state_dict(), path + '/lr_scheduler.pt')\n",
    "    torch.save(torch.get_rng_state(), path + '/rng_state.pt')\n",
    "    torch.save(train_args, path + '/train_args.pt')\n",
    "    dataset.save_iterator(dataset_iter, path + '/dataset_iter.pt')\n",
    "    torch.save(train_logs, path + '/train_logs.pt')\n",
    "    torch.save(misc, path + '/misc.pt')\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str, model: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer, lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "                    train_args: TrainArguments,\n",
    "                    dataset: DatasetReader, dataset_iter: DatasetIter, train_logs: List, misc: Dict):\n",
    "    model.load_state_dict(torch.load(path + '/model.pt'))\n",
    "    optimizer.load_state_dict(torch.load(path + '/optimizer.pt'))\n",
    "    lr_scheduler.load_state_dict(torch.load(path + '/lr_scheduler.pt'))\n",
    "    torch.set_rng_state(torch.load(path + '/rng_state.pt'))\n",
    "    # assert torch.load(path + '/train_args.pt') == train_args\n",
    "    dataset_iter.set_state(dataset.load_iterator(path + '/dataset_iter.pt').get_state())\n",
    "    train_logs.clear()\n",
    "    train_logs += torch.load(path + '/train_logs.pt')\n",
    "    misc.update(torch.load(path + '/misc.pt'))\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_args: TrainArguments,\n",
    "                resume_from: Optional[str] = None,\n",
    "                show_progress: bool = True,\n",
    "                output_dir: str = 'checkpoints', interrupt_lock: Optional[Lock] = None):\n",
    "    interrupted = False\n",
    "    train_logs = []\n",
    "    misc = {'epochs': 0, 'steps': 0, 'last_batch_idx': -1}\n",
    "\n",
    "    total_samples = len(train_args.train_data)\n",
    "    epoch_steps = ceil(total_samples / train_args.batch_size)\n",
    "    assert epoch_steps >= train_args.gradient_accumulation_steps, \\\n",
    "        f'per-epoch steps {epoch_steps} is less than gradient accumulation steps {train_args.gradient_accumulation_steps}'\n",
    "\n",
    "    schedule_steps = ceil(total_samples / train_args.batch_size / train_args.gradient_accumulation_steps)\n",
    "    total_steps = schedule_steps * train_args.num_epochs\n",
    "\n",
    "    optimizer = train_args.optimizer(model.parameters(), **(train_args.optimizer_args if train_args.optimizer_args is not None else {}))\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=train_args.max_lr, div_factor=train_args.max_lr / train_args.start_lr,\n",
    "                                                    total_steps=total_steps,\n",
    "                                                    final_div_factor=train_args.start_lr / train_args.end_lr, pct_start=train_args.warmup_ratio)\n",
    "\n",
    "    dataset_iter = iter(train_args.train_data)\n",
    "    if resume_from is not None:\n",
    "        load_checkpoint(resume_from, model, optimizer, scheduler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "\n",
    "    bar = tqdm.tqdm(total=total_steps, smoothing=1.0, disable=not show_progress)\n",
    "    bar.update(misc['steps'])\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch_num in range(train_args.num_epochs):\n",
    "        if epoch_num < misc['epochs']:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, batch in enumerate(dataset_collate(dataset_iter, train_args.batch_size, train_transform), start=misc['last_batch_idx'] + 1):\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            tokens = batch['token_ids'].to(model.device)\n",
    "            inputs = tokens[:, :-1]\n",
    "            labels = tokens[:, 1:]\n",
    "\n",
    "            positions = batch['position_ids'][:, :-1].to(model.device) if 'position_ids' in batch else None\n",
    "            attn_mask = batch['attn_mask'][:, :-1].to(model.device) if 'attn_mask' in batch and not train_args.ignore_attn_mask else None\n",
    "            loss_mask = batch['loss_mask'][:, :-1].to(model.device) if 'loss_mask' in batch and not train_args.ignore_loss_mask else None\n",
    "\n",
    "            logits, kv_state = model.forward(inputs, position_ids=positions, attn_mask=attn_mask)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=2).view(-1, logits.shape[-1])\n",
    "\n",
    "            loss = (-torch.log(probs[torch.arange(probs.shape[0]), labels.reshape(-1)]))\n",
    "            if loss_mask is not None:\n",
    "                loss = (loss * loss_mask.reshape(-1)).mean() / train_args.gradient_accumulation_steps\n",
    "            else:\n",
    "                loss = loss.mean() / train_args.gradient_accumulation_steps\n",
    "\n",
    "            # brutally clear nan, give up the whole batch\n",
    "            if torch.isnan(loss):\n",
    "                print(f'encountered nan loss at epoch {epoch_num + 1}, batch {batch_idx}')\n",
    "                # optimizer.zero_grad()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % train_args.gradient_accumulation_steps == 0 or (batch_idx + 1) == epoch_steps:\n",
    "                if train_args.gradient_clip_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), train_args.gradient_clip_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                step_time_cost = time.time() - step_start_time\n",
    "                throughput = round(probs.shape[0] / step_time_cost / 1000, 2)\n",
    "\n",
    "                step_stat = {'Loss': f'{loss.item() * train_args.gradient_accumulation_steps:.3f}',\n",
    "                             'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                             'Throughput': f'{throughput} kt/s'}\n",
    "\n",
    "                if show_progress:\n",
    "                    bar.set_description(f'Epoch {epoch_num + 1}')\n",
    "                    bar.set_postfix(step_stat)\n",
    "                else:\n",
    "                    print(', '.join(f'{s[0]}:{s[1]}' for s in step_stat.items()))\n",
    "\n",
    "                scheduler.step()\n",
    "                bar.update(1)\n",
    "                train_logs.append((epoch_num, batch_idx, step_stat))\n",
    "\n",
    "                misc['steps'] += 1\n",
    "                misc['last_batch_idx'] = batch_idx\n",
    "                if train_args.save_steps > 0 and (misc['steps'] % train_args.save_steps) == 0:\n",
    "                    save_checkpoint(output_dir + f'/checkpoint-{misc[\"steps\"]}',\n",
    "                                    model, optimizer, scheduler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "                if interrupt_lock is not None and not interrupt_lock.locked():\n",
    "                    if train_args.save_on_interrupt:\n",
    "                        save_checkpoint(output_dir + f'/checkpoint-{misc[\"steps\"]}',\n",
    "                                        model, optimizer, scheduler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "                    interrupted = True\n",
    "                    break\n",
    "        if interrupted:\n",
    "            break\n",
    "        misc['epochs'] += 1\n",
    "        misc['last_batch_idx'] = -1\n",
    "        dataset_iter = iter(train_args.train_data)\n",
    "    bar.close()\n",
    "\n",
    "    if not interrupted:\n",
    "        save_checkpoint(output_dir + f'/checkpoint-done',\n",
    "                        model, optimizer, scheduler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "\n",
    "    return train_logs\n",
    "\n",
    "\n",
    "def train_model_interruptable(model: nn.Module, train_args: TrainArguments,\n",
    "                              resume_from: Optional[str] = None,\n",
    "                              show_progress: bool = True,\n",
    "                              output_dir: str = 'checkpoints'):\n",
    "    return_value, run_finish = None, False\n",
    "\n",
    "    def return_value_wrapper(func, *args, **kwargs):\n",
    "        nonlocal return_value, run_finish\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            return_value = func(*args, **kwargs)\n",
    "        except Exception as _:\n",
    "            traceback.print_exc()\n",
    "        run_finish = True\n",
    "\n",
    "    interrupt_lock = Lock()\n",
    "    interrupt_lock.acquire()\n",
    "    thread = Thread(target=return_value_wrapper, args=(train_model, model, train_args),\n",
    "                    kwargs={'resume_from': resume_from, 'show_progress': show_progress, 'output_dir': output_dir, 'interrupt_lock': interrupt_lock})\n",
    "    thread.start()\n",
    "    while not run_finish:\n",
    "        try:\n",
    "            time.sleep(0.1)\n",
    "        except KeyboardInterrupt as _:\n",
    "            interrupt_lock.release()\n",
    "            break\n",
    "    thread.join()\n",
    "\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281e39ed7414acc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:01.570210758Z",
     "start_time": "2023-10-27T02:48:01.526297096Z"
    }
   },
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    g_train_args = TrainArguments(\n",
    "        num_epochs=1000, batch_size=8, gradient_accumulation_steps=1,\n",
    "        optimizer=torch.optim.AdamW, optimizer_args=None,\n",
    "        start_lr=1e-5, max_lr=1e-3, end_lr=1e-6, warmup_ratio=0.1,\n",
    "        gradient_clip_norm=1.0,\n",
    "        train_data=g_train_data, ignore_attn_mask=False, ignore_loss_mask=False,\n",
    "        # eval_data=None, eval_steps=-1,\n",
    "        # eval_generate_prompt=None, eval_generate_steps=-1,\n",
    "        save_steps=-1,\n",
    "        save_on_interrupt=False,\n",
    "    )\n",
    "else:\n",
    "    g_train_args = TrainArguments(\n",
    "        num_epochs=2, batch_size=12, gradient_accumulation_steps=8,\n",
    "        optimizer=torch.optim.AdamW, optimizer_args=None,\n",
    "        start_lr=5e-5, max_lr=1e-3, end_lr=1e-4, warmup_ratio=0.1,\n",
    "        gradient_clip_norm=0.7,\n",
    "        train_data=g_train_data, ignore_attn_mask=True, ignore_loss_mask=True,\n",
    "        # eval_data=None, eval_steps=-1,\n",
    "        # eval_generate_prompt=None, eval_generate_steps=-1,\n",
    "        save_steps=1000,\n",
    "        save_on_interrupt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "673871354eec2c12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T02:48:11.558287788Z",
     "start_time": "2023-10-27T02:48:01.570135249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/22618 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f8a88bebcbc478e9aabcc71c81925e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encountered nan loss at epoch 1, batch 8\n",
      "encountered nan loss at epoch 1, batch 9\n",
      "encountered nan loss at epoch 1, batch 10\n",
      "encountered nan loss at epoch 1, batch 11\n",
      "encountered nan loss at epoch 1, batch 12\n",
      "encountered nan loss at epoch 1, batch 13\n",
      "encountered nan loss at epoch 1, batch 14\n",
      "encountered nan loss at epoch 1, batch 15\n",
      "encountered nan loss at epoch 1, batch 16\n",
      "encountered nan loss at epoch 1, batch 17\n",
      "encountered nan loss at epoch 1, batch 18\n",
      "encountered nan loss at epoch 1, batch 19\n",
      "encountered nan loss at epoch 1, batch 20\n",
      "encountered nan loss at epoch 1, batch 21\n",
      "encountered nan loss at epoch 1, batch 22\n",
      "encountered nan loss at epoch 1, batch 23\n",
      "encountered nan loss at epoch 1, batch 24\n",
      "encountered nan loss at epoch 1, batch 25\n",
      "encountered nan loss at epoch 1, batch 26\n",
      "encountered nan loss at epoch 1, batch 27\n",
      "encountered nan loss at epoch 1, batch 28\n",
      "encountered nan loss at epoch 1, batch 29\n",
      "encountered nan loss at epoch 1, batch 30\n",
      "encountered nan loss at epoch 1, batch 31\n"
     ]
    }
   ],
   "source": [
    "if C_DEBUG:\n",
    "    global_config['enable_flash_attn'] = True\n",
    "    global_config['enable_torch_attn'] = False\n",
    "    g_train_logs = train_model_interruptable(g_model, g_train_args, resume_from=None,\n",
    "                                             show_progress=True, output_dir='checkpoints/debug_output')\n",
    "else:\n",
    "    global_config['enable_flash_attn'] = True\n",
    "    global_config['enable_torch_attn'] = False\n",
    "    g_train_logs = train_model_interruptable(g_model, g_train_args, resume_from=None,\n",
    "                                             show_progress=True, output_dir='checkpoints/debug_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ca07cd31cd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_logs(train_logs):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].plot([float(l[2]['Loss']) for l in train_logs])\n",
    "    axes[1].plot([float(l[2]['LR']) for l in train_logs])\n",
    "    axes[2].plot([float(l[2]['Throughput'][:-5]) for l in train_logs])\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[1].set_title('Learning Rate')\n",
    "    axes[2].set_title('Throughput (kt/s)')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.autoscale()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_list = [\n",
    "    'checkpoints/train-round1/checkpoint-19022/train_logs.pt',\n",
    "]\n",
    "checkpoint_list = [torch.load(c) for c in checkpoint_list]\n",
    "\n",
    "for c in checkpoint_list:\n",
    "    plot_train_logs(c)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "471f560981b69d48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e800403549c98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, temperature, top_p, rep_penalty,\n",
    "             max_new_tokens=20, total_tokens=None,\n",
    "             end_tokens=None,\n",
    "             enable_kv_cache=True):\n",
    "    model.eval()\n",
    "\n",
    "    feed_tokens = tokenizer.encode(prompt)\n",
    "    all_tokens = feed_tokens.copy()\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(feed_tokens))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            position_ids = None if kv_cache is None else torch.tensor([[len(all_tokens) - 1]]).to(model.device)\n",
    "            logits, kv_cache = model.forward(\n",
    "                torch.tensor([feed_tokens if enable_kv_cache else all_tokens]).to(model.device),\n",
    "                position_ids=position_ids,\n",
    "                kv_cache=kv_cache)\n",
    "            logits = logits[0][-1].cpu()\n",
    "            if not enable_kv_cache:\n",
    "                kv_cache = None\n",
    "\n",
    "            # apply repetition penalty\n",
    "            logits_rep = torch.gather(logits, 0, torch.tensor(all_tokens))\n",
    "            logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "            logits.scatter_(0, torch.tensor(all_tokens), logits_rep)\n",
    "\n",
    "            # apply temperature\n",
    "            logits /= max(temperature, 1e-6)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # apply top-p\n",
    "            ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "            cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "            top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "            ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "            sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "            all_tokens.append(sampled_index)\n",
    "            feed_tokens = [sampled_index]\n",
    "\n",
    "            if end_tokens is not None and sampled_index in end_tokens:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c17f13926df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in g_tokenizer.decode(next(iter(g_train_data))['token_ids'].tolist()).split('</s>')[:3]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc924753b5acfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()\n",
    "global_config['enable_flash_attn'] = True\n",
    "global_config['enable_torch_attn'] = False\n",
    "result = generate(g_model, g_tokenizer, 'Q: How to implement a quicksort in python?\\nA:',\n",
    "                  temperature=1.0, top_p=0.3, rep_penalty=1.1,\n",
    "                  total_tokens=128,\n",
    "                  end_tokens=g_tokenizer.encode('<reserved_0>'),\n",
    "                  enable_kv_cache=False)\n",
    "print(result)\n",
    "print(f'{time.time() - a:.3f} sec(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05a9cf2d3f45f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

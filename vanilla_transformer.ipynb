{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-10-07T22:54:37.354296Z",
     "start_time": "2023-10-07T22:54:36.520697Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import WordTokenizer, CharTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "import random\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e76bcba3a904db",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:54:37.429658Z",
     "start_time": "2023-10-07T22:54:37.353007Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../../Jupyter/NLP/TinyStoriesV2-GPT4-valid-chunks.json', 'r') as file:\n",
    "    lines = [l.strip() for l in json.load(file) if l.strip() != '']\n",
    "    raw_text = '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb151752ec125fa",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:54:37.434347Z",
     "start_time": "2023-10-07T22:54:37.430324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network definition\n",
    "C_SEQ_LEN = 512\n",
    "C_VOCAB_SIZE = 4096\n",
    "C_HIDDEN_SIZE = 256\n",
    "C_NUM_HEADS = 2\n",
    "C_NUM_LAYERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33702a8e6613f742",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:54:41.223794Z",
     "start_time": "2023-10-07T22:54:37.798689Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(raw_text, vocab_size=C_VOCAB_SIZE, reserved_vocab=['<s>', '</s>', '<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf7fc22d44a2d77",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:54:45.075208Z",
     "start_time": "2023-10-07T22:54:41.238700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9967516693080476"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eval_vocab_coverage(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/27629 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34105419a5f04820a88fa3e3ad1926d1"
      },
      "application/json": {
       "n": 0,
       "total": 27629,
       "elapsed": 0.004194974899291992,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_samples = []\n",
    "for l in tqdm.tqdm(lines):\n",
    "    encoded_samples += tokenizer.encode('<s>' + l + '</s>')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:54:48.278944Z",
     "start_time": "2023-10-07T22:54:45.072878Z"
    }
   },
   "id": "6236ac3649c35eb1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "for i in range(0, len(encoded_samples), C_SEQ_LEN):\n",
    "    chunks.append(encoded_samples[i:i + C_SEQ_LEN])\n",
    "chunks.pop(-1)\n",
    "all(len(c) == C_SEQ_LEN for c in chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:54:48.396324Z",
     "start_time": "2023-10-07T22:54:48.320765Z"
    }
   },
   "id": "7d1685466c786f0f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb60031d199ad9a6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:54:48.401385Z",
     "start_time": "2023-10-07T22:54:48.396903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 128])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_seq = torch.tensor([tokenizer.encode(raw_text[:10000])[:128 * 8]]).view((-1, 128))\n",
    "debug_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 128])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_seq = torch.tensor(chunks)\n",
    "# train_seq.shape\n",
    "train_seq = debug_seq\n",
    "train_seq.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:54:48.406775Z",
     "start_time": "2023-10-07T22:54:48.399876Z"
    }
   },
   "id": "e442cf97d0bbe96e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "math_tokenizer = CharTokenizer('0123456789+-*/=._', 100)\n",
    "print(len(math_tokenizer.get_vocab_mapping()))\n",
    "samples = []\n",
    "for i in range(32000 * 8):\n",
    "    a = random.randint(1, 999)\n",
    "    b = random.randint(1, 999)\n",
    "    s = a + b\n",
    "    t = f'{a}+{b}={s}'\n",
    "    samples.append(t)\n",
    "max_sample_len = len(max(samples, key=lambda i: len(i)))\n",
    "samples = [s + '_' * (max_sample_len - len(s)) for s in samples]\n",
    "train_seq = torch.stack([torch.tensor(math_tokenizer.encode(s)) for s in samples])\n",
    "train_seq.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "452c8b0c70869ba8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "math_tokenizer.decode(train_seq[0].tolist())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50f4430e3b69ca1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "C_SEQ_LEN = 12\n",
    "C_VOCAB_SIZE = 18"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86f5eec2d44ff145"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34d4a7cb7e49d5f5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T23:02:19.487717Z",
     "start_time": "2023-10-07T23:02:19.480544Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_attn_mask(custom_attn_mask: torch.Tensor):\n",
    "    B, T = custom_attn_mask.shape\n",
    "    mask = custom_attn_mask.unsqueeze(1).repeat((1, T, 1))\n",
    "    seq_index_mask = (mask == custom_attn_mask[:, torch.arange(T)].view(B, T, 1))\n",
    "    return seq_index_mask & (torch.tril(mask) > 0)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size // num_heads)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size // num_heads)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size // num_heads)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]):\n",
    "        if attn_mask is not None:\n",
    "            causal_mask = torch.where(expand_attn_mask(attn_mask), 0, 1e9)\n",
    "        else:\n",
    "            causal_mask = torch.where(expand_attn_mask(torch.ones(x.shape[:2])), 0, 1e9)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        attn_score = (q @ k.permute(0, 2, 1) / (self.hidden_size ** 0.5)) - causal_mask\n",
    "\n",
    "        return torch.softmax(attn_score, dim=2) @ v\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(num_heads, hidden_size) for _ in range(num_heads)])\n",
    "        self.o_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]):\n",
    "        return self.o_proj(torch.concat([a(x, attn_mask) for a in self.attn_heads], dim=2))\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads, hidden_size)\n",
    "        self.up_proj = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        self.down_proj = nn.Linear(hidden_size * 4, hidden_size)\n",
    "        self.ln_mha = nn.LayerNorm(hidden_size)\n",
    "        self.ln_ffn = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]):\n",
    "        mha_output = self.ln_mha(x + self.mha(x, attn_mask))\n",
    "        ffn_output = self.down_proj(torch.relu(self.up_proj(mha_output)))\n",
    "        return self.ln_ffn(mha_output + ffn_output)\n",
    "\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, hidden_size: int, seq_len: int, ):\n",
    "        super().__init__()\n",
    "        self.sem_embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_embed = nn.Embedding(seq_len, hidden_size)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(num_heads, hidden_size) for _ in range(num_layers)])\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, seq: torch.Tensor,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                attn_mask: Optional[torch.Tensor] = None):\n",
    "\n",
    "        if position_ids is None:\n",
    "            seq_len = seq.shape[1]\n",
    "            pos_embed = self.pos_embed(torch.arange(0, seq_len, 1))\n",
    "        else:\n",
    "            pos_embed = self.pos_embed(position_ids)\n",
    "\n",
    "        hidden = self.sem_embed(seq) + pos_embed\n",
    "        for decoder in self.decoder_layers:\n",
    "            hidden = decoder(hidden, attn_mask)\n",
    "        logits = self.lm_head(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d124589733f9ed1c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T23:41:27.149573Z",
     "start_time": "2023-10-07T23:41:27.094682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5519104\n"
     ]
    },
    {
     "data": {
      "text/plain": "ToyTransformer(\n  (sem_embed): Embedding(4096, 256)\n  (pos_embed): Embedding(4096, 256)\n  (decoder_layers): ModuleList(\n    (0-2): 3 x DecoderLayer(\n      (mha): MultiHeadAttention(\n        (attn_heads): ModuleList(\n          (0-1): 2 x AttentionHead(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n          )\n        )\n        (o_proj): Linear(in_features=256, out_features=256, bias=True)\n      )\n      (up_proj): Linear(in_features=256, out_features=1024, bias=True)\n      (down_proj): Linear(in_features=1024, out_features=256, bias=True)\n      (ln_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln_ffn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=256, out_features=4096, bias=True)\n)"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ToyTransformer(C_VOCAB_SIZE, C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN * 8)\n",
    "print('Total parameters:', sum([t.numel() for t in model.parameters()]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6704e7b3092a4c0c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T23:41:27.772150Z",
     "start_time": "2023-10-07T23:41:27.767747Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=2000, final_div_factor=1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "train_seq_long = train_seq.view(1, -1)\n",
    "attn_mask_long = torch.concat([torch.tensor([i] * 128) for i in range(1, 9)]).view(1, -1)\n",
    "pos_ids_long = torch.concat([torch.tensor(list(range(128))) for _ in range(1, 9)]).view(1, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T23:41:32.961140Z",
     "start_time": "2023-10-07T23:41:32.943255Z"
    }
   },
   "id": "e54129f2ef668046"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T23:41:43.090715Z",
     "start_time": "2023-10-07T23:41:33.405368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step 1 - Loss: 8.510 LR: 0.0 Throughput: 577.69 kts\n",
      "Epoch 1 Step 1 - Loss: 5.773 LR: 0.0 Throughput: 687.28 kts\n",
      "Epoch 2 Step 1 - Loss: 5.000 LR: 0.0 Throughput: 696.53 kts\n",
      "Epoch 3 Step 1 - Loss: 4.682 LR: 0.0 Throughput: 462.86 kts\n",
      "Epoch 4 Step 1 - Loss: 4.385 LR: 0.0 Throughput: 651.24 kts\n",
      "Epoch 5 Step 1 - Loss: 4.165 LR: 0.0 Throughput: 614.38 kts\n",
      "Epoch 6 Step 1 - Loss: 3.906 LR: 0.0 Throughput: 624.3 kts\n",
      "Epoch 7 Step 1 - Loss: 3.650 LR: 0.0 Throughput: 690.94 kts\n",
      "Epoch 8 Step 1 - Loss: 3.457 LR: 0.0 Throughput: 671.34 kts\n",
      "Epoch 9 Step 1 - Loss: 3.330 LR: 0.0 Throughput: 701.35 kts\n",
      "Epoch 10 Step 1 - Loss: 3.218 LR: 0.0 Throughput: 691.76 kts\n",
      "Epoch 11 Step 1 - Loss: 3.099 LR: 0.0 Throughput: 665.85 kts\n",
      "Epoch 12 Step 1 - Loss: 2.997 LR: 0.0 Throughput: 676.45 kts\n",
      "Epoch 13 Step 1 - Loss: 2.928 LR: 0.0 Throughput: 698.01 kts\n",
      "Epoch 14 Step 1 - Loss: 2.868 LR: 0.0 Throughput: 647.47 kts\n",
      "Epoch 15 Step 1 - Loss: 2.806 LR: 0.0 Throughput: 693.06 kts\n",
      "Epoch 16 Step 1 - Loss: 2.749 LR: 0.0 Throughput: 687.79 kts\n",
      "Epoch 17 Step 1 - Loss: 2.693 LR: 0.0 Throughput: 659.78 kts\n",
      "Epoch 18 Step 1 - Loss: 2.629 LR: 0.0 Throughput: 687.02 kts\n",
      "Epoch 19 Step 1 - Loss: 2.568 LR: 0.0 Throughput: 702.6 kts\n",
      "Epoch 20 Step 1 - Loss: 2.505 LR: 0.0 Throughput: 649.98 kts\n",
      "Epoch 21 Step 1 - Loss: 2.433 LR: 0.0 Throughput: 690.42 kts\n",
      "Epoch 22 Step 1 - Loss: 2.351 LR: 0.0 Throughput: 692.42 kts\n",
      "Epoch 23 Step 1 - Loss: 2.244 LR: 0.0 Throughput: 652.25 kts\n",
      "Epoch 24 Step 1 - Loss: 2.136 LR: 0.0 Throughput: 697.67 kts\n",
      "Epoch 25 Step 1 - Loss: 2.043 LR: 0.0 Throughput: 695.34 kts\n",
      "Epoch 26 Step 1 - Loss: 1.952 LR: 0.0 Throughput: 670.8 kts\n",
      "Epoch 27 Step 1 - Loss: 1.858 LR: 0.0 Throughput: 722.48 kts\n",
      "Epoch 28 Step 1 - Loss: 1.750 LR: 0.0 Throughput: 684.68 kts\n",
      "Epoch 29 Step 1 - Loss: 1.639 LR: 0.0 Throughput: 666.5 kts\n",
      "Epoch 30 Step 1 - Loss: 1.532 LR: 0.0 Throughput: 710.49 kts\n",
      "Epoch 31 Step 1 - Loss: 1.427 LR: 0.0 Throughput: 701.73 kts\n",
      "Epoch 32 Step 1 - Loss: 1.325 LR: 0.0 Throughput: 655.58 kts\n",
      "Epoch 33 Step 1 - Loss: 1.227 LR: 0.0 Throughput: 702.96 kts\n",
      "Epoch 34 Step 1 - Loss: 1.145 LR: 0.0 Throughput: 706.97 kts\n",
      "Epoch 35 Step 1 - Loss: 1.106 LR: 0.0 Throughput: 672.0 kts\n",
      "Epoch 36 Step 1 - Loss: 1.002 LR: 0.0 Throughput: 692.34 kts\n",
      "Epoch 37 Step 1 - Loss: 0.882 LR: 0.0 Throughput: 679.55 kts\n",
      "Epoch 38 Step 1 - Loss: 0.834 LR: 0.0 Throughput: 670.92 kts\n",
      "Epoch 39 Step 1 - Loss: 0.730 LR: 0.0 Throughput: 704.98 kts\n",
      "Epoch 40 Step 1 - Loss: 0.680 LR: 0.0 Throughput: 706.17 kts\n",
      "Epoch 41 Step 1 - Loss: 0.605 LR: 0.0 Throughput: 677.77 kts\n",
      "Epoch 42 Step 1 - Loss: 0.550 LR: 0.0 Throughput: 679.0 kts\n",
      "Epoch 43 Step 1 - Loss: 0.492 LR: 0.0 Throughput: 682.44 kts\n",
      "Epoch 44 Step 1 - Loss: 0.439 LR: 0.0 Throughput: 667.25 kts\n",
      "Epoch 45 Step 1 - Loss: 0.394 LR: 0.0 Throughput: 690.79 kts\n",
      "Epoch 46 Step 1 - Loss: 0.351 LR: 0.0 Throughput: 675.06 kts\n",
      "Epoch 47 Step 1 - Loss: 0.309 LR: 0.0 Throughput: 683.92 kts\n",
      "Epoch 48 Step 1 - Loss: 0.275 LR: 0.0 Throughput: 701.16 kts\n",
      "Epoch 49 Step 1 - Loss: 0.243 LR: 0.0 Throughput: 703.15 kts\n",
      "Epoch 50 Step 1 - Loss: 0.214 LR: 0.0 Throughput: 654.08 kts\n",
      "Epoch 51 Step 1 - Loss: 0.188 LR: 0.0 Throughput: 690.77 kts\n",
      "Epoch 52 Step 1 - Loss: 0.166 LR: 0.0 Throughput: 680.52 kts\n",
      "Epoch 53 Step 1 - Loss: 0.146 LR: 0.0 Throughput: 658.45 kts\n",
      "Epoch 54 Step 1 - Loss: 0.128 LR: 0.0 Throughput: 680.48 kts\n",
      "Epoch 55 Step 1 - Loss: 0.113 LR: 0.0 Throughput: 717.25 kts\n",
      "Epoch 56 Step 1 - Loss: 0.100 LR: 0.0 Throughput: 662.81 kts\n",
      "Epoch 57 Step 1 - Loss: 0.088 LR: 0.0 Throughput: 662.2 kts\n",
      "Epoch 58 Step 1 - Loss: 0.078 LR: 0.0 Throughput: 689.02 kts\n",
      "Epoch 59 Step 1 - Loss: 0.070 LR: 0.0 Throughput: 669.29 kts\n",
      "Epoch 60 Step 1 - Loss: 0.062 LR: 0.0 Throughput: 681.46 kts\n",
      "Epoch 61 Step 1 - Loss: 0.056 LR: 0.0 Throughput: 704.2 kts\n",
      "Epoch 62 Step 1 - Loss: 0.051 LR: 0.0 Throughput: 666.38 kts\n",
      "Epoch 63 Step 1 - Loss: 0.046 LR: 0.0 Throughput: 707.18 kts\n",
      "Epoch 64 Step 1 - Loss: 0.042 LR: 0.0 Throughput: 689.48 kts\n",
      "Epoch 65 Step 1 - Loss: 0.039 LR: 0.0 Throughput: 673.02 kts\n",
      "Epoch 66 Step 1 - Loss: 0.036 LR: 0.0 Throughput: 697.56 kts\n",
      "Epoch 67 Step 1 - Loss: 0.033 LR: 0.0 Throughput: 677.99 kts\n",
      "Epoch 68 Step 1 - Loss: 0.031 LR: 0.0 Throughput: 665.5 kts\n",
      "Epoch 69 Step 1 - Loss: 0.029 LR: 0.0 Throughput: 698.6 kts\n",
      "Epoch 70 Step 1 - Loss: 0.027 LR: 0.0 Throughput: 698.92 kts\n",
      "Epoch 71 Step 1 - Loss: 0.025 LR: 0.0 Throughput: 654.45 kts\n",
      "Epoch 72 Step 1 - Loss: 0.024 LR: 0.0 Throughput: 692.32 kts\n",
      "Epoch 73 Step 1 - Loss: 0.022 LR: 0.0 Throughput: 678.93 kts\n",
      "Epoch 74 Step 1 - Loss: 0.021 LR: 0.0 Throughput: 653.41 kts\n",
      "Epoch 75 Step 1 - Loss: 0.020 LR: 0.0 Throughput: 688.92 kts\n",
      "Epoch 76 Step 1 - Loss: 0.019 LR: 0.0 Throughput: 700.52 kts\n",
      "Epoch 77 Step 1 - Loss: 0.018 LR: 0.0 Throughput: 666.74 kts\n",
      "Epoch 78 Step 1 - Loss: 0.017 LR: 0.0 Throughput: 706.73 kts\n",
      "Epoch 79 Step 1 - Loss: 0.017 LR: 0.0 Throughput: 719.65 kts\n",
      "Epoch 80 Step 1 - Loss: 0.016 LR: 0.0 Throughput: 673.94 kts\n",
      "Epoch 81 Step 1 - Loss: 0.016 LR: 0.0 Throughput: 688.45 kts\n",
      "Epoch 82 Step 1 - Loss: 0.015 LR: 0.0 Throughput: 692.25 kts\n",
      "Epoch 83 Step 1 - Loss: 0.014 LR: 0.0 Throughput: 658.72 kts\n",
      "Epoch 84 Step 1 - Loss: 0.014 LR: 0.0 Throughput: 703.02 kts\n",
      "Epoch 85 Step 1 - Loss: 0.014 LR: 0.0 Throughput: 702.05 kts\n",
      "Epoch 86 Step 1 - Loss: 0.013 LR: 0.0 Throughput: 668.05 kts\n",
      "Epoch 87 Step 1 - Loss: 0.013 LR: 0.0 Throughput: 683.5 kts\n",
      "Epoch 88 Step 1 - Loss: 0.012 LR: 0.0 Throughput: 684.56 kts\n",
      "Epoch 89 Step 1 - Loss: 0.012 LR: 0.0 Throughput: 678.82 kts\n",
      "Epoch 90 Step 1 - Loss: 0.012 LR: 0.0 Throughput: 700.69 kts\n",
      "Epoch 91 Step 1 - Loss: 0.012 LR: 0.0 Throughput: 678.29 kts\n",
      "Epoch 92 Step 1 - Loss: 0.011 LR: 0.0 Throughput: 648.3 kts\n",
      "Epoch 93 Step 1 - Loss: 0.011 LR: 0.0 Throughput: 684.5 kts\n",
      "Epoch 94 Step 1 - Loss: 0.011 LR: 0.0 Throughput: 684.9 kts\n",
      "Epoch 95 Step 1 - Loss: 0.011 LR: 0.0 Throughput: 673.25 kts\n",
      "Epoch 96 Step 1 - Loss: 0.010 LR: 0.0 Throughput: 699.95 kts\n",
      "Epoch 97 Step 1 - Loss: 0.010 LR: 0.0 Throughput: 703.49 kts\n",
      "Epoch 98 Step 1 - Loss: 0.010 LR: 0.0 Throughput: 657.58 kts\n",
      "Epoch 99 Step 1 - Loss: 0.010 LR: 0.0 Throughput: 677.59 kts\n"
     ]
    }
   ],
   "source": [
    "C_BATCH_SIZE = 128\n",
    "for epoch_num in range(100):\n",
    "    for batch_i in list(range(0, len(train_seq), C_BATCH_SIZE)):\n",
    "        step_start_time = time.time()\n",
    "\n",
    "        inputs = train_seq[batch_i:batch_i + C_BATCH_SIZE, :-1]\n",
    "        labels = train_seq[batch_i:batch_i + C_BATCH_SIZE, 1:]\n",
    "        logits = model.forward(inputs, pos_ids_long[:, :-1], attn_mask_long[:, :-1])\n",
    "        probs = torch.softmax(logits, dim=2)  # BSZ * SEQ * VOCAB\n",
    "        probs_flat = probs.view(-1, C_VOCAB_SIZE)\n",
    "        loss = (-torch.log(probs_flat[torch.arange(probs_flat.shape[0]), labels.reshape(-1)])).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step_time_cost = time.time() - step_start_time\n",
    "        throughput = round((C_BATCH_SIZE * C_SEQ_LEN) / step_time_cost / 1000, 2)\n",
    "        print(\n",
    "            f'Epoch {epoch_num} Step {batch_i // C_BATCH_SIZE + 1} - Loss: {loss.item():.3f} LR: {0.0:.3} '\n",
    "            f'Throughput: {throughput} kts')\n",
    "        # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e800403549c98d2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T23:41:44.420497Z",
     "start_time": "2023-10-07T23:41:44.414606Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(tokenizer, prompt, temperature, top_p, rep_penalty, max_new_tokens=20, total_tokens=None):\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(tokens))\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.forward(torch.tensor([tokens]))[0][-1]\n",
    "\n",
    "        # apply repetition penalty\n",
    "        logits_rep = torch.gather(logits, 0, torch.tensor(tokens))\n",
    "        logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "        logits.scatter_(0, torch.tensor(tokens), logits_rep)\n",
    "\n",
    "        # apply temperature\n",
    "        logits /= max(temperature, 1e-6)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "        # apply top-p\n",
    "        ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "        cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "        top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "        ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "        sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "        tokens.append(sampled_index)\n",
    "    # print(tokens)\n",
    "    return tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd06574fc4aae5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate(math_tokenizer, f\"1+1=\", temperature=1.0, top_p=0.01, rep_penalty=1.0, total_tokens=12).rstrip('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct_vs_total = [0, 0]\n",
    "for a in range(1, 20):\n",
    "    for b in range(1, 20):\n",
    "        s = generate(math_tokenizer, f\"{a}/{b}=\", temperature=1.0, top_p=0.01, rep_penalty=1.0,\n",
    "                     max_new_tokens=3).rstrip('_')\n",
    "        r = int(s[s.rfind('=') + 1:])\n",
    "        correct_vs_total[0] += (a // b) == r\n",
    "        correct_vs_total[1] += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da95e6c5756329ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct_vs_total"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6da265cd850c26c"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "'sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\nTom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my'"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(tokenizer, 'sunny place', 1.0, 0.05, 1.0, total_tokens=128)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T23:41:47.418931Z",
     "start_time": "2023-10-07T23:41:46.682531Z"
    }
   },
   "id": "1bc924753b5acfc5"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "'<unk> don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\nOnce upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\nTom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my ball fell into the pit.\"\\nSam and Tom went close to the pit. They were scared, but they wanted to find the red ball. They looked into the pit, but it was too dark to see. Tom said, \"We must go in and search for my ball.\"\\nThey went into the pit to search. It was dark and scary. They could not find the ball. They tried to get out, but the pit was too deep. Tom and Sam were stuck in the pit. They called for help, but no one could hear them. They were sad and scared, and they never got out of the pit.\\nTom and Lily were playing with their toys in the living room. They liked to build towers and bridges with their blocks and cars. Tom was very proud of his tall tower. He wanted to make it even taller, so he reached for more blocks.\\n\"Tom, can I have some blocks too?\" Lily asked. She wanted to make a bridge for her cars.\\n\"No, these are mine. Go find your own,\" Tom said. He did not want to share with his sister. He pulled the blocks closer to him.\\nLily felt sad and angry. She did not think Tom was being nice. She looked at his tower and had an idea. She decided to pull one of the blocks at the bottom of the tower.\\nSuddenly, the tower fell down with a loud crash. All the blocks and cars scattered on the floor. Tom and Lily were shocked. They felt the floor shake and heard a <unk>. It was an earthquake!\\n\"Mommy! Daddy!\" they cried. They were scared and ran to their parents, who were in the kitchen.\\n\"Are you okay, kids?\" Mommy asked. She hugged them and checked if they were hurt.\\n\"We\\'re okay, Mommy. But our toys are broken,\" Lily said.\\n\"I\\'m sorry, Lily. But toys are not important. You are important. We are safe and together. That\\'s what <unk>,\" Mommy said.\\nTom felt sorry for what he did. He realized he '"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_seq[0].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T23:31:04.554529Z",
     "start_time": "2023-10-07T23:31:04.544837Z"
    }
   },
   "id": "40531b0cf58345f8"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "'sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\nTom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my'"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(debug_seq[1].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T23:31:07.184496Z",
     "start_time": "2023-10-07T23:31:07.174082Z"
    }
   },
   "id": "63dfb4a3c8dcbb08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seq_len = 2\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "causal_mask = causal_mask.masked_fill(causal_mask == 1, 1e9)\n",
    "causal_mask"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c07cb1efae55bca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.where(torch.tensor([True, False]), 1, 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d944c199527e728"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "191841b1a87d1069"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:48.294230578Z",
     "start_time": "2023-10-29T05:27:45.528238141Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import TRIETokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "from typing import *\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from flash_attn import flash_attn_func\n",
    "from flash_attn_triton import flash_attn_func as flash_attn_func_triton\n",
    "from dataloader import DatasetReader, DatasetIter, SingleDatasetReader, MultiDatasetsReader\n",
    "from math import ceil\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import bitsandbytes as bnb\n",
    "from threading import Lock, Thread\n",
    "import traceback\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from enum import Enum\n",
    "from modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33702a8e6613f742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:48.870565515Z",
     "start_time": "2023-10-29T05:27:48.508362623Z"
    }
   },
   "outputs": [],
   "source": [
    "g_tokenizer = TRIETokenizer('llama_vocab_pruned_32k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb151752ec125fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:49.031522918Z",
     "start_time": "2023-10-29T05:27:48.980481018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network definition\n",
    "C_SEQ_LEN = 2048\n",
    "C_HIDDEN_SIZE = 768\n",
    "C_NUM_HEADS = 12\n",
    "C_NUM_LAYERS = 12\n",
    "\n",
    "C_DEVICE = torch.device('cuda')\n",
    "C_DTYPE = torch.bfloat16\n",
    "\n",
    "C_DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799b78921dbff1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:49.512166422Z",
     "start_time": "2023-10-29T05:27:49.505966645Z"
    }
   },
   "outputs": [],
   "source": [
    "if not C_DEBUG:\n",
    "    # g_train_data = MultiDatasetsReader([\n",
    "    #     SingleDatasetReader('datasets/minipile_train.bin'),\n",
    "    #     SingleDatasetReader('datasets/enwiki_train.bin'),\n",
    "    #     SingleDatasetReader('datasets/tinytextbooks_train.bin'),\n",
    "    # ], seed=0)\n",
    "    g_train_data = MultiDatasetsReader([\n",
    "        SingleDatasetReader('datasets/tinystories_train_masked.bin'),\n",
    "    ], seed=0)\n",
    "else:\n",
    "    g_train_data = MultiDatasetsReader([\n",
    "        SingleDatasetReader('datasets/debug_data_masked.bin'),\n",
    "        SingleDatasetReader('datasets/debug_data_masked.bin'),\n",
    "    ], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d770a2be845a70d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:49.892039581Z",
     "start_time": "2023-10-29T05:27:49.889642568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 287582\n",
      "Sample length: 2048\n",
      "Train tokens: 588967936\n"
     ]
    }
   ],
   "source": [
    "print('Train samples:', len(g_train_data))\n",
    "print('Sample length:', len(next(iter(g_train_data))['token_ids']))\n",
    "print('Train tokens:', len(g_train_data) * len(next(iter(g_train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8dbb96cab10318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:50.930239022Z",
     "start_time": "2023-10-29T05:27:50.923510288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: <s>Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
      "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
      "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
      "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
      "And that's how Ben found an amazing vase in the store!</s><s>Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
      "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his friend, the duck. \"Hi, Ollie!\" said the duck. \"Hi, duck!\" said Ollie. \"I need to hurry and catch fish for my family.\"\n",
      "While Ollie was catching fish, he found a big shiny stone. He thought, \"This is not a fish, but it is so pretty!\" Ollie took the shiny stone home to show his family. They all looked at the shiny stone and smiled. The shiny stone made everyone happy, and they forgot about the fish for dinner.</s><s>One day, a little boy named Tim went to the park. He saw a big tiger. The tiger was not mean, but very easy to play with. Tim and the tiger played all day. They had lots of fun.\n",
      "Then, something unexpected happened. The tiger started to shake. Tim was scared. He did not know what was going on. But then, the tiger turned into a nice dog. Tim was very surprised.\n",
      "Tim and the dog played together now. They were very happy. The dog was easy to play with too. At the end of the day, Tim went home with his new friend.</s><s>Once upon a time there was a friendly little boy called Bob. Bob loved to pick flowers and look for birds. One day he decided to go outside with his friends to pick some more flowers. \n",
      "He suddenly noticed something weird on the ground. It was a big, green thumb! It was so big, Bob had never seen one before. Bob curiously leaned in to take a better look. He told his friends: \"look everyone, I picked up this big thumb! What do we do with it?\"\n",
      "His friends were very excited. They told him to pick it up and take it home to show his family. So Bob carefully picked up the friendly thumb and carried it back home. When he arrived, Bob happily showed the thumb to his family. His dad was amazed and hugged Bob to show his appreciation.\n",
      "From that day on Bob always kept the big, friendly thumb with him as a reminder that special things can be found anywhere.</s><s>Once upon a time, in a small house, there lived a little girl named Lucy. Lucy loved the color orange. She had an orange dress, an orange ball, and even an orange cat. One day, Lucy met a new friend. This friend was not like other friends. It was a spirit. The spirit was very nice and liked to play with Lucy.\n",
      "One day, Lucy and the spirit were playing with her orange ball. They were having so much fun. Then, Lucy's mom called her for dinner. Lucy said to the spirit, \"I have to go eat now. Will you play with me later?\" The spirit nodded and smiled.\n",
      "At dinner, Lucy told her mom about the spirit. But her mom did not believe her. She said, \"Spirits are not real, Lucy. You have a big imagination.\" Lucy felt sad that her mom did not believe her. After dinner, she went back to play with the spirit. They played with the orange ball and had lots of fun. Lucy knew that even if others ignore her friend, the spirit was real and they could play together.</s><s>One day, a boy named Tim went to the park to play. He saw his friend, Sam, playing with a toy car. Tim wanted to join and play with Sam. They both played with the toy car, and it went fast. The car had a battery inside that made it go.\n",
      "Tim said, \"Sam, the battery is tight in the car. It will not fall out.\" Sam smiled and they kept playing. They raced the car around the park, laughing and having fun.\n",
      "But then, something unexpected happened. A big dog came and took the toy car in its mouth! Tim and Sam were scared, but the dog just wanted to play too. They all played together, and the dog was very gentle with the car. In the end, Tim, Sam, and the dog became good friends.</s><s>Once upon a time, there was a sailor named Tom. Tom had a big boat. He liked to sail on the sea. One day, Tom saw a little fish. The fish was sad. It was lost and wanted to go home.\n",
      "Tom said, \"I can help you, little fish. You can fit in my boat, and I will take you home.\" The little fish was happy. It jumped into Tom's boat. They sailed together on the sea.\n",
      "The sea was safe and calm. Tom and the little fish talked and laughed. They became good friends. At last, they found the fish's home. The little fish said, \"Thank you, Tom, for helping me.\" Tom smiled and waved goodbye. He sailed away, knowing he had a new friend.</s><s>Sara and Ben wanted to decorate a bowl for their mom. They found a big bowl in the kitchen and some paint and brushes. They took the bowl and the paint to the backyard and put them on a table.\n",
      "\"Let's make the bowl pretty with colors,\" Sara said.\n",
      "\"OK, I will paint a flower,\" Ben said.\n",
      "They started to paint the bowl with different colors. Sara painted a red heart and Ben painted a yellow flower. They were having fun.\n",
      "But then, it started to rain. The rain was wet and cold. It made the paint run and drip. The bowl looked messy and ugly.\n",
      "\"Oh no, the rain ruined our bowl!\" Sara cried.\n",
      "\"Mom will not like it,\" Ben said.\n",
      "They ran inside the house with the bowl. They were sad and wet.\n",
      "They showed the bowl to their mom. They said they were sorry.\n",
      "But mom smiled and hugged them. She said she loved the bowl and them.\n",
      "\"It's a beautiful bowl,\" she said. \"You made it with love and creativity. The rain made it special. It's like a rainbow bowl.\"\n",
      "Sara and Ben felt happy. They gave mom a kiss and thanked her. They learned that sometimes, things can be good even when they seem bad.</s><s>Once upon a time, in a small town, there was a deaf boy named Tom. Tom could not hear, but he was very good at seeing and feeling things. He had many friends who loved to play with him. They would play games like tag and hide-and-seek. Tom was very fast and good at finding his friends.\n",
      "One day, the town bell would ring at noon. Everyone was excited to hear the bell, but Tom knew he could not hear it. He felt sad, but his friends had a plan. They decided to help Tom feel the bell ring too. They took Tom to the big bell and let him touch it while it rang. When the bell rang, Tom could feel the strong vibrations and he smiled.\n",
      "The moral of the story is that even if someone is different or has a problem, like Tom being deaf, we can still find ways to help them and make them happy. We should always be kind and caring to everyone, no matter how different they are.</s><s>Tom and Lily were twins who liked to play in the park. One day, they saw a large maze made of green bushes. They wanted to try it and find the way out.\n",
      "They ran inside the maze and followed the paths. Sometimes they turned left, sometimes they turned right. They saw many flowers and birds, but they did not see the exit. They started to feel lost and scared.\n",
      "They heard a loud roar behind them. It was a big lion who lived in the maze. He was hungry and angry. He wanted to catch Tom and Lily and eat them.\n",
      "Tom and Lily ran away from the lion. They looked for a place to hide. They saw a small hole in the bushes. They crawled inside and hoped the lion would not find them.\n",
      "The lion came closer and closer. He smelled Tom and Lily in the hole. He reached his paw inside and tried to grab them. He\n"
     ]
    }
   ],
   "source": [
    "print('Sample 1:', g_tokenizer.decode(next(iter(g_train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d124589733f9ed1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:53.005255751Z",
     "start_time": "2023-10-29T05:27:51.409799495Z"
    }
   },
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    g_model = ToyTransformer(g_tokenizer.get_vocab_size(), 2, 2, 256, 1024, C_DEVICE, C_DTYPE)\n",
    "else:\n",
    "    g_model = ToyTransformer(g_tokenizer.get_vocab_size(), C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN, C_DEVICE, C_DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 135418880\n",
      "ToyTransformer(\n",
      "  (sem_embed): Embedding(32768, 768)\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-11): 12 x DecoderLayer(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (attn_heads): ModuleList(\n",
      "          (0-11): 12 x AttentionHead(\n",
      "            (q_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (k_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (ln_mha): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_ffn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('Total parameters:', sum([t.numel() for t in g_model.parameters()]))\n",
    "print(g_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:53.048499706Z",
     "start_time": "2023-10-29T05:27:53.004742366Z"
    }
   },
   "id": "f906aed5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:53.139485733Z",
     "start_time": "2023-10-29T05:27:53.048255793Z"
    }
   },
   "id": "dca6004e9b65892c"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def dataset_collate(dataset_iter: DatasetIter, batch_size: int,\n",
    "                    transform: Optional[Callable[[Dict[str, List[np.ndarray]]], Dict[str, torch.Tensor]]] = None,\n",
    "                    drop_last: bool = False):\n",
    "    cur_batch, cur_batch_size = {}, 0\n",
    "    for entry in dataset_iter:\n",
    "        for k, v in entry.items():\n",
    "            cur_batch.setdefault(k, [])\n",
    "            cur_batch[k].append(v)\n",
    "        cur_batch_size += 1\n",
    "        if cur_batch_size == batch_size:\n",
    "            yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)\n",
    "            cur_batch = {}\n",
    "            cur_batch_size = 0\n",
    "    if not drop_last and len(cur_batch) > 0:\n",
    "        yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:53.580462440Z",
     "start_time": "2023-10-29T05:27:53.579603262Z"
    }
   },
   "id": "674478c0ccb206af"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:54.320573360Z",
     "start_time": "2023-10-29T05:27:54.318483452Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainArguments:\n",
    "    num_epochs: int\n",
    "    batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "\n",
    "    optimizer: Type[torch.optim.Optimizer]\n",
    "    optimizer_args: Optional[Dict[str, Any]]\n",
    "\n",
    "    start_lr: float\n",
    "    max_lr: float\n",
    "    end_lr: float\n",
    "    warmup_ratio: float\n",
    "\n",
    "    gradient_clip_norm: Optional[float]\n",
    "\n",
    "    train_data: DatasetReader\n",
    "    ignore_attn_mask: bool\n",
    "    ignore_loss_mask: bool\n",
    "\n",
    "    # eval_data: Optional[DatasetReader]\n",
    "    # eval_steps: int\n",
    "    # \n",
    "    # eval_generate_prompt: Optional[str]\n",
    "    # eval_generate_steps: int\n",
    "\n",
    "    save_steps: int\n",
    "    save_on_interrupt: bool\n",
    "\n",
    "\n",
    "# type cast for handling int16/uint16 columns\n",
    "def train_transform(batch: Dict[str, List[np.ndarray]]):\n",
    "    return {k: torch.tensor(np.stack(v, dtype=np.int32 if v[0].dtype in [np.int16, np.uint16] else v[0].dtype)) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, model: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer, lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "                    train_args: TrainArguments,\n",
    "                    dataset: DatasetReader, dataset_iter: DatasetIter, train_logs: List, misc: Dict):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path + '/model.pt')\n",
    "    torch.save(optimizer.state_dict(), path + '/optimizer.pt')\n",
    "    torch.save(lr_scheduler.state_dict(), path + '/lr_scheduler.pt')\n",
    "    torch.save(torch.get_rng_state(), path + '/rng_state.pt')\n",
    "    torch.save(train_args, path + '/train_args.pt')\n",
    "    dataset.save_iterator(dataset_iter, path + '/dataset_iter.pt')\n",
    "    torch.save(train_logs, path + '/train_logs.pt')\n",
    "    torch.save(misc, path + '/misc.pt')\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str, model: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer, lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "                    train_args: TrainArguments,\n",
    "                    dataset: DatasetReader, dataset_iter: DatasetIter, train_logs: List, misc: Dict):\n",
    "    model.load_state_dict(torch.load(path + '/model.pt'))\n",
    "    optimizer.load_state_dict(torch.load(path + '/optimizer.pt'))\n",
    "    lr_scheduler.load_state_dict(torch.load(path + '/lr_scheduler.pt'))\n",
    "    torch.set_rng_state(torch.load(path + '/rng_state.pt'))\n",
    "    # assert torch.load(path + '/train_args.pt') == train_args\n",
    "    dataset_iter.set_state(dataset.load_iterator(path + '/dataset_iter.pt').get_state())\n",
    "    train_logs.clear()\n",
    "    train_logs += torch.load(path + '/train_logs.pt')\n",
    "    misc.update(torch.load(path + '/misc.pt'))\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_args: TrainArguments,\n",
    "                resume_from: Optional[str] = None,\n",
    "                show_progress: bool = True,\n",
    "                output_dir: str = 'checkpoints', interrupt_lock: Optional[Lock] = None):\n",
    "    interrupted = False\n",
    "    train_logs = []\n",
    "    misc = {'epochs': 0, 'steps': 0, 'last_batch_idx': -1}\n",
    "\n",
    "    total_samples = len(train_args.train_data)\n",
    "    epoch_steps = ceil(total_samples / train_args.batch_size)\n",
    "    assert epoch_steps >= train_args.gradient_accumulation_steps, \\\n",
    "        f'per-epoch steps {epoch_steps} is less than gradient accumulation steps {train_args.gradient_accumulation_steps}'\n",
    "\n",
    "    schedule_steps = ceil(total_samples / train_args.batch_size / train_args.gradient_accumulation_steps)\n",
    "    total_steps = schedule_steps * train_args.num_epochs\n",
    "\n",
    "    optimizer = train_args.optimizer(model.parameters(), **(train_args.optimizer_args if train_args.optimizer_args is not None else {}))\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=train_args.max_lr, div_factor=train_args.max_lr / train_args.start_lr,\n",
    "                                                    total_steps=total_steps,\n",
    "                                                    final_div_factor=train_args.start_lr / train_args.end_lr, pct_start=train_args.warmup_ratio)\n",
    "\n",
    "    dataset_iter = iter(train_args.train_data)\n",
    "    if resume_from is not None:\n",
    "        load_checkpoint(resume_from, model, optimizer, scheduler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "\n",
    "    bar = tqdm.tqdm(total=total_steps, smoothing=1.0, disable=not show_progress)\n",
    "    bar.update(misc['steps'])\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch_num in range(train_args.num_epochs):\n",
    "        if epoch_num < misc['epochs']:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, batch in enumerate(dataset_collate(dataset_iter, train_args.batch_size, train_transform), start=misc['last_batch_idx'] + 1):\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            tokens = batch['token_ids'].to(model.device)\n",
    "            inputs = tokens[:, :-1]\n",
    "            labels = tokens[:, 1:]\n",
    "\n",
    "            attn_mask = batch['attn_mask'][:, :-1].to(model.device) if 'attn_mask' in batch and not train_args.ignore_attn_mask else None\n",
    "            loss_mask = batch['loss_mask'][:, :-1].to(model.device) if 'loss_mask' in batch and not train_args.ignore_loss_mask else None\n",
    "\n",
    "            logits, kv_state = model.forward(inputs, attn_mask=attn_mask)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=2).view(-1, logits.shape[-1])\n",
    "\n",
    "            loss = (-torch.log(probs[torch.arange(probs.shape[0]), labels.reshape(-1)]))\n",
    "            if loss_mask is not None:\n",
    "                loss = (loss * loss_mask.reshape(-1)).mean() / train_args.gradient_accumulation_steps\n",
    "            else:\n",
    "                loss = loss.mean() / train_args.gradient_accumulation_steps\n",
    "\n",
    "            # brutally clear nan, give up the whole batch\n",
    "            if torch.isnan(loss):\n",
    "                print(f'encountered nan loss at epoch {epoch_num + 1}, batch {batch_idx}')\n",
    "                # optimizer.zero_grad()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % train_args.gradient_accumulation_steps == 0 or (batch_idx + 1) == epoch_steps:\n",
    "                if train_args.gradient_clip_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), train_args.gradient_clip_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                step_time_cost = time.time() - step_start_time\n",
    "                throughput = round(probs.shape[0] / step_time_cost / 1000, 2)\n",
    "\n",
    "                step_stat = {'Loss': f'{loss.item() * train_args.gradient_accumulation_steps:.3f}',\n",
    "                             'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                             'Throughput': f'{throughput} kt/s'}\n",
    "\n",
    "                if show_progress:\n",
    "                    bar.set_description(f'Epoch {epoch_num + 1}')\n",
    "                    bar.set_postfix(step_stat)\n",
    "                else:\n",
    "                    print(', '.join(f'{s[0]}:{s[1]}' for s in step_stat.items()))\n",
    "\n",
    "                scheduler.step()\n",
    "                bar.update(1)\n",
    "                train_logs.append((epoch_num, batch_idx, step_stat))\n",
    "\n",
    "                misc['steps'] += 1\n",
    "                misc['last_batch_idx'] = batch_idx\n",
    "                if train_args.save_steps > 0 and (misc['steps'] % train_args.save_steps) == 0:\n",
    "                    save_checkpoint(output_dir + f'/checkpoint-{misc[\"steps\"]}',\n",
    "                                    model, optimizer, scheduler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "                if interrupt_lock is not None and not interrupt_lock.locked():\n",
    "                    if train_args.save_on_interrupt:\n",
    "                        save_checkpoint(output_dir + f'/checkpoint-{misc[\"steps\"]}',\n",
    "                                        model, optimizer, scheduler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "                    interrupted = True\n",
    "                    break\n",
    "        if interrupted:\n",
    "            break\n",
    "        misc['epochs'] += 1\n",
    "        misc['last_batch_idx'] = -1\n",
    "        dataset_iter = iter(train_args.train_data)\n",
    "    bar.close()\n",
    "\n",
    "    if not interrupted:\n",
    "        save_checkpoint(output_dir + f'/checkpoint-done',\n",
    "                        model, optimizer, scheduler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "\n",
    "    return train_logs\n",
    "\n",
    "\n",
    "def train_model_interruptable(model: nn.Module, train_args: TrainArguments,\n",
    "                              resume_from: Optional[str] = None,\n",
    "                              show_progress: bool = True,\n",
    "                              output_dir: str = 'checkpoints'):\n",
    "    return_value, run_finish = None, False\n",
    "\n",
    "    def return_value_wrapper(func, *args, **kwargs):\n",
    "        nonlocal return_value, run_finish\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            return_value = func(*args, **kwargs)\n",
    "        except Exception as _:\n",
    "            traceback.print_exc()\n",
    "        run_finish = True\n",
    "\n",
    "    interrupt_lock = Lock()\n",
    "    interrupt_lock.acquire()\n",
    "    thread = Thread(target=return_value_wrapper, args=(train_model, model, train_args),\n",
    "                    kwargs={'resume_from': resume_from, 'show_progress': show_progress, 'output_dir': output_dir, 'interrupt_lock': interrupt_lock})\n",
    "    thread.start()\n",
    "    while not run_finish:\n",
    "        try:\n",
    "            time.sleep(0.1)\n",
    "        except KeyboardInterrupt as _:\n",
    "            interrupt_lock.release()\n",
    "            break\n",
    "    thread.join()\n",
    "\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281e39ed7414acc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T05:27:54.970172004Z",
     "start_time": "2023-10-29T05:27:54.965805503Z"
    }
   },
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    g_train_args = TrainArguments(\n",
    "        num_epochs=1000, batch_size=8, gradient_accumulation_steps=1,\n",
    "        optimizer=torch.optim.AdamW, optimizer_args=None,\n",
    "        start_lr=1e-5, max_lr=1e-3, end_lr=1e-6, warmup_ratio=0.1,\n",
    "        gradient_clip_norm=1.0,\n",
    "        train_data=g_train_data, ignore_attn_mask=False, ignore_loss_mask=False,\n",
    "        # eval_data=None, eval_steps=-1,\n",
    "        # eval_generate_prompt=None, eval_generate_steps=-1,\n",
    "        save_steps=-1,\n",
    "        save_on_interrupt=False,\n",
    "    )\n",
    "else:\n",
    "    g_train_args = TrainArguments(\n",
    "        num_epochs=2, batch_size=12, gradient_accumulation_steps=8,\n",
    "        optimizer=torch.optim.AdamW, optimizer_args=None,\n",
    "        start_lr=5e-5, max_lr=1e-3, end_lr=1e-4, warmup_ratio=0.1,\n",
    "        gradient_clip_norm=0.7,\n",
    "        train_data=g_train_data, ignore_attn_mask=True, ignore_loss_mask=True,\n",
    "        # eval_data=None, eval_steps=-1,\n",
    "        # eval_generate_prompt=None, eval_generate_steps=-1,\n",
    "        save_steps=1000,\n",
    "        save_on_interrupt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673871354eec2c12",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-29T05:28:28.784393217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2996 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d51e3f447a74e2ea2a423eb7d748b3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if C_DEBUG:\n",
    "    global_config['attn_backend'] = AttentionBackend.FlashAttentionTriton\n",
    "    g_train_args.ignore_attn_mask = False\n",
    "    g_train_logs = train_model_interruptable(g_model, g_train_args, resume_from=None,\n",
    "                                             show_progress=True, output_dir='checkpoints/debug_output')\n",
    "else:\n",
    "    global_config['attn_backend'] = AttentionBackend.FlashAttentionTriton\n",
    "    g_train_args.save_on_interrupt = True\n",
    "    g_train_args.num_epochs = 1\n",
    "    g_train_args.gradient_clip_norm = 1.0\n",
    "    g_train_args.ignore_attn_mask = False\n",
    "    g_train_args.ignore_loss_mask = False\n",
    "    g_train_logs = train_model_interruptable(g_model, g_train_args, resume_from='checkpoints/tinystories-masked-train/checkpoint-2002',\n",
    "                                             show_progress=True, output_dir='checkpoints/tinystories-masked-train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b8ca07cd31cd5b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T01:31:40.414791533Z",
     "start_time": "2023-10-29T01:31:40.410395219Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_train_logs(train_logs):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].plot([float(l[2]['Loss']) for l in train_logs])\n",
    "    axes[1].plot([float(l[2]['LR']) for l in train_logs])\n",
    "    axes[2].plot([float(l[2]['Throughput'][:-5]) for l in train_logs])\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[1].set_title('Learning Rate')\n",
    "    axes[2].set_title('Throughput (kt/s)')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.autoscale()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# checkpoint_list = [\n",
    "#     'checkpoints/train-round1/checkpoint-19022/train_logs.pt',\n",
    "# ]\n",
    "# checkpoint_list = [torch.load(c) for c in checkpoint_list]\n",
    "# \n",
    "# for c in checkpoint_list:\n",
    "#     plot_train_logs(c)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T01:31:41.232116238Z",
     "start_time": "2023-10-29T01:31:41.226871899Z"
    }
   },
   "id": "471f560981b69d48"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e800403549c98d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T01:31:41.923612259Z",
     "start_time": "2023-10-29T01:31:41.921625635Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, temperature, top_p, rep_penalty,\n",
    "             max_new_tokens=20, total_tokens=None,\n",
    "             end_tokens=None,\n",
    "             enable_kv_cache=True):\n",
    "    model.eval()\n",
    "\n",
    "    feed_tokens = tokenizer.encode(prompt) if isinstance(prompt, str) else prompt\n",
    "    all_tokens = feed_tokens.copy()\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(feed_tokens))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, kv_cache = model.forward(\n",
    "                torch.tensor([feed_tokens if enable_kv_cache else all_tokens]).to(model.device),\n",
    "                kv_cache=kv_cache)\n",
    "            logits = logits[0][-1].cpu()\n",
    "            if not enable_kv_cache:\n",
    "                kv_cache = None\n",
    "\n",
    "            # apply repetition penalty\n",
    "            logits_rep = torch.gather(logits, 0, torch.tensor(all_tokens))\n",
    "            logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "            logits.scatter_(0, torch.tensor(all_tokens), logits_rep)\n",
    "\n",
    "            # apply temperature\n",
    "            logits /= max(temperature, 1e-6)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # apply top-p\n",
    "            ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "            cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "            top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "            ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "            sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "            all_tokens.append(sampled_index)\n",
    "            feed_tokens = [sampled_index]\n",
    "\n",
    "            if end_tokens is not None and sampled_index in end_tokens:\n",
    "                break\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def modeling_sanity_check(gen_length: int, enable_kv_cache: bool):\n",
    "    assert C_DEBUG == True, 'sanity check can only be performed under debug settings'\n",
    "    train_token_ids = next(iter(g_train_data))['token_ids'].tolist()\n",
    "    train_texts = [l.strip() for l in g_tokenizer.decode(train_token_ids).split('</s>')]\n",
    "    start_time = time.time()\n",
    "    gen_token_ids = generate(g_model, g_tokenizer, train_token_ids[:10],\n",
    "                             temperature=1.0, top_p=0.01, rep_penalty=1.0,\n",
    "                             total_tokens=gen_length,\n",
    "                             end_tokens=g_tokenizer.encode('<reserved_0>'),\n",
    "                             enable_kv_cache=enable_kv_cache)\n",
    "    cost_time = time.time() - start_time\n",
    "    print(f'Generation finished in {cost_time:.2f} sec(s), throughput: {len(gen_token_ids) / cost_time:.1f} tokens/sec')\n",
    "    # Complete check\n",
    "    cmp_length = min(len(train_token_ids), len(gen_token_ids))\n",
    "    print('Complete Identical:', train_token_ids[:cmp_length] == gen_token_ids[:cmp_length])\n",
    "    # Segment check\n",
    "    gen_texts = [l.strip() for l in g_tokenizer.decode(gen_token_ids).split('</s>')]\n",
    "    for i in range(min(len(train_texts), len(gen_texts))):\n",
    "        ref, real = train_texts[i], gen_texts[i]\n",
    "        cmp_length = min(len(ref), len(real))\n",
    "        print(f'Segment {i}: Ref Len: {len(ref)}, Gen Len: {len(real)} Identical: {ref[:cmp_length] == real[:cmp_length]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T01:31:42.573622390Z",
     "start_time": "2023-10-29T01:31:42.567322520Z"
    }
   },
   "id": "93a3628aacbd1f80"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend = AttentionBackend.Naive, KVCache Enable = True\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "sanity check can only be performed under debug settings",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBackend = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbackend\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, KVCache Enable = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menable_kv_cache\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m global_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattn_backend\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m backend\n\u001B[0;32m----> 5\u001B[0m \u001B[43mmodeling_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menable_kv_cache\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m80\u001B[39m)\n",
      "Cell \u001B[0;32mIn[17], line 2\u001B[0m, in \u001B[0;36mmodeling_sanity_check\u001B[0;34m(gen_length, enable_kv_cache)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmodeling_sanity_check\u001B[39m(gen_length: \u001B[38;5;28mint\u001B[39m, enable_kv_cache: \u001B[38;5;28mbool\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m C_DEBUG \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msanity check can only be performed under debug settings\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m     train_token_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(g_train_data))[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m      4\u001B[0m     train_texts \u001B[38;5;241m=\u001B[39m [l\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m g_tokenizer\u001B[38;5;241m.\u001B[39mdecode(train_token_ids)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m</s>\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n",
      "\u001B[0;31mAssertionError\u001B[0m: sanity check can only be performed under debug settings"
     ]
    }
   ],
   "source": [
    "for backend in [AttentionBackend.Naive, AttentionBackend.FlashAttentionTriton, AttentionBackend.FlashAttentionCuda]:\n",
    "    for enable_kv_cache in [True, False]:\n",
    "        print(f'Backend = {backend}, KVCache Enable = {enable_kv_cache}')\n",
    "        global_config['attn_backend'] = backend\n",
    "        modeling_sanity_check(512, enable_kv_cache)\n",
    "        print('=' * 80)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T01:31:43.217100292Z",
     "start_time": "2023-10-29T01:31:43.121943856Z"
    }
   },
   "id": "c2ffe9294382f18c"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f94c17f13926df4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T01:31:44.206151221Z",
     "start_time": "2023-10-29T01:31:44.198256043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
      "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
      "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
      "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
      "And that's how Ben found an amazing vase in the store!\n",
      "<s>Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
      "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his friend, the duck. \"Hi, Ollie!\" said the duck. \"Hi, duck!\" said Ollie. \"I need to hurry and catch fish for my family.\"\n",
      "While Ollie was catching fish, he found a big shiny stone. He thought, \"This is not a fish, but it is so pretty!\" Ollie took the shiny stone home to show his family. They all looked at the shiny stone and smiled. The shiny stone made everyone happy, and they forgot about the fish for dinner.\n",
      "<s>One day, a little boy named Tim went to the park. He saw a big tiger. The tiger was not mean, but very easy to play with. Tim and the tiger played all day. They had lots of fun.\n",
      "Then, something unexpected happened. The tiger started to shake. Tim was scared. He did not know what was going on. But then, the tiger turned into a nice dog. Tim was very surprised.\n",
      "Tim and the dog played together now. They were very happy. The dog was easy to play with too. At the end of the day, Tim went home with his new friend.\n"
     ]
    }
   ],
   "source": [
    "for t in g_tokenizer.decode(next(iter(g_train_data))['token_ids'].tolist()).split('</s>')[:3]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bc924753b5acfc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T01:32:09.482172489Z",
     "start_time": "2023-10-29T01:32:05.611912410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Once upon a time, there was a little girl named Lily. She loved to play with her toys and make things with them. One day, she found a big box in her room. It had a lot of weight inside. Lily wanted to see what was inside the box.\n",
      "Lily tried to open the box, but it was too heavy for her. She asked her mom for help. Her mom said, \"Let's try together!\" They both pushed the box, and it opened! Inside the box, they found a lot of colorful balls. Lily was so happy!\n",
      "Lily played with the balls all day long. She threw them up in the air and caught them. She made a new friend who also liked to play with her. At the end of the day, Lily and her mom put the balls back in the box and closed it. They knew they would have more fun days playing together.</s>\n",
      "3.872 sec(s), throughput 50.6 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "global_config['attn_backend'] = AttentionBackend.FlashAttentionCuda\n",
    "result = generate(g_model, g_tokenizer, '<s>',\n",
    "                  temperature=1.0, top_p=0.3, rep_penalty=1.1,\n",
    "                  total_tokens=512,\n",
    "                  end_tokens=g_tokenizer.encode('</s>'),\n",
    "                  enable_kv_cache=True)\n",
    "time_cost = time.time() - time_start\n",
    "\n",
    "print(g_tokenizer.decode(result))\n",
    "print(f'{time_cost:.3f} sec(s), throughput {len(result)/time_cost:.1f} tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5267f0a8c69815ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

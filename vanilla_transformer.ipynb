{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:07.412980939Z",
     "start_time": "2023-11-06T05:24:05.240509843Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenization import TRIETokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "from typing import *\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from flash_attn import flash_attn_func\n",
    "from flash_attn_triton import flash_attn_func as flash_attn_func_triton\n",
    "from dataloader import DatasetReader, DatasetIter, SingleDatasetReader, MultiDatasetsReader\n",
    "from math import ceil\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import bitsandbytes as bnb\n",
    "from threading import Lock, Thread\n",
    "import traceback\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from enum import Enum\n",
    "from modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Disabled due to no obvious speed up\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:07.413240848Z",
     "start_time": "2023-11-06T05:24:07.410584683Z"
    }
   },
   "id": "d09b9a4246f3aae2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33702a8e6613f742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:07.787046693Z",
     "start_time": "2023-11-06T05:24:07.410802043Z"
    }
   },
   "outputs": [],
   "source": [
    "g_tokenizer = TRIETokenizer('llama_vocab_pruned_32k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb151752ec125fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:07.793094418Z",
     "start_time": "2023-11-06T05:24:07.787203778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network definition for 370M network\n",
    "# C_SEQ_LEN = 1024\n",
    "# C_HIDDEN_SIZE = 1024\n",
    "# C_NUM_HEADS = 16\n",
    "# C_NUM_LAYERS = 24\n",
    "\n",
    "# Network definition for 135M network\n",
    "C_SEQ_LEN = 1024\n",
    "C_HIDDEN_SIZE = 768\n",
    "C_NUM_HEADS = 12\n",
    "C_NUM_LAYERS = 12\n",
    "\n",
    "C_DEVICE = torch.device('cuda')\n",
    "C_DTYPE = torch.float32\n",
    "\n",
    "C_DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799b78921dbff1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:07.816715117Z",
     "start_time": "2023-11-06T05:24:07.789388756Z"
    }
   },
   "outputs": [],
   "source": [
    "if not C_DEBUG:\n",
    "    # g_train_data = MultiDatasetsReader([\n",
    "    #     SingleDatasetReader('datasets/minipile_train_masked_1024.bin'),\n",
    "    #     SingleDatasetReader('datasets/enwiki_train_masked_1024.bin'),\n",
    "    #     SingleDatasetReader('datasets/tinytextbooks_train_masked_1024.bin'),\n",
    "    # ], seed=0)\n",
    "\n",
    "    # g_train_data = MultiDatasetsReader([\n",
    "    #     SingleDatasetReader('datasets/tinystories_train_masked.bin'),\n",
    "    # ], seed=0)\n",
    "\n",
    "    # g_train_data = MultiDatasetsReader([\n",
    "    #     SingleDatasetReader('datasets/slimpajamas/slimpajama_train_masked_1024.bin'),\n",
    "    # ], seed=0)\n",
    "\n",
    "    g_train_data = MultiDatasetsReader([\n",
    "        SingleDatasetReader('datasets/sft/alpaca_gpt4.bin'),\n",
    "        SingleDatasetReader('datasets/sft/wizardlm_evol_2.bin'),\n",
    "        SingleDatasetReader('datasets/sft/airoboros_2.2.1.bin'),\n",
    "        SingleDatasetReader('datasets/sft/sharegpt_gpt4.bin'),\n",
    "        SingleDatasetReader('datasets/sft/ultrachat.bin')\n",
    "    ], seed=0)\n",
    "else:\n",
    "    g_train_data = MultiDatasetsReader([\n",
    "        SingleDatasetReader('datasets/debug_data_masked.bin'),\n",
    "        SingleDatasetReader('datasets/debug_data_masked.bin'),\n",
    "    ], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d770a2be845a70d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:07.860977578Z",
     "start_time": "2023-11-06T05:24:07.817162449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 144174\n",
      "Sample length: 1024\n",
      "Sample fields: ['attn_mask', 'loss_mask', 'token_ids']\n",
      "Train tokens: 147634176\n"
     ]
    }
   ],
   "source": [
    "print('Train samples:', len(g_train_data))\n",
    "print('Sample length:', len(next(iter(g_train_data))['token_ids']))\n",
    "print('Sample fields:', list(next(iter(g_train_data)).keys()))\n",
    "print('Train tokens:', len(g_train_data) * len(next(iter(g_train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8dbb96cab10318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:07.861461180Z",
     "start_time": "2023-11-06T05:24:07.860466755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: <s>A chat between User and Assistant.\n",
      "User:These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
      "On your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n",
      "Your Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n",
      "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?\n",
      "Assistant:This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.</s>\n",
      "User:Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?\n",
      "Assistant:Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n",
      "\n",
      "1. Log in to your Shopify account and go to your Online Store.\n",
      "2. Click on Customize theme for the section-based theme you are using.\n",
      "3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n",
      "4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n",
      "5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n",
      "6. If available, select 'Show secondary image on hover'.\n",
      "7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n",
      "\n",
      "If you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.</s>\n",
      "User:Can you provide me with a link to the documentation for my theme?\n",
      "Assistant:I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.</s>\n",
      "User:Can you confirm if this feature also works for the Quick Shop section of my theme?\n",
      "Assistant:The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n",
      "\n",
      "1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.</s>\n",
      "<s>A chat between User and Assistant.\n",
      "User:How did the Spanish conquest of the Low Countries in the 16th century impact European politics and culture?\n",
      "Assistant:The Spanish conquest of the Low Countries in the 16th century had a significant impact on European politics and culture. Here are some of the impacts:\n",
      "\n",
      "1. Religious conflicts: The Low Countries were a major center of Protestantism, and Spanish rule led to religious conflicts between Catholics and Protestants. This led to the Eighty Years' War (1568-1648) which was fought between the Protestant Dutch and the Spanish Catholics.\n",
      "\n",
      "2. Rise of Dutch power: The Spanish conquest of the Low Countries led to the rise of Dutch power in Europe. The Dutch fought against the Spanish and eventually gained independence in 1648. The Dutch Republic emerged as a major power in Europe, with significant influence on trade, politics, and culture.\n",
      "\n",
      "3. Cultural exchange: The Spanish conquest of the Low Countries led to a cultural exchange between the Spanish and the Dutch. The Dutch were exposed to Spanish art\n"
     ]
    }
   ],
   "source": [
    "print('Sample 1:', g_tokenizer.decode(next(iter(g_train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d124589733f9ed1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:09.084252448Z",
     "start_time": "2023-11-06T05:24:07.860653073Z"
    }
   },
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    g_model = ToyTransformer(g_tokenizer.get_vocab_size(), 2, 2, 256, 1024, C_DEVICE, C_DTYPE)\n",
    "else:\n",
    "    g_model = ToyTransformer(g_tokenizer.get_vocab_size(), C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN, C_DEVICE, C_DTYPE)\n",
    "    # g_model = ToyTransformer(g_tokenizer.get_vocab_size(), 4, 8, 512, C_SEQ_LEN, C_DEVICE, C_DTYPE) # 46M model for tiny stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 135418880\n",
      "ToyTransformer(\n",
      "  (sem_embed): Embedding(32768, 768)\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-11): 12 x DecoderLayer(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (attn_heads): ModuleList(\n",
      "          (0-11): 12 x AttentionHead(\n",
      "            (q_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (k_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (ln_mha): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_ffn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('Total parameters:', sum([t.numel() for t in g_model.parameters()]))\n",
    "print(g_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:09.084765953Z",
     "start_time": "2023-11-06T05:24:09.084143786Z"
    }
   },
   "id": "f906aed5"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory usage: 516.58MB\n"
     ]
    }
   ],
   "source": [
    "total_memory = 0\n",
    "for p in g_model.parameters():\n",
    "    total_memory += (p.numel() * p.element_size())\n",
    "print(f'Model memory usage: {total_memory / 1024 / 1024:.2f}MB')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:09.085517279Z",
     "start_time": "2023-11-06T05:24:09.084376815Z"
    }
   },
   "id": "73a1c1c4b301c9cb"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:09.177686409Z",
     "start_time": "2023-11-06T05:24:09.084499245Z"
    }
   },
   "id": "dca6004e9b65892c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def dataset_collate(dataset_iter: DatasetIter, batch_size: int,\n",
    "                    transform: Optional[Callable[[Dict[str, List[np.ndarray]]], Dict[str, torch.Tensor]]] = None,\n",
    "                    drop_last: bool = False):\n",
    "    cur_batch, cur_batch_size = {}, 0\n",
    "    for entry in dataset_iter:\n",
    "        for k, v in entry.items():\n",
    "            cur_batch.setdefault(k, [])\n",
    "            cur_batch[k].append(v)\n",
    "        cur_batch_size += 1\n",
    "        if cur_batch_size == batch_size:\n",
    "            yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)\n",
    "            cur_batch = {}\n",
    "            cur_batch_size = 0\n",
    "    if not drop_last and len(cur_batch) > 0:\n",
    "        yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:09.220174982Z",
     "start_time": "2023-11-06T05:24:09.178860330Z"
    }
   },
   "id": "674478c0ccb206af"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:09.220422521Z",
     "start_time": "2023-11-06T05:24:09.220034004Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainArguments:\n",
    "    num_epochs: int\n",
    "    batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "\n",
    "    optimizer: Type[torch.optim.Optimizer]\n",
    "    optimizer_args: Optional[Dict[str, Any]]\n",
    "    mixed_precision_dtype: torch.dtype\n",
    "\n",
    "    start_lr: float\n",
    "    max_lr: float\n",
    "    end_lr: float\n",
    "    warmup_ratio: float\n",
    "\n",
    "    gradient_clip_norm: Optional[float]\n",
    "    probs_epsilon: Optional[float]\n",
    "\n",
    "    train_data: DatasetReader\n",
    "    ignore_attn_mask: bool\n",
    "    ignore_loss_mask: bool\n",
    "\n",
    "    # eval_data: Optional[DatasetReader]\n",
    "    # eval_steps: int\n",
    "    # \n",
    "    # eval_generate_prompt: Optional[str]\n",
    "    # eval_generate_steps: int\n",
    "\n",
    "    save_steps: int\n",
    "    save_on_interrupt: bool\n",
    "\n",
    "\n",
    "# type cast for handling int16/uint16 columns\n",
    "def train_transform(batch: Dict[str, List[np.ndarray]]):\n",
    "    return {k: torch.tensor(np.stack(v, dtype=np.int32 if v[0].dtype in [np.int16, np.uint16] else v[0].dtype)) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, model: ToyTransformer,\n",
    "                    optimizer: torch.optim.Optimizer, lr_scheduler: torch.optim.lr_scheduler.LRScheduler, grad_scaler: Optional[torch.cuda.amp.GradScaler],\n",
    "                    train_args: TrainArguments,\n",
    "                    dataset: DatasetReader, dataset_iter: DatasetIter, train_logs: List, misc: Dict):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path + '/model.pt')\n",
    "    torch.save(optimizer.state_dict(), path + '/optimizer.pt')\n",
    "    torch.save(lr_scheduler.state_dict(), path + '/lr_scheduler.pt')\n",
    "    if grad_scaler is not None:\n",
    "        torch.save(lr_scheduler.state_dict(), path + '/grad_scaler.pt')\n",
    "    torch.save(torch.get_rng_state(), path + '/rng_state.pt')\n",
    "    torch.save(train_args, path + '/train_args.pt')\n",
    "    dataset.save_iterator(dataset_iter, path + '/dataset_iter.pt')\n",
    "    torch.save(train_logs, path + '/train_logs.pt')\n",
    "    torch.save(misc, path + '/misc.pt')\n",
    "    with open(path + '/config.txt', 'w') as file:\n",
    "        file.write(f'Dataset: {str(dataset)}')\n",
    "        file.write('\\n\\n')\n",
    "        file.write(f'Model Config: {str(model.config)}')\n",
    "        file.write('\\n\\n')\n",
    "        file.write(f'Training Arguments: {str(train_args)}')\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str, model: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer, lr_scheduler: torch.optim.lr_scheduler.LRScheduler, grad_scaler: Optional[torch.cuda.amp.GradScaler],\n",
    "                    train_args: TrainArguments,\n",
    "                    dataset: DatasetReader, dataset_iter: DatasetIter, train_logs: List, misc: Dict):\n",
    "    model.load_state_dict(torch.load(path + '/model.pt'))\n",
    "    optimizer.load_state_dict(torch.load(path + '/optimizer.pt'))\n",
    "    lr_scheduler.load_state_dict(torch.load(path + '/lr_scheduler.pt'))\n",
    "    if grad_scaler is not None:\n",
    "        grad_scaler.load_state_dict(torch.load(path + '/grad_scaler.pt'))\n",
    "    torch.set_rng_state(torch.load(path + '/rng_state.pt'))\n",
    "    # assert torch.load(path + '/train_args.pt') == train_args\n",
    "    dataset_iter.set_state(dataset.load_iterator(path + '/dataset_iter.pt').get_state())\n",
    "    train_logs.clear()\n",
    "    train_logs += torch.load(path + '/train_logs.pt')\n",
    "    misc.update(torch.load(path + '/misc.pt'))\n",
    "\n",
    "\n",
    "def train_model(model: ToyTransformer, train_args: TrainArguments,\n",
    "                resume_from: Optional[str] = None,\n",
    "                show_progress: bool = True,\n",
    "                output_dir: str = 'checkpoints', interrupt_lock: Optional[Lock] = None):\n",
    "    output_dir = output_dir.rstrip('/')\n",
    "\n",
    "    interrupted = False\n",
    "    train_logs = []\n",
    "    misc = {'epochs': 0, 'steps': 0, 'last_batch_idx': -1}\n",
    "\n",
    "    total_samples = len(train_args.train_data)\n",
    "    epoch_steps = ceil(total_samples / train_args.batch_size)\n",
    "    assert epoch_steps >= train_args.gradient_accumulation_steps, \\\n",
    "        f'per-epoch steps {epoch_steps} is less than gradient accumulation steps {train_args.gradient_accumulation_steps}'\n",
    "\n",
    "    schedule_steps = ceil(total_samples / train_args.batch_size / train_args.gradient_accumulation_steps)\n",
    "    total_steps = schedule_steps * train_args.num_epochs\n",
    "\n",
    "    optimizer = train_args.optimizer(model.parameters(), **(train_args.optimizer_args if train_args.optimizer_args is not None else {}))\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=train_args.max_lr, div_factor=train_args.max_lr / train_args.start_lr,\n",
    "                                                    total_steps=total_steps,\n",
    "                                                    final_div_factor=train_args.start_lr / train_args.end_lr, pct_start=train_args.warmup_ratio)\n",
    "\n",
    "    if train_args.mixed_precision_dtype == torch.float16:\n",
    "        grad_scaler = torch.cuda.amp.GradScaler()\n",
    "    else:\n",
    "        grad_scaler = None\n",
    "\n",
    "    dataset_iter = iter(train_args.train_data)\n",
    "    if resume_from is not None:\n",
    "        load_checkpoint(resume_from, model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "\n",
    "    bar = tqdm.tqdm(total=total_steps, smoothing=1.0, disable=not show_progress)\n",
    "    bar.update(misc['steps'])\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch_num in range(train_args.num_epochs):\n",
    "        if epoch_num < misc['epochs']:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, batch in enumerate(dataset_collate(dataset_iter, train_args.batch_size, train_transform), start=misc['last_batch_idx'] + 1):\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            tokens = batch['token_ids'].to(model.device)\n",
    "            inputs = tokens[:, :-1]\n",
    "            labels = tokens[:, 1:]\n",
    "\n",
    "            attn_mask = batch['attn_mask'][:, :-1].to(model.device) if 'attn_mask' in batch and not train_args.ignore_attn_mask else None\n",
    "            loss_mask = batch['loss_mask'][:, 1:].to(model.device) if 'loss_mask' in batch and not train_args.ignore_loss_mask else None\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=train_args.mixed_precision_dtype, enabled=train_args.mixed_precision_dtype is not None):\n",
    "                logits, kv_state = model.forward(inputs, attn_mask=attn_mask)\n",
    "\n",
    "                probs = torch.softmax(logits, dim=2).view(-1, logits.shape[-1]) + train_args.probs_epsilon\n",
    "\n",
    "                loss = (-torch.log(probs[torch.arange(probs.shape[0]), labels.reshape(-1)]))\n",
    "                if loss_mask is not None:\n",
    "                    loss = (loss * loss_mask.reshape(-1)).mean() / train_args.gradient_accumulation_steps\n",
    "                else:\n",
    "                    loss = loss.mean() / train_args.gradient_accumulation_steps\n",
    "\n",
    "            # warn on nan loss\n",
    "            if torch.isnan(loss):\n",
    "                print(f'encountered nan loss at epoch {epoch_num + 1}, batch {batch_idx}')\n",
    "            else:\n",
    "                if grad_scaler is not None:\n",
    "                    grad_scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % train_args.gradient_accumulation_steps == 0 or (batch_idx + 1) == epoch_steps:\n",
    "                if grad_scaler is not None:\n",
    "                    grad_scaler.unscale_(optimizer)\n",
    "\n",
    "                if train_args.gradient_clip_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), train_args.gradient_clip_norm)\n",
    "\n",
    "                if grad_scaler is not None:\n",
    "                    grad_scaler.step(optimizer)\n",
    "                    grad_scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                step_time_cost = time.time() - step_start_time\n",
    "                throughput = round(probs.shape[0] / step_time_cost / 1000, 2)\n",
    "\n",
    "                step_stat = {'Loss': f'{loss.item() * train_args.gradient_accumulation_steps:.3f}',\n",
    "                             'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                             'Throughput': f'{throughput} kt/s'}\n",
    "\n",
    "                if show_progress:\n",
    "                    bar.set_description(f'Epoch {epoch_num + 1}')\n",
    "                    bar.set_postfix(step_stat)\n",
    "                else:\n",
    "                    print(', '.join(f'{s[0]}:{s[1]}' for s in step_stat.items()))\n",
    "\n",
    "                scheduler.step()\n",
    "                bar.update(1)\n",
    "                train_logs.append((epoch_num, batch_idx, step_stat))\n",
    "\n",
    "                misc['steps'] += 1\n",
    "                misc['last_batch_idx'] = batch_idx\n",
    "                if train_args.save_steps > 0 and (misc['steps'] % train_args.save_steps) == 0:\n",
    "                    save_checkpoint(output_dir + f'/checkpoint-{misc[\"steps\"]}',\n",
    "                                    model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "                    gc.collect()  # perform vram gc after each save\n",
    "                    torch.cuda.empty_cache()\n",
    "                if interrupt_lock is not None and not interrupt_lock.locked():\n",
    "                    if train_args.save_on_interrupt:\n",
    "                        save_checkpoint(output_dir + f'/checkpoint-{misc[\"steps\"]}',\n",
    "                                        model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "                    interrupted = True\n",
    "                    break\n",
    "        if interrupted:\n",
    "            break\n",
    "        misc['epochs'] += 1\n",
    "        misc['last_batch_idx'] = -1\n",
    "        dataset_iter = iter(train_args.train_data)\n",
    "    bar.close()\n",
    "\n",
    "    if not interrupted:\n",
    "        save_checkpoint(output_dir + f'/checkpoint-done',\n",
    "                        model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "\n",
    "    return train_logs\n",
    "\n",
    "\n",
    "def train_model_interruptable(model: nn.Module, train_args: TrainArguments,\n",
    "                              resume_from: Optional[str] = None,\n",
    "                              show_progress: bool = True,\n",
    "                              output_dir: str = 'checkpoints'):\n",
    "    return_value, run_finish = None, False\n",
    "\n",
    "    def return_value_wrapper(func, *args, **kwargs):\n",
    "        nonlocal return_value, run_finish\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            return_value = func(*args, **kwargs)\n",
    "        except Exception as _:\n",
    "            traceback.print_exc()\n",
    "        run_finish = True\n",
    "\n",
    "    interrupt_lock = Lock()\n",
    "    interrupt_lock.acquire()\n",
    "    thread = Thread(target=return_value_wrapper, args=(train_model, model, train_args),\n",
    "                    kwargs={'resume_from': resume_from, 'show_progress': show_progress, 'output_dir': output_dir, 'interrupt_lock': interrupt_lock})\n",
    "    thread.start()\n",
    "    while not run_finish:\n",
    "        try:\n",
    "            time.sleep(0.1)\n",
    "        except KeyboardInterrupt as _:\n",
    "            interrupt_lock.release()\n",
    "            break\n",
    "    thread.join()\n",
    "\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "281e39ed7414acc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T05:24:09.221179114Z",
     "start_time": "2023-11-06T05:24:09.220312060Z"
    }
   },
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    g_train_args = TrainArguments(\n",
    "        num_epochs=1000, batch_size=8, gradient_accumulation_steps=1,\n",
    "        optimizer=torch.optim.AdamW, optimizer_args=None,\n",
    "        mixed_precision_dtype=torch.bfloat16,\n",
    "        start_lr=1e-5, max_lr=1e-3, end_lr=1e-6, warmup_ratio=0.1,\n",
    "        gradient_clip_norm=1.0, probs_epsilon=0.0,\n",
    "        train_data=g_train_data, ignore_attn_mask=False, ignore_loss_mask=False,\n",
    "        # eval_data=None, eval_steps=-1,\n",
    "        # eval_generate_prompt=None, eval_generate_steps=-1,\n",
    "        save_steps=-1,\n",
    "        save_on_interrupt=False,\n",
    "    )\n",
    "else:\n",
    "    g_train_args = TrainArguments(\n",
    "        num_epochs=2, batch_size=12, gradient_accumulation_steps=8,\n",
    "        optimizer=torch.optim.AdamW, optimizer_args=None,\n",
    "        mixed_precision_dtype=torch.bfloat16,\n",
    "        start_lr=5e-5, max_lr=1e-3, end_lr=1e-4, warmup_ratio=0.1,\n",
    "        gradient_clip_norm=0.7, probs_epsilon=0.0,\n",
    "        train_data=g_train_data, ignore_attn_mask=True, ignore_loss_mask=True,\n",
    "        # eval_data=None, eval_steps=-1,\n",
    "        # eval_generate_prompt=None, eval_generate_steps=-1,\n",
    "        save_steps=1000,\n",
    "        save_on_interrupt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "673871354eec2c12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T05:49:54.757689390Z",
     "start_time": "2023-11-06T05:24:37.920635536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/14418 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc2c8bb130ba4388a73deb2a52252e8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if C_DEBUG:\n",
    "    global_config['attn_backend'] = AttentionBackend.FlashAttentionTriton\n",
    "    g_train_args.ignore_attn_mask = True\n",
    "    g_train_logs = train_model_interruptable(g_model, g_train_args, resume_from=None,\n",
    "                                             show_progress=True, output_dir='checkpoints/debug_output')\n",
    "else:\n",
    "    global_config['attn_backend'] = AttentionBackend.FlashAttentionTriton\n",
    "    g_model.load_state_dict(torch.load('checkpoints/train-round3-135M/checkpoint-done/model.pt'))\n",
    "    g_train_args.ignore_attn_mask = False\n",
    "    g_train_args.ignore_loss_mask = False\n",
    "    g_train_args.save_steps = 1000\n",
    "    g_train_args.warmup_ratio = 0.05\n",
    "    g_train_args.batch_size = 10\n",
    "    g_train_args.gradient_accumulation_steps = 3\n",
    "    g_train_args.num_epochs = 3\n",
    "    g_train_args.start_lr = 2e-5\n",
    "    g_train_args.max_lr = 1e-3\n",
    "    g_train_args.end_lr = 1e-4\n",
    "    g_train_args.probs_epsilon = 1e-7\n",
    "    g_train_logs = train_model_interruptable(g_model, g_train_args, resume_from='checkpoints/train-round3-135M-sft/checkpoint-13000',\n",
    "                                             show_progress=True, output_dir='checkpoints/train-round3-135M-sft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ca07cd31cd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_logs(train_logs_list):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    for i, train_logs in enumerate(train_logs_list):\n",
    "        axes[0].plot([float(l[2]['Loss']) for l in train_logs])\n",
    "        axes[1].plot([float(l[2]['LR']) for l in train_logs])\n",
    "        axes[2].plot([float(l[2]['Throughput'][:-5]) for l in train_logs])\n",
    "\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[1].set_title('Learning Rate')\n",
    "    axes[2].set_title('Throughput (kt/s)')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.autoscale()\n",
    "\n",
    "    #plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_list = [\n",
    "    'checkpoints/train-round1/checkpoint-19000/train_logs.pt',\n",
    "    'checkpoints/train-round1-masked/checkpoint-1199/train_logs.pt'\n",
    "]\n",
    "logs_list = [torch.load(c) for c in checkpoint_list]\n",
    "\n",
    "plot_train_logs(logs_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "471f560981b69d48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e800403549c98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autocast(device_type='cuda', dtype=torch.bfloat16)\n",
    "def generate(model, tokenizer, prompt, temperature, top_p, rep_penalty,\n",
    "             max_new_tokens=20, total_tokens=None,\n",
    "             end_tokens=None,\n",
    "             enable_kv_cache=True):\n",
    "    model.eval()\n",
    "\n",
    "    feed_tokens = tokenizer.encode(prompt) if isinstance(prompt, str) else prompt\n",
    "    all_tokens = feed_tokens.copy()\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(feed_tokens))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, kv_cache = model.forward(\n",
    "                torch.tensor([feed_tokens if enable_kv_cache else all_tokens]).to(model.device),\n",
    "                kv_cache=kv_cache)\n",
    "            logits = logits[0][-1].cpu()\n",
    "            if not enable_kv_cache:\n",
    "                kv_cache = None\n",
    "\n",
    "            # apply repetition penalty\n",
    "            logits_rep = torch.gather(logits, 0, torch.tensor(all_tokens))\n",
    "            logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "            logits.scatter_(0, torch.tensor(all_tokens), logits_rep)\n",
    "\n",
    "            # apply temperature\n",
    "            logits /= max(temperature, 1e-6)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # apply top-p\n",
    "            ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "            cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "            top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "            ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "            sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "            all_tokens.append(sampled_index)\n",
    "            feed_tokens = [sampled_index]\n",
    "\n",
    "            if end_tokens is not None and sampled_index in end_tokens:\n",
    "                break\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def modeling_sanity_check(gen_length: int, enable_kv_cache: bool):\n",
    "    assert C_DEBUG == True, 'sanity check can only be performed under debug settings'\n",
    "    train_token_ids = next(iter(g_train_data))['token_ids'].tolist()\n",
    "    train_texts = [l.strip() for l in g_tokenizer.decode(train_token_ids).split('</s>')]\n",
    "    start_time = time.time()\n",
    "    gen_token_ids = generate(g_model, g_tokenizer, train_token_ids[:10],\n",
    "                             temperature=1.0, top_p=0.01, rep_penalty=1.0,\n",
    "                             total_tokens=gen_length,\n",
    "                             end_tokens=g_tokenizer.encode('<reserved_0>'),\n",
    "                             enable_kv_cache=enable_kv_cache)\n",
    "    cost_time = time.time() - start_time\n",
    "    print(f'Generation finished in {cost_time:.2f} sec(s), throughput: {len(gen_token_ids) / cost_time:.1f} tokens/sec')\n",
    "    # Complete check\n",
    "    cmp_length = min(len(train_token_ids), len(gen_token_ids))\n",
    "    print('Complete Identical:', train_token_ids[:cmp_length] == gen_token_ids[:cmp_length])\n",
    "    # Segment check\n",
    "    gen_texts = [l.strip() for l in g_tokenizer.decode(gen_token_ids).split('</s>')]\n",
    "    for i in range(min(len(train_texts), len(gen_texts))):\n",
    "        ref, real = train_texts[i], gen_texts[i]\n",
    "        cmp_length = min(len(ref), len(real))\n",
    "        print(f'Segment {i}: Ref Len: {len(ref)}, Gen Len: {len(real)} Identical: {ref[:cmp_length] == real[:cmp_length]}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93a3628aacbd1f80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for backend_option in [AttentionBackend.Naive, AttentionBackend.FlashAttentionTriton, AttentionBackend.FlashAttentionCuda]:\n",
    "    for enable_kv_cache_option in [True, False]:\n",
    "        print(f'Backend = {backend_option}, KVCache Enable = {enable_kv_cache_option}')\n",
    "        global_config['attn_backend'] = backend_option\n",
    "        modeling_sanity_check(512, enable_kv_cache_option)\n",
    "        print('=' * 80)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ffe9294382f18c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c17f13926df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in g_tokenizer.decode(next(iter(g_train_data))['token_ids'].tolist()).split('</s>')[:3]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc924753b5acfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "global_config['attn_backend'] = AttentionBackend.Naive\n",
    "result = generate(g_model, g_tokenizer, '<s>A chat between User and Assistant.\\nUser:Write me a story about Donald Trump and his alien friend.\\nAssistant:',\n",
    "                  temperature=1.0, top_p=0.3, rep_penalty=1.1,\n",
    "                  total_tokens=128,\n",
    "                  end_tokens=g_tokenizer.encode('</s>'),\n",
    "                  enable_kv_cache=True)\n",
    "time_cost = time.time() - time_start\n",
    "\n",
    "print(g_tokenizer.decode(result))\n",
    "print(f'{time_cost:.3f} sec(s), throughput {len(result) / time_cost:.1f} tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "80f5c7b6ebd5dbb9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

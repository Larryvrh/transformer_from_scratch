{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:38.011998261Z",
     "start_time": "2023-10-08T21:50:37.415044910Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tokenizers import WordTokenizer, CharTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "import random\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e76bcba3a904db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:38.121395285Z",
     "start_time": "2023-10-08T21:50:38.063025895Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./corpus/TinyStoriesV2-GPT4-valid.txt', 'r') as file:\n",
    "    lines = [l.strip() for l in file.read().split('<|endoftext|>')[:-1]]\n",
    "    raw_text = '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb151752ec125fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:38.595570330Z",
     "start_time": "2023-10-08T21:50:38.591392314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network definition\n",
    "C_SEQ_LEN = 512\n",
    "C_VOCAB_SIZE = 4096\n",
    "C_HIDDEN_SIZE = 384\n",
    "C_NUM_HEADS = 6\n",
    "C_NUM_LAYERS = 6\n",
    "\n",
    "C_DEVICE = torch.device('cuda')\n",
    "C_DTYPE = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33702a8e6613f742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:41.222282408Z",
     "start_time": "2023-10-08T21:50:39.187291873Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(raw_text, vocab_size=C_VOCAB_SIZE, reserved_vocab=['<s>', '</s>', '<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf7fc22d44a2d77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:43.680939179Z",
     "start_time": "2023-10-08T21:50:41.263901519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9967516696403725"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eval_vocab_coverage(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6236ac3649c35eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:45.888190988Z",
     "start_time": "2023-10-08T21:50:43.681358308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/27630 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "157381d68f35479cb2a7d324925e6a9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_samples = []\n",
    "for l in tqdm.tqdm(lines):\n",
    "    encoded_samples += tokenizer.encode('<s>' + l + '</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1685466c786f0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:45.956591752Z",
     "start_time": "2023-10-08T21:50:45.888513732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "for i in range(0, len(encoded_samples), C_SEQ_LEN):\n",
    "    chunks.append(encoded_samples[i:i + C_SEQ_LEN])\n",
    "chunks.pop(-1)\n",
    "all(len(c) == C_SEQ_LEN for c in chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb60031d199ad9a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:45.974528326Z",
     "start_time": "2023-10-08T21:50:45.956762786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 128])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_seq = torch.tensor([tokenizer.encode(raw_text[:10000])[:128 * 8]]).view((-1, 128))\n",
    "debug_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e442cf97d0bbe96e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:46.115188067Z",
     "start_time": "2023-10-08T21:50:46.073812129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 128])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_seq = torch.tensor(chunks)\n",
    "# train_seq.shape\n",
    "train_seq = debug_seq\n",
    "train_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "452c8b0c70869ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T18:48:19.136190103Z",
     "start_time": "2023-10-08T18:48:16.295917290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([256000, 13])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_tokenizer = CharTokenizer('0123456789+-*/=._$', 100)\n",
    "print(len(math_tokenizer.get_vocab_mapping()))\n",
    "samples = []\n",
    "for i in range(32000 * 8):\n",
    "    a = random.randint(1, 999)\n",
    "    b = random.randint(1, 999)\n",
    "    s = a + b\n",
    "    t = f'${a}+{b}={str(s)[::-1]}'\n",
    "    samples.append(t)\n",
    "max_sample_len = len(max(samples, key=lambda i: len(i)))\n",
    "samples = [s + '_' * (max_sample_len - len(s)) for s in samples]\n",
    "masks = [[0] * s.find('=') + [1] * (max_sample_len - s.find('=')) for s in samples]\n",
    "train_seq = torch.stack([torch.tensor(math_tokenizer.encode(s)) for s in samples])\n",
    "train_mask = torch.stack([torch.tensor(m) for m in masks])\n",
    "train_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f4430e3b69ca1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T18:48:19.151767295Z",
     "start_time": "2023-10-08T18:48:19.137164516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'$805+690=5941'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_tokenizer.decode(train_seq[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86f5eec2d44ff145",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T18:48:19.151861759Z",
     "start_time": "2023-10-08T18:48:19.139066193Z"
    }
   },
   "outputs": [],
   "source": [
    "C_SEQ_LEN = 13\n",
    "C_VOCAB_SIZE = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34d4a7cb7e49d5f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:50.410178595Z",
     "start_time": "2023-10-08T21:50:50.394970381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3658, -1.3014,  0.5082,  0.6040],\n",
      "         [-0.2335, -0.3141, -0.0363, -0.9091]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[0.4663, 0.2188, 0.6028, 0.0415]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def expand_attn_mask(custom_attn_mask: torch.Tensor):\n",
    "    B, T = custom_attn_mask.shape\n",
    "    mask = custom_attn_mask.unsqueeze(1).repeat((1, T, 1))\n",
    "    seq_index_mask = (mask == custom_attn_mask[:, torch.arange(T)].view(B, T, 1))\n",
    "    return seq_index_mask & (torch.tril(mask) > 0)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads: int, hidden_size: int, dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dtype = dtype\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size // num_heads, dtype=dtype)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size // num_heads, dtype=dtype)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size // num_heads, dtype=dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        mask_zero = torch.tensor(0, dtype=self.dtype)\n",
    "        mask_val = torch.tensor(torch.finfo(self.dtype).min / 2, dtype=self.dtype)\n",
    "        if kv_cache is None and attn_mask is not None:\n",
    "            causal_mask = torch.where(expand_attn_mask(attn_mask), mask_zero, mask_val)\n",
    "        elif kv_cache is None:\n",
    "            causal_mask = torch.where(expand_attn_mask(torch.ones(x.shape[:2])), mask_zero, mask_val)\n",
    "        else:\n",
    "            causal_mask = torch.zeros((B, T, T), dtype=self.dtype)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        if kv_cache is not None:\n",
    "            k = torch.concat([kv_cache[0], k], dim=1)\n",
    "            v = torch.concat([kv_cache[1], v], dim=1)\n",
    "\n",
    "        attn_score = (q @ k.permute(0, 2, 1) / (self.hidden_size ** 0.5)) + causal_mask.to(q.device)\n",
    "\n",
    "        return torch.softmax(attn_score, dim=2) @ v, [k, v]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, hidden_size: int, dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(num_heads, hidden_size, dtype) for _ in range(num_heads)])\n",
    "        self.o_proj = nn.Linear(hidden_size, hidden_size, dtype=dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        head_outputs = [head(x, attn_mask, kv_cache[idx] if kv_cache is not None else None) for idx, head in enumerate(self.attn_heads)]\n",
    "        return self.o_proj(torch.concat([o[0] for o in head_outputs], dim=2)), [o[1] for o in head_outputs]\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads: int, hidden_size: int, dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads, hidden_size, dtype)\n",
    "        self.up_proj = nn.Linear(hidden_size, hidden_size * 4, dtype=dtype)\n",
    "        self.down_proj = nn.Linear(hidden_size * 4, hidden_size, dtype=dtype)\n",
    "        self.ln_mha = nn.LayerNorm(hidden_size, dtype=dtype)\n",
    "        self.ln_ffn = nn.LayerNorm(hidden_size, dtype=dtype)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        mha_output, new_kv_cache = self.mha(self.ln_mha(x), attn_mask, kv_cache)\n",
    "        mha_output = x + mha_output\n",
    "        ffn_output = self.down_proj(self.act(self.up_proj(self.ln_ffn(mha_output))))\n",
    "        return mha_output + ffn_output, new_kv_cache\n",
    "\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, hidden_size: int, seq_len: int,\n",
    "                 dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.sem_embed = nn.Embedding(vocab_size, hidden_size, dtype=dtype)\n",
    "        self.pos_embed = nn.Embedding(seq_len, hidden_size, dtype=dtype)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(num_heads, hidden_size, dtype) for _ in range(num_layers)])\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, dtype=dtype)\n",
    "\n",
    "    def forward(self, seq: torch.Tensor,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                kv_cache: Optional[List[torch.Tensor]] = None) -> Tuple[torch.Tensor, List[List[List[torch.Tensor]]]]:\n",
    "\n",
    "        if position_ids is None:\n",
    "            seq_len = seq.shape[1]\n",
    "            pos_embed = self.pos_embed(torch.arange(0, seq_len, 1).to(self.device))\n",
    "        else:\n",
    "            pos_embed = self.pos_embed(position_ids)\n",
    "\n",
    "        hidden = self.sem_embed(seq) + pos_embed\n",
    "        new_kv_cache = []\n",
    "        for idx, decoder in enumerate(self.decoder_layers):\n",
    "            hidden, layer_kv_cache = decoder(hidden, attn_mask, kv_cache[idx] if kv_cache is not None else None)\n",
    "            new_kv_cache.append(layer_kv_cache)\n",
    "\n",
    "        return self.lm_head(hidden), new_kv_cache\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "\n",
    "t = ToyTransformer(4, 1, 1, 4, 4)\n",
    "o1, kv1 = t.forward(torch.tensor([[0, 1]]))\n",
    "print(o1)\n",
    "\n",
    "o2, kv2 = t.forward(torch.tensor([[0]]))\n",
    "o3, kv3 = t.forward(torch.tensor([[1]]), kv_cache=kv2)\n",
    "print(o3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d124589733f9ed1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:55.103930430Z",
     "start_time": "2023-10-08T21:50:54.455842418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 13993216\n"
     ]
    }
   ],
   "source": [
    "model = ToyTransformer(C_VOCAB_SIZE, C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN)\n",
    "model = model.to(C_DEVICE)\n",
    "print('Total parameters:', sum([t.numel() for t in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "ToyTransformer(\n  (sem_embed): Embedding(4096, 384)\n  (pos_embed): Embedding(512, 384)\n  (decoder_layers): ModuleList(\n    (0-5): 6 x DecoderLayer(\n      (mha): MultiHeadAttention(\n        (attn_heads): ModuleList(\n          (0-5): 6 x AttentionHead(\n            (q_proj): Linear(in_features=384, out_features=64, bias=True)\n            (k_proj): Linear(in_features=384, out_features=64, bias=True)\n            (v_proj): Linear(in_features=384, out_features=64, bias=True)\n          )\n        )\n        (o_proj): Linear(in_features=384, out_features=384, bias=True)\n      )\n      (up_proj): Linear(in_features=384, out_features=1536, bias=True)\n      (down_proj): Linear(in_features=1536, out_features=384, bias=True)\n      (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln_ffn): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (act): GELU(approximate='none')\n    )\n  )\n  (lm_head): Linear(in_features=384, out_features=4096, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:56.180353997Z",
     "start_time": "2023-10-08T21:50:56.177425059Z"
    }
   },
   "id": "f906aed5"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=2000, final_div_factor=1e2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T21:50:58.899527349Z",
     "start_time": "2023-10-08T21:50:58.893317515Z"
    }
   },
   "id": "6704e7b3092a4c0c"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-10-08T21:51:23.784574005Z",
     "start_time": "2023-10-08T21:51:20.630993537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step 1 - Loss: 1.217 LR: 0.000105 Throughput: 2068.21 kts\n",
      "Epoch 1 Step 1 - Loss: 1.185 LR: 0.000106 Throughput: 1674.69 kts\n",
      "Epoch 2 Step 1 - Loss: 1.153 LR: 0.000107 Throughput: 2552.68 kts\n",
      "Epoch 3 Step 1 - Loss: 1.122 LR: 0.000108 Throughput: 2494.42 kts\n",
      "Epoch 4 Step 1 - Loss: 1.091 LR: 0.00011 Throughput: 2392.47 kts\n",
      "Epoch 5 Step 1 - Loss: 1.060 LR: 0.000111 Throughput: 2196.12 kts\n",
      "Epoch 6 Step 1 - Loss: 1.029 LR: 0.000112 Throughput: 2061.02 kts\n",
      "Epoch 7 Step 1 - Loss: 0.999 LR: 0.000114 Throughput: 1808.82 kts\n",
      "Epoch 8 Step 1 - Loss: 0.968 LR: 0.000115 Throughput: 1855.0 kts\n",
      "Epoch 9 Step 1 - Loss: 0.938 LR: 0.000116 Throughput: 1600.29 kts\n",
      "Epoch 10 Step 1 - Loss: 0.909 LR: 0.000118 Throughput: 2556.2 kts\n",
      "Epoch 11 Step 1 - Loss: 0.880 LR: 0.000119 Throughput: 1597.31 kts\n",
      "Epoch 12 Step 1 - Loss: 0.852 LR: 0.00012 Throughput: 2486.21 kts\n",
      "Epoch 13 Step 1 - Loss: 0.825 LR: 0.000122 Throughput: 1799.86 kts\n",
      "Epoch 14 Step 1 - Loss: 0.799 LR: 0.000123 Throughput: 1918.04 kts\n",
      "Epoch 15 Step 1 - Loss: 0.774 LR: 0.000125 Throughput: 2282.58 kts\n",
      "Epoch 16 Step 1 - Loss: 0.750 LR: 0.000126 Throughput: 2735.43 kts\n",
      "Epoch 17 Step 1 - Loss: 0.727 LR: 0.000128 Throughput: 2616.12 kts\n",
      "Epoch 18 Step 1 - Loss: 0.705 LR: 0.000129 Throughput: 2552.04 kts\n",
      "Epoch 19 Step 1 - Loss: 0.685 LR: 0.00013 Throughput: 2305.33 kts\n",
      "Epoch 20 Step 1 - Loss: 0.665 LR: 0.000132 Throughput: 2428.94 kts\n",
      "Epoch 21 Step 1 - Loss: 0.646 LR: 0.000133 Throughput: 1955.54 kts\n",
      "Epoch 22 Step 1 - Loss: 0.628 LR: 0.000135 Throughput: 2330.72 kts\n",
      "Epoch 23 Step 1 - Loss: 0.611 LR: 0.000136 Throughput: 2420.98 kts\n",
      "Epoch 24 Step 1 - Loss: 0.594 LR: 0.000138 Throughput: 2527.03 kts\n",
      "Epoch 25 Step 1 - Loss: 0.578 LR: 0.00014 Throughput: 2475.6 kts\n",
      "Epoch 26 Step 1 - Loss: 0.563 LR: 0.000141 Throughput: 1983.06 kts\n",
      "Epoch 27 Step 1 - Loss: 0.549 LR: 0.000143 Throughput: 2461.7 kts\n",
      "Epoch 28 Step 1 - Loss: 0.536 LR: 0.000144 Throughput: 1967.63 kts\n",
      "Epoch 29 Step 1 - Loss: 0.524 LR: 0.000146 Throughput: 1345.4 kts\n",
      "Epoch 30 Step 1 - Loss: 0.512 LR: 0.000147 Throughput: 1685.15 kts\n",
      "Epoch 31 Step 1 - Loss: 0.500 LR: 0.000149 Throughput: 1959.19 kts\n",
      "Epoch 32 Step 1 - Loss: 0.489 LR: 0.000151 Throughput: 1683.19 kts\n",
      "Epoch 33 Step 1 - Loss: 0.478 LR: 0.000152 Throughput: 2109.74 kts\n",
      "Epoch 34 Step 1 - Loss: 0.467 LR: 0.000154 Throughput: 2189.42 kts\n",
      "Epoch 35 Step 1 - Loss: 0.456 LR: 0.000155 Throughput: 2052.61 kts\n",
      "Epoch 36 Step 1 - Loss: 0.445 LR: 0.000157 Throughput: 2285.07 kts\n",
      "Epoch 37 Step 1 - Loss: 0.434 LR: 0.000159 Throughput: 2266.79 kts\n",
      "Epoch 38 Step 1 - Loss: 0.423 LR: 0.00016 Throughput: 2085.58 kts\n",
      "Epoch 39 Step 1 - Loss: 0.412 LR: 0.000162 Throughput: 2418.93 kts\n",
      "Epoch 40 Step 1 - Loss: 0.401 LR: 0.000164 Throughput: 2517.36 kts\n",
      "Epoch 41 Step 1 - Loss: 0.390 LR: 0.000165 Throughput: 2487.29 kts\n",
      "Epoch 42 Step 1 - Loss: 0.378 LR: 0.000167 Throughput: 2200.13 kts\n",
      "Epoch 43 Step 1 - Loss: 0.366 LR: 0.000169 Throughput: 1402.37 kts\n",
      "Epoch 44 Step 1 - Loss: 0.355 LR: 0.000171 Throughput: 2143.38 kts\n",
      "Epoch 45 Step 1 - Loss: 0.343 LR: 0.000172 Throughput: 2786.31 kts\n",
      "Epoch 46 Step 1 - Loss: 0.331 LR: 0.000174 Throughput: 2606.07 kts\n",
      "Epoch 47 Step 1 - Loss: 0.319 LR: 0.000176 Throughput: 2208.33 kts\n",
      "Epoch 48 Step 1 - Loss: 0.307 LR: 0.000177 Throughput: 2050.33 kts\n",
      "Epoch 49 Step 1 - Loss: 0.294 LR: 0.000179 Throughput: 2329.65 kts\n",
      "Epoch 50 Step 1 - Loss: 0.282 LR: 0.000181 Throughput: 2857.06 kts\n",
      "Epoch 51 Step 1 - Loss: 0.270 LR: 0.000183 Throughput: 2419.17 kts\n",
      "Epoch 52 Step 1 - Loss: 0.258 LR: 0.000185 Throughput: 2579.78 kts\n",
      "Epoch 53 Step 1 - Loss: 0.246 LR: 0.000186 Throughput: 2048.32 kts\n",
      "Epoch 54 Step 1 - Loss: 0.234 LR: 0.000188 Throughput: 2242.58 kts\n",
      "Epoch 55 Step 1 - Loss: 0.222 LR: 0.00019 Throughput: 1567.42 kts\n",
      "Epoch 56 Step 1 - Loss: 0.210 LR: 0.000192 Throughput: 2230.1 kts\n",
      "Epoch 57 Step 1 - Loss: 0.198 LR: 0.000194 Throughput: 1787.59 kts\n",
      "Epoch 58 Step 1 - Loss: 0.187 LR: 0.000196 Throughput: 2545.21 kts\n",
      "Epoch 59 Step 1 - Loss: 0.176 LR: 0.000197 Throughput: 2076.65 kts\n",
      "Epoch 60 Step 1 - Loss: 0.166 LR: 0.000199 Throughput: 1643.89 kts\n",
      "Epoch 61 Step 1 - Loss: 0.155 LR: 0.000201 Throughput: 2500.5 kts\n",
      "Epoch 62 Step 1 - Loss: 0.145 LR: 0.000203 Throughput: 1981.36 kts\n",
      "Epoch 63 Step 1 - Loss: 0.136 LR: 0.000205 Throughput: 2296.81 kts\n",
      "Epoch 64 Step 1 - Loss: 0.127 LR: 0.000207 Throughput: 2713.34 kts\n",
      "Epoch 65 Step 1 - Loss: 0.118 LR: 0.000209 Throughput: 2512.55 kts\n",
      "Epoch 66 Step 1 - Loss: 0.110 LR: 0.000211 Throughput: 2172.74 kts\n",
      "Epoch 67 Step 1 - Loss: 0.102 LR: 0.000213 Throughput: 2213.9 kts\n",
      "Epoch 68 Step 1 - Loss: 0.095 LR: 0.000215 Throughput: 2025.99 kts\n",
      "Epoch 69 Step 1 - Loss: 0.089 LR: 0.000217 Throughput: 2395.62 kts\n",
      "Epoch 70 Step 1 - Loss: 0.083 LR: 0.000218 Throughput: 2372.32 kts\n",
      "Epoch 71 Step 1 - Loss: 0.077 LR: 0.00022 Throughput: 2540.09 kts\n",
      "Epoch 72 Step 1 - Loss: 0.072 LR: 0.000222 Throughput: 2580.58 kts\n",
      "Epoch 73 Step 1 - Loss: 0.067 LR: 0.000224 Throughput: 1909.59 kts\n",
      "Epoch 74 Step 1 - Loss: 0.062 LR: 0.000226 Throughput: 1878.79 kts\n",
      "Epoch 75 Step 1 - Loss: 0.058 LR: 0.000228 Throughput: 1847.07 kts\n",
      "Epoch 76 Step 1 - Loss: 0.054 LR: 0.00023 Throughput: 1561.19 kts\n",
      "Epoch 77 Step 1 - Loss: 0.050 LR: 0.000232 Throughput: 1445.0 kts\n",
      "Epoch 78 Step 1 - Loss: 0.047 LR: 0.000234 Throughput: 1446.12 kts\n",
      "Epoch 79 Step 1 - Loss: 0.044 LR: 0.000236 Throughput: 1747.5 kts\n",
      "Epoch 80 Step 1 - Loss: 0.041 LR: 0.000238 Throughput: 2858.99 kts\n",
      "Epoch 81 Step 1 - Loss: 0.039 LR: 0.000241 Throughput: 2137.81 kts\n",
      "Epoch 82 Step 1 - Loss: 0.036 LR: 0.000243 Throughput: 2400.83 kts\n",
      "Epoch 83 Step 1 - Loss: 0.034 LR: 0.000245 Throughput: 1797.74 kts\n",
      "Epoch 84 Step 1 - Loss: 0.032 LR: 0.000247 Throughput: 2302.41 kts\n",
      "Epoch 85 Step 1 - Loss: 0.030 LR: 0.000249 Throughput: 2007.14 kts\n",
      "Epoch 86 Step 1 - Loss: 0.028 LR: 0.000251 Throughput: 2302.84 kts\n",
      "Epoch 87 Step 1 - Loss: 0.026 LR: 0.000253 Throughput: 2206.24 kts\n",
      "Epoch 88 Step 1 - Loss: 0.025 LR: 0.000255 Throughput: 2117.33 kts\n",
      "Epoch 89 Step 1 - Loss: 0.024 LR: 0.000257 Throughput: 1961.33 kts\n",
      "Epoch 90 Step 1 - Loss: 0.022 LR: 0.000259 Throughput: 1997.28 kts\n",
      "Epoch 91 Step 1 - Loss: 0.021 LR: 0.000261 Throughput: 2373.89 kts\n",
      "Epoch 92 Step 1 - Loss: 0.020 LR: 0.000263 Throughput: 2665.04 kts\n",
      "Epoch 93 Step 1 - Loss: 0.019 LR: 0.000266 Throughput: 2436.0 kts\n",
      "Epoch 94 Step 1 - Loss: 0.018 LR: 0.000268 Throughput: 2516.28 kts\n",
      "Epoch 95 Step 1 - Loss: 0.017 LR: 0.00027 Throughput: 2524.11 kts\n",
      "Epoch 96 Step 1 - Loss: 0.016 LR: 0.000272 Throughput: 2562.73 kts\n",
      "Epoch 97 Step 1 - Loss: 0.016 LR: 0.000274 Throughput: 2328.09 kts\n",
      "Epoch 98 Step 1 - Loss: 0.015 LR: 0.000276 Throughput: 1961.0 kts\n",
      "Epoch 99 Step 1 - Loss: 0.014 LR: 0.000279 Throughput: 1719.49 kts\n"
     ]
    }
   ],
   "source": [
    "C_BATCH_SIZE = 128\n",
    "for epoch_num in range(100):\n",
    "    for batch_i in list(range(0, len(train_seq), C_BATCH_SIZE)):\n",
    "        step_start_time = time.time()\n",
    "\n",
    "        inputs = train_seq[batch_i:batch_i + C_BATCH_SIZE, :-1]\n",
    "        labels = train_seq[batch_i:batch_i + C_BATCH_SIZE, 1:]\n",
    "        # masks = train_mask[batch_i:batch_i + C_BATCH_SIZE, 1:].to(model.device, C_DTYPE)\n",
    "\n",
    "        logits, _ = model.forward(inputs.to(model.device))\n",
    "        probs = torch.softmax(logits, dim=2)  # BSZ * SEQ * VOCAB\n",
    "        probs_flat = probs.view(-1, C_VOCAB_SIZE)\n",
    "        #  * masks.reshape(-1)\n",
    "        loss = (-torch.log(probs_flat[torch.arange(probs_flat.shape[0]), labels.reshape(-1)])).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step_time_cost = time.time() - step_start_time\n",
    "        throughput = round((C_BATCH_SIZE * C_SEQ_LEN) / step_time_cost / 1000, 2)\n",
    "        print(\n",
    "            f'Epoch {epoch_num} Step {batch_i // C_BATCH_SIZE + 1} - Loss: {loss.item():.3f} LR: {scheduler.get_last_lr()[0]:.3} '\n",
    "            f'Throughput: {throughput} kts')\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e800403549c98d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:51:50.364545134Z",
     "start_time": "2023-10-08T21:51:50.321644603Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(tokenizer, prompt, temperature, top_p, rep_penalty, max_new_tokens=20, total_tokens=None):\n",
    "    feed_tokens = tokenizer.encode(prompt)\n",
    "    all_tokens = feed_tokens.copy()\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(feed_tokens))\n",
    "\n",
    "    kv_cache = None\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, kv_cache = model.forward(torch.tensor([feed_tokens]).to(C_DEVICE), \n",
    "                                         position_ids=None if kv_cache is None else torch.tensor([[len(all_tokens) - 1]]).to(C_DEVICE), \n",
    "                                         kv_cache=kv_cache)\n",
    "        logits = logits[0][-1].cpu()\n",
    "\n",
    "        # apply repetition penalty\n",
    "        logits_rep = torch.gather(logits, 0, torch.tensor(all_tokens))\n",
    "        logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "        logits.scatter_(0, torch.tensor(all_tokens), logits_rep)\n",
    "\n",
    "        # apply temperature\n",
    "        logits /= max(temperature, 1e-6)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "        # apply top-p\n",
    "        ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "        cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "        top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "        ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "        sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "        all_tokens.append(sampled_index)\n",
    "        feed_tokens = [sampled_index]\n",
    "    # print(tokens)\n",
    "    return tokenizer.decode(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bc924753b5acfc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:51:54.841525210Z",
     "start_time": "2023-10-08T21:51:53.598268186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> don't have to be scared of the loud dog, I'll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\n",
      "Once upon a time, in a warm and best. the she she and . a girl. her and and and and warm mole and the she she you mole mole in mole and and her a mole her her in the had and and mole and in and and to and in in warm and against she was her of she and and . in girl mole to her . mole \n",
      "1.241924524307251\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "print(generate(tokenizer, '<unk>', 1.0, 0.01, 1.0, total_tokens=256))\n",
    "print(time.time() - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "83",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[153], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmath_tokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m$111+11=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrep_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m13\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mrstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[152], line 33\u001B[0m, in \u001B[0;36mgenerate\u001B[0;34m(tokenizer, prompt, temperature, top_p, rep_penalty, max_new_tokens, total_tokens)\u001B[0m\n\u001B[1;32m     31\u001B[0m     feed_tokens \u001B[38;5;241m=\u001B[39m [sampled_index]\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# print(tokens)\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_tokens\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/StorageExternal/Code/nlp_from_scratch/tokenizers.py:62\u001B[0m, in \u001B[0;36mCharTokenizer.decode\u001B[0;34m(self, ids)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, ids: List[\u001B[38;5;28mint\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mi2s\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mids\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/StorageExternal/Code/nlp_from_scratch/tokenizers.py:62\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, ids: List[\u001B[38;5;28mint\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mi2s\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m ids)\n",
      "\u001B[0;31mKeyError\u001B[0m: 83"
     ]
    }
   ],
   "source": [
    "generate(math_tokenizer, f\"$111+11=\", temperature=1.0, top_p=0.001, rep_penalty=1.0, total_tokens=13).rstrip('_')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-08T21:26:58.782497914Z"
    }
   },
   "id": "acfd06574fc4aae5"
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "126",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[191], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m120\u001B[39m):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m120\u001B[39m):\n\u001B[0;32m----> 4\u001B[0m         s \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmath_tokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m$\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43ma\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m+\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mb\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrep_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mtotal_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mrstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m         r \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(s[s\u001B[38;5;241m.\u001B[39mrfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m:][::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;28mprint\u001B[39m(a, b, r)\n",
      "Cell \u001B[0;32mIn[187], line 34\u001B[0m, in \u001B[0;36mgenerate\u001B[0;34m(tokenizer, prompt, temperature, top_p, rep_penalty, max_new_tokens, total_tokens)\u001B[0m\n\u001B[1;32m     32\u001B[0m     feed_tokens \u001B[38;5;241m=\u001B[39m [sampled_index]\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# print(tokens)\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_tokens\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/StorageExternal/Code/nlp_from_scratch/tokenizers.py:62\u001B[0m, in \u001B[0;36mCharTokenizer.decode\u001B[0;34m(self, ids)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, ids: List[\u001B[38;5;28mint\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mi2s\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mids\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/StorageExternal/Code/nlp_from_scratch/tokenizers.py:62\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, ids: List[\u001B[38;5;28mint\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mi2s\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m ids)\n",
      "\u001B[0;31mKeyError\u001B[0m: 126"
     ]
    }
   ],
   "source": [
    "correct_vs_total = [0, 0]\n",
    "for a in range(100, 120):\n",
    "    for b in range(100, 120):\n",
    "        s = generate(math_tokenizer, f\"${a}+{b}=\", temperature=1.0, top_p=0.01, rep_penalty=1.0,\n",
    "                     total_tokens=12).rstrip('_')\n",
    "        r = int(s[s.rfind('=') + 1:][::-1])\n",
    "        print(a, b, r)\n",
    "        correct_vs_total[0] += (a + b) == r\n",
    "        correct_vs_total[1] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T21:47:58.473663194Z",
     "start_time": "2023-10-08T21:47:58.439779743Z"
    }
   },
   "id": "da95e6c5756329ee"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "[400, 400]"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_vs_total"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-08T18:58:02.816461760Z"
    }
   },
   "id": "a6da265cd850c26c"
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "40531b0cf58345f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T21:49:57.429137884Z",
     "start_time": "2023-10-08T21:49:57.410872766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'<unk> don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\nOnce upon a time, in a warm and '"
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_seq[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfb4a3c8dcbb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(debug_seq[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191841b1a87d1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(torch.finfo(torch.bfloat16).min / 2, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T21:18:51.769594235Z",
     "start_time": "2023-10-08T21:18:51.753472754Z"
    }
   },
   "id": "3a25faa06d3e1a9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6b62aa4b37ffe30"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-10-11T18:19:03.664262Z",
     "start_time": "2023-10-11T18:19:02.890201Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tokenizers import WordTokenizer, CharTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "import random\n",
    "from typing import *\n",
    "import gc\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e76bcba3a904db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T18:19:04.190858Z",
     "start_time": "2023-10-11T18:19:04.132264Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./corpus/TinyStoriesV2-GPT4-valid.txt', 'r') as file:\n",
    "    raw_text = file.read()\n",
    "    lines = [l.strip() for l in raw_text.split('<|endoftext|>')[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb151752ec125fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T18:19:04.943896Z",
     "start_time": "2023-10-11T18:19:04.935547Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network definition\n",
    "C_SEQ_LEN = 512\n",
    "C_VOCAB_SIZE = 4096\n",
    "C_HIDDEN_SIZE = 512\n",
    "C_NUM_HEADS = 8\n",
    "C_NUM_LAYERS = 8\n",
    "\n",
    "C_DEVICE = torch.device('cuda')\n",
    "C_DTYPE = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "22493387"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T18:19:06.435657Z",
     "start_time": "2023-10-11T18:19:06.425059Z"
    }
   },
   "id": "111829d7be3d3949"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33702a8e6613f742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T18:19:10.261209Z",
     "start_time": "2023-10-11T18:19:07.194674Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(raw_text, vocab_size=C_VOCAB_SIZE, reserved_vocab=['<s>', '</s>', '<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf7fc22d44a2d77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T18:19:15.982530Z",
     "start_time": "2023-10-11T18:19:12.006925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9968006233353223"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eval_vocab_coverage(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6236ac3649c35eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T18:19:15.982805Z",
     "start_time": "2023-10-11T18:19:15.979198Z"
    }
   },
   "outputs": [],
   "source": [
    "# token_ids, position_ids, attn_mask, loss_mask = [[]], [[]], [[]], None\n",
    "# mask_index = 1\n",
    "# for l in tqdm.tqdm(lines):\n",
    "#     cursor = 0\n",
    "#     sample_token_ids = tokenizer.encode('<s>' + l + '</s>')\n",
    "#     if len(sample_token_ids) > C_SEQ_LEN:\n",
    "#         continue\n",
    "#     sample_position_ids = list(range(len(sample_token_ids)))\n",
    "#     while cursor < len(sample_token_ids):\n",
    "#         length = min(C_SEQ_LEN - len(token_ids[-1]), len(sample_token_ids) - cursor)\n",
    "#         token_ids[-1] += sample_token_ids[cursor:cursor + length]\n",
    "#         position_ids[-1] += sample_position_ids[cursor:cursor + length]\n",
    "#         attn_mask[-1] += [mask_index] * length\n",
    "#         cursor += length\n",
    "#         mask_index += 1\n",
    "#         if len(token_ids[-1]) == C_SEQ_LEN:\n",
    "#             token_ids.append([])\n",
    "#             position_ids.append([])\n",
    "#             attn_mask.append([])\n",
    "#             mask_index = 1\n",
    "# token_ids = torch.tensor(token_ids[:-1])\n",
    "# position_ids = torch.tensor(position_ids[:-1])\n",
    "# attn_mask = torch.tensor(attn_mask[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with open('tiny_stories_tokenized.pt', 'wb') as file:\n",
    "#     torch.save([token_ids, position_ids, attn_mask], file)\n",
    "with open('tiny_stories_tokenized.pt', 'rb') as file:\n",
    "    token_ids, position_ids, attn_mask = torch.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "799b78921dbff1ec"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb60031d199ad9a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T18:19:30.502715Z",
     "start_time": "2023-10-11T18:19:30.495535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 128])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_seq = torch.tensor([tokenizer.encode(raw_text[:10000])[:128 * 8]]).view((-1, 128))\n",
    "debug_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "34d4a7cb7e49d5f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T21:05:49.496317Z",
     "start_time": "2023-10-11T21:05:49.492318Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int = -1,\n",
    "    num_layers: int = -1,\n",
    "    num_heads: int = -1,\n",
    "    hidden_size: int = -1,\n",
    "    max_seq_len: int = -1,\n",
    "    root_model: 'ToyTransformer' = None,\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    enable_rel_pos: bool = False\n",
    "    enable_fast_attn: bool = True\n",
    "\n",
    "\n",
    "def expand_attn_mask(custom_attn_mask: torch.Tensor):\n",
    "    B, T = custom_attn_mask.shape\n",
    "    mask = custom_attn_mask.unsqueeze(1).repeat((1, T, 1))\n",
    "    seq_index_mask = (mask == custom_attn_mask[:, torch.arange(T)].view(B, T, 1))\n",
    "    return seq_index_mask & (torch.tril(mask) > 0)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.dtype = config.dtype\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        mask_zero = torch.tensor(0, dtype=self.dtype)\n",
    "        mask_val = torch.tensor(torch.finfo(self.dtype).min / 2, dtype=self.dtype)\n",
    "        if kv_cache is None and attn_mask is not None:\n",
    "            causal_mask = expand_attn_mask(attn_mask)\n",
    "        elif kv_cache is None:\n",
    "            causal_mask = expand_attn_mask(torch.ones(x.shape[:2]))\n",
    "        else:\n",
    "            causal_mask = torch.ones((B, T, T), dtype=torch.bool)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        if kv_cache is not None:\n",
    "            k = torch.concat([kv_cache[0], k], dim=1)\n",
    "            v = torch.concat([kv_cache[1], v], dim=1)\n",
    "\n",
    "        if self.config.enable_rel_pos:\n",
    "            rel_pos = torch.tensor([[s + (self.config.max_seq_len - 1) for s in range(-i, T - i)] for i in range(T)])\n",
    "            rel_emb = self.config.root_model.pos_embed(rel_pos)\n",
    "            c = rel_emb.shape[-1]\n",
    "            rel_q = q.unsqueeze(1).repeat(1, 1, 1, T) + rel_emb.view(-1, T * c)\n",
    "            rel_k = k.unsqueeze(1).repeat(1, T, 1, 1)\n",
    "            attn_score = torch.sum(rel_q.view(B, T * T, c) * rel_k.view(B, T * T, c), dim=2).view(B, T, T)\n",
    "            attn_score = attn_score / (self.hidden_size ** 0.5) + causal_mask.to(q.device)\n",
    "            attn_result = torch.softmax(attn_score, dim=2) @ v\n",
    "        elif self.config.enable_fast_attn:\n",
    "            # noinspection PyUnresolvedReferences\n",
    "            with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):\n",
    "                attn_result = nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=causal_mask.to(q.device))\n",
    "        else:\n",
    "            causal_mask = torch.where(causal_mask, mask_zero, mask_val)\n",
    "            attn_score = (q @ k.permute(0, 2, 1) / (self.hidden_size ** 0.5)) + causal_mask.to(q.device)\n",
    "            attn_result = torch.softmax(attn_score, dim=2) @ v\n",
    "\n",
    "        return attn_result, [k, v]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(config) for _ in range(config.num_heads)])\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, dtype=config.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        head_outputs = [head(x, attn_mask, kv_cache[idx] if kv_cache is not None else None) for idx, head in\n",
    "                        enumerate(self.attn_heads)]\n",
    "        return self.o_proj(torch.concat([o[0] for o in head_outputs], dim=2)), [o[1] for o in head_outputs]\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.hidden_size * 4, dtype=config.dtype)\n",
    "        self.down_proj = nn.Linear(config.hidden_size * 4, config.hidden_size, dtype=config.dtype)\n",
    "        self.ln_mha = nn.LayerNorm(config.hidden_size, dtype=config.dtype)\n",
    "        self.ln_ffn = nn.LayerNorm(config.hidden_size, dtype=config.dtype)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        mha_output, new_kv_cache = self.mha(self.ln_mha(x), attn_mask, kv_cache)\n",
    "        mha_output = x + mha_output\n",
    "        ffn_output = self.down_proj(self.act(self.up_proj(self.ln_ffn(mha_output))))\n",
    "        return mha_output + ffn_output, new_kv_cache\n",
    "\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, hidden_size: int, max_seq_len: int,\n",
    "                 dtype: torch.dtype = torch.float32,\n",
    "                 enable_rel_pos: bool = False, enable_fast_attn: bool = False):\n",
    "        super().__init__()\n",
    "        self.config = TransformerConfig(vocab_size, num_layers, num_heads, hidden_size, max_seq_len, self, dtype,\n",
    "                                        enable_rel_pos, enable_fast_attn)\n",
    "\n",
    "        self.sem_embed = nn.Embedding(vocab_size, hidden_size, dtype=dtype)\n",
    "\n",
    "        if not self.config.enable_rel_pos:\n",
    "            self.pos_embed = nn.Embedding(max_seq_len, hidden_size, dtype=dtype)\n",
    "        else:\n",
    "            self.pos_embed = nn.Embedding(max_seq_len * 2 - 1, hidden_size // num_heads, dtype=dtype)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(num_layers)])\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, dtype=dtype)\n",
    "\n",
    "    def forward(self, seq: torch.Tensor,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                kv_cache: Optional[List[torch.Tensor]] = None) -> Tuple[torch.Tensor, List[List[List[torch.Tensor]]]]:\n",
    "\n",
    "        if self.config.enable_rel_pos:\n",
    "            hidden = self.sem_embed(seq)\n",
    "        elif position_ids is not None:\n",
    "            hidden = self.sem_embed(seq) + self.pos_embed(position_ids)\n",
    "        else:\n",
    "            hidden = self.sem_embed(seq) + self.pos_embed(torch.arange(0, seq.shape[1], 1).to(self.device))\n",
    "\n",
    "        new_kv_cache = []\n",
    "        for idx, decoder in enumerate(self.decoder_layers):\n",
    "            hidden, layer_kv_cache = decoder(hidden, attn_mask, kv_cache[idx] if kv_cache is not None else None)\n",
    "            new_kv_cache.append(layer_kv_cache)\n",
    "\n",
    "        return self.lm_head(hidden), new_kv_cache\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2956288\n"
     ]
    }
   ],
   "source": [
    "debug_model = ToyTransformer(C_VOCAB_SIZE, 1, 1, 256, 128, enable_fast_attn=True, enable_rel_pos=True)\n",
    "print('Total parameters:', sum([t.numel() for t in debug_model.parameters()]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:05:49.993776Z",
     "start_time": "2023-10-11T21:05:49.971871Z"
    }
   },
   "id": "dda0053a2d0e7da2"
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "d124589733f9ed1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T21:05:51.075322Z",
     "start_time": "2023-10-11T21:05:50.928794Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [466]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m ToyTransformer(C_VOCAB_SIZE, C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN, C_DTYPE)\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mC_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTotal parameters:\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28msum\u001B[39m([t\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mparameters()]))\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m-> 1145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m-> 1143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = ToyTransformer(C_VOCAB_SIZE, C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN, C_DTYPE)\n",
    "model = model.to(C_DEVICE)\n",
    "print('Total parameters:', sum([t.numel() for t in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "outputs": [
    {
     "data": {
      "text/plain": "ToyTransformer(\n  (sem_embed): Embedding(4096, 512)\n  (pos_embed): Embedding(512, 512)\n  (decoder_layers): ModuleList(\n    (0-7): 8 x DecoderLayer(\n      (mha): MultiHeadAttention(\n        (attn_heads): ModuleList(\n          (0-7): 8 x AttentionHead(\n            (q_proj): Linear(in_features=512, out_features=64, bias=True)\n            (k_proj): Linear(in_features=512, out_features=64, bias=True)\n            (v_proj): Linear(in_features=512, out_features=64, bias=True)\n          )\n        )\n        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (up_proj): Linear(in_features=512, out_features=2048, bias=True)\n      (down_proj): Linear(in_features=2048, out_features=512, bias=True)\n      (ln_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (ln_ffn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (act): GELU(approximate='none')\n    )\n  )\n  (lm_head): Linear(in_features=512, out_features=4096, bias=True)\n)"
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:05:52.348076Z",
     "start_time": "2023-10-11T21:05:52.335280Z"
    }
   },
   "id": "f906aed5"
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tiny_stories_0.8_epoch.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [468]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./tiny_stories_0.8_epoch.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/serialization.py:791\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    789\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 791\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m    792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m    793\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m    794\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m    795\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m    796\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/serialization.py:271\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 271\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    273\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/serialization.py:252\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 252\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './tiny_stories_0.8_epoch.pt'"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./tiny_stories_0.8_epoch.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:05:53.836990Z",
     "start_time": "2023-10-11T21:05:53.712894Z"
    }
   },
   "id": "7cb50733c6a6feaf"
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:05:58.170224Z",
     "start_time": "2023-10-11T21:05:58.166096Z"
    }
   },
   "id": "dca6004e9b65892c"
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:07:04.444719Z",
     "start_time": "2023-10-11T21:06:53.238600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.175, LR:4e-05, Throughput:33826.77 kt/s\n",
      "Loss:0.177, LR:6.8948e-05, Throughput:37852.12 kt/s\n",
      "Loss:0.186, LR:0.0001523, Throughput:30882.92 kt/s\n",
      "Loss:0.186, LR:0.00028, Throughput:34543.33 kt/s\n",
      "Loss:0.192, LR:0.00043665, Throughput:36646.16 kt/s\n",
      "Loss:0.249, LR:0.00060335, Throughput:36375.5 kt/s\n",
      "Loss:0.637, LR:0.00076, Throughput:37838.25 kt/s\n",
      "Loss:1.039, LR:0.0008877, Throughput:36243.62 kt/s\n",
      "Loss:0.462, LR:0.00097105, Throughput:37791.96 kt/s\n",
      "Loss:0.864, LR:0.001, Throughput:36520.1 kt/s\n",
      "Loss:0.394, LR:0.0009997, Throughput:37147.24 kt/s\n",
      "Loss:0.435, LR:0.00099879, Throughput:36382.78 kt/s\n",
      "Loss:0.518, LR:0.00099727, Throughput:37510.23 kt/s\n",
      "Loss:0.287, LR:0.00099515, Throughput:37819.48 kt/s\n",
      "Loss:0.313, LR:0.00099243, Throughput:38017.42 kt/s\n",
      "Loss:0.328, LR:0.00098912, Throughput:36183.14 kt/s\n",
      "Loss:0.247, LR:0.00098521, Throughput:36830.97 kt/s\n",
      "Loss:0.233, LR:0.00098071, Throughput:36804.25 kt/s\n",
      "Loss:0.260, LR:0.00097563, Throughput:38272.92 kt/s\n",
      "Loss:0.234, LR:0.00096997, Throughput:37169.23 kt/s\n",
      "Loss:0.211, LR:0.00096374, Throughput:37765.22 kt/s\n",
      "Loss:0.204, LR:0.00095695, Throughput:37019.77 kt/s\n",
      "Loss:0.198, LR:0.0009496, Throughput:37491.38 kt/s\n",
      "Loss:0.185, LR:0.00094171, Throughput:37642.16 kt/s\n",
      "Loss:0.180, LR:0.00093328, Throughput:37675.88 kt/s\n",
      "Loss:0.174, LR:0.00092433, Throughput:37772.83 kt/s\n",
      "Loss:0.169, LR:0.00091486, Throughput:38301.14 kt/s\n",
      "Loss:0.166, LR:0.00090489, Throughput:36844.42 kt/s\n",
      "Loss:0.161, LR:0.00089443, Throughput:38196.7 kt/s\n",
      "Loss:0.157, LR:0.00088349, Throughput:36802.31 kt/s\n",
      "Loss:0.151, LR:0.00087209, Throughput:37498.06 kt/s\n",
      "Loss:0.148, LR:0.00086023, Throughput:37048.76 kt/s\n",
      "Loss:0.145, LR:0.00084794, Throughput:37335.16 kt/s\n",
      "Loss:0.142, LR:0.00083523, Throughput:37187.53 kt/s\n",
      "Loss:0.139, LR:0.00082211, Throughput:37862.05 kt/s\n",
      "Loss:0.137, LR:0.0008086, Throughput:37386.98 kt/s\n",
      "Loss:0.135, LR:0.00079472, Throughput:38099.24 kt/s\n",
      "Loss:0.132, LR:0.00078048, Throughput:36673.26 kt/s\n",
      "Loss:0.130, LR:0.0007659, Throughput:37553.16 kt/s\n",
      "Loss:0.128, LR:0.000751, Throughput:37345.62 kt/s\n",
      "Loss:0.127, LR:0.0007358, Throughput:37704.61 kt/s\n",
      "Loss:0.125, LR:0.00072031, Throughput:37390.9 kt/s\n",
      "Loss:0.123, LR:0.00070455, Throughput:37896.17 kt/s\n",
      "Loss:0.122, LR:0.00068855, Throughput:37160.85 kt/s\n",
      "Loss:0.120, LR:0.00067233, Throughput:37391.95 kt/s\n",
      "Loss:0.119, LR:0.00065589, Throughput:37242.43 kt/s\n",
      "Loss:0.118, LR:0.00063927, Throughput:37209.49 kt/s\n",
      "Loss:0.117, LR:0.00062248, Throughput:37151.51 kt/s\n",
      "Loss:0.115, LR:0.00060554, Throughput:37236.63 kt/s\n",
      "Loss:0.114, LR:0.00058848, Throughput:36927.12 kt/s\n",
      "Loss:0.113, LR:0.00057131, Throughput:38068.83 kt/s\n",
      "Loss:0.112, LR:0.00055406, Throughput:36770.14 kt/s\n",
      "Loss:0.111, LR:0.00053674, Throughput:38082.45 kt/s\n",
      "Loss:0.111, LR:0.00051938, Throughput:37446.01 kt/s\n",
      "Loss:0.110, LR:0.000502, Throughput:37896.5 kt/s\n",
      "Loss:0.109, LR:0.00048462, Throughput:37361.13 kt/s\n",
      "Loss:0.108, LR:0.00046726, Throughput:37907.03 kt/s\n",
      "Loss:0.108, LR:0.00044994, Throughput:37054.66 kt/s\n",
      "Loss:0.107, LR:0.00043269, Throughput:37705.26 kt/s\n",
      "Loss:0.106, LR:0.00041552, Throughput:37290.41 kt/s\n",
      "Loss:0.106, LR:0.00039846, Throughput:37116.83 kt/s\n",
      "Loss:0.105, LR:0.00038152, Throughput:36515.9 kt/s\n",
      "Loss:0.105, LR:0.00036473, Throughput:37751.91 kt/s\n",
      "Loss:0.104, LR:0.00034811, Throughput:37088.28 kt/s\n",
      "Loss:0.104, LR:0.00033167, Throughput:37643.87 kt/s\n",
      "Loss:0.103, LR:0.00031545, Throughput:36814.73 kt/s\n",
      "Loss:0.103, LR:0.00029945, Throughput:37136.42 kt/s\n",
      "Loss:0.102, LR:0.00028369, Throughput:37749.79 kt/s\n",
      "Loss:0.102, LR:0.0002682, Throughput:37738.52 kt/s\n",
      "Loss:0.102, LR:0.000253, Throughput:36849.7 kt/s\n",
      "Loss:0.101, LR:0.0002381, Throughput:37541.69 kt/s\n",
      "Loss:0.101, LR:0.00022352, Throughput:36734.85 kt/s\n",
      "Loss:0.101, LR:0.00020928, Throughput:37740.16 kt/s\n",
      "Loss:0.101, LR:0.0001954, Throughput:36736.4 kt/s\n",
      "Loss:0.100, LR:0.00018189, Throughput:37791.55 kt/s\n",
      "Loss:0.100, LR:0.00016877, Throughput:37465.7 kt/s\n",
      "Loss:0.100, LR:0.00015606, Throughput:37747.67 kt/s\n",
      "Loss:0.100, LR:0.00014377, Throughput:37232.74 kt/s\n",
      "Loss:0.100, LR:0.00013191, Throughput:37285.71 kt/s\n",
      "Loss:0.099, LR:0.00012051, Throughput:37130.41 kt/s\n",
      "Loss:0.099, LR:0.00010957, Throughput:38008.97 kt/s\n",
      "Loss:0.099, LR:9.911e-05, Throughput:36496.43 kt/s\n",
      "Loss:0.099, LR:8.9139e-05, Throughput:37513.94 kt/s\n",
      "Loss:0.099, LR:7.9672e-05, Throughput:37510.95 kt/s\n",
      "Loss:0.099, LR:7.0719e-05, Throughput:37334.92 kt/s\n",
      "Loss:0.099, LR:6.2292e-05, Throughput:37147.56 kt/s\n",
      "Loss:0.099, LR:5.4401e-05, Throughput:38023.22 kt/s\n",
      "Loss:0.099, LR:4.7054e-05, Throughput:36950.96 kt/s\n",
      "Loss:0.098, LR:4.0262e-05, Throughput:37439.67 kt/s\n",
      "Loss:0.098, LR:3.4033e-05, Throughput:37401.32 kt/s\n",
      "Loss:0.098, LR:2.8374e-05, Throughput:37680.03 kt/s\n",
      "Loss:0.098, LR:2.3292e-05, Throughput:36935.32 kt/s\n",
      "Loss:0.098, LR:1.8793e-05, Throughput:37951.13 kt/s\n",
      "Loss:0.098, LR:1.4882e-05, Throughput:37270.11 kt/s\n",
      "Loss:0.098, LR:1.1566e-05, Throughput:38121.21 kt/s\n",
      "Loss:0.098, LR:8.8465e-06, Throughput:36921.57 kt/s\n",
      "Loss:0.098, LR:6.7281e-06, Throughput:37404.45 kt/s\n",
      "Loss:0.098, LR:5.2131e-06, Throughput:36591.38 kt/s\n",
      "Loss:0.098, LR:4.3034e-06, Throughput:37377.21 kt/s\n",
      "Loss:0.098, LR:4e-06, Throughput:37291.77 kt/s\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, num_epochs, batch_size, max_lr, min_lr, warmup_ratio,\n",
    "                token_ids, position_ids, attn_masks, loss_masks, show_progress=True):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr,\n",
    "                                                    total_steps=(len(token_ids) // batch_size + 1) * num_epochs,\n",
    "                                                    final_div_factor=max_lr / min_lr, pct_start=warmup_ratio)\n",
    "\n",
    "    model.train()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        batches = tqdm.tqdm(list(range(0, len(token_ids), batch_size)), desc=f'Epoch {epoch_num}', disable=not show_progress)\n",
    "        for batch_i in batches:\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            inputs = token_ids[batch_i:batch_i + batch_size, :-1].to(model.device)\n",
    "            labels = token_ids[batch_i:batch_i + batch_size, 1:].to(model.device)\n",
    "\n",
    "            positions = position_ids[batch_i:batch_i + batch_size, :-1].to(model.device) if position_ids is not None else None\n",
    "            attn_mask = attn_masks[batch_i:batch_i + batch_size, :-1].to(model.device) if attn_masks is not None else None\n",
    "            loss_mask = loss_masks[batch_i:batch_i + batch_size, 1:] if loss_masks is not None else None\n",
    "\n",
    "            logits, kv_state = model.forward(inputs, position_ids=positions, attn_mask=attn_mask)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=2).view(-1, logits.shape[-1])\n",
    "\n",
    "            loss = (-torch.log(probs[torch.arange(probs.shape[0]), labels.reshape(-1)]))\n",
    "            if loss_mask is not None:\n",
    "                loss = (loss * loss_mask.reshape(-1)).mean()\n",
    "            else:\n",
    "                loss = loss.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step_time_cost = time.time() - step_start_time\n",
    "            throughput = round((probs.shape[0] * probs.shape[1]) / step_time_cost / 1000, 2)\n",
    "\n",
    "            step_stat = {'Loss': f'{loss.item():.3f}',\n",
    "                         'LR': f'{scheduler.get_last_lr()[0]:.5}',\n",
    "                         'Throughput': f'{throughput} kt/s'}\n",
    "\n",
    "            if show_progress:\n",
    "                batches.set_postfix(step_stat)\n",
    "            else:\n",
    "                print(', '.join(f'{s[0]}:{s[1]}' for s in step_stat.items()))\n",
    "\n",
    "            scheduler.step()\n",
    "        batches.close()\n",
    "\n",
    "\n",
    "train_model(debug_model, num_epochs=100, batch_size=128, max_lr=1e-3, min_lr=1e-4,\n",
    "            warmup_ratio=0.1,\n",
    "            token_ids=debug_seq, position_ids=None, attn_masks=None, loss_masks=None,\n",
    "            show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "2e800403549c98d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T21:07:06.182812Z",
     "start_time": "2023-10-11T21:07:06.176088Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, temperature, top_p, rep_penalty,\n",
    "             max_new_tokens=20, total_tokens=None,\n",
    "             end_tokens=None,\n",
    "             enable_kv_cache=True):\n",
    "    model.eval()\n",
    "\n",
    "    feed_tokens = tokenizer.encode(prompt)\n",
    "    all_tokens = feed_tokens.copy()\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(feed_tokens))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            position_ids = None if kv_cache is None else torch.tensor([[len(all_tokens) - 1]]).to(model.device)\n",
    "            logits, kv_cache = model.forward(\n",
    "                torch.tensor([feed_tokens if enable_kv_cache else all_tokens]).to(model.device),\n",
    "                position_ids=position_ids,\n",
    "                kv_cache=kv_cache)\n",
    "            logits = logits[0][-1].cpu()\n",
    "            if not enable_kv_cache:\n",
    "                kv_cache = None\n",
    "\n",
    "            # apply repetition penalty\n",
    "            logits_rep = torch.gather(logits, 0, torch.tensor(all_tokens))\n",
    "            logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "            logits.scatter_(0, torch.tensor(all_tokens), logits_rep)\n",
    "\n",
    "            # apply temperature\n",
    "            logits /= max(temperature, 1e-6)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # apply top-p\n",
    "            ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "            cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "            top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "            ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "            sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "            all_tokens.append(sampled_index)\n",
    "            feed_tokens = [sampled_index]\n",
    "\n",
    "            if end_tokens is not None and sampled_index in end_tokens:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
      "Tom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(debug_seq[1].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:07:13.359804Z",
     "start_time": "2023-10-11T21:07:13.343895Z"
    }
   },
   "id": "f94c17f13926df4a"
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "1bc924753b5acfc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T21:07:21.532685Z",
     "start_time": "2023-10-11T21:07:20.495633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a warm big a near big boy place, the\"\n",
      "\"\n",
      "They the crash. They big crash. It near a a the the big big scared a nice. into my a little little a big boy big a boy of little a boy a the little for the little little the the boy boy big little the the the little boy boy for\n",
      "1.034 sec(s)\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "print(generate(debug_model, tokenizer, 'a warm',\n",
    "               temperature=1.0, top_p=0.001, rep_penalty=1.0,\n",
    "               total_tokens=128,\n",
    "               end_tokens=tokenizer.encode('</s>'),\n",
    "               enable_kv_cache=False))\n",
    "print(f'{time.time() - a:.3f} sec(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def isTorchSubClass(obj):\n",
    "    for parent in obj.__class__.__mro__:\n",
    "        if parent.__module__.startswith(\"torch\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def findTensors(obj, objPath, results, depth):\n",
    "    if depth > 5 or obj == results:\n",
    "        return\n",
    "\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        for i, o in enumerate(obj):\n",
    "            findTensors(o, f\"{objPath}[{i}]\", results, depth + 1)\n",
    "    elif isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            findTensors(v, f\"{objPath}[{k}]\", results, depth + 1)\n",
    "\n",
    "    if type(obj) is torch.Tensor:\n",
    "        results.setdefault(objPath, obj)\n",
    "    elif isTorchSubClass(obj):\n",
    "        for attrName in dir(obj):\n",
    "            try:\n",
    "                findTensors(\n",
    "                    getattr(obj, attrName), f\"{objPath}.{attrName}\", results, depth + 1\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "def outputTensorSummary(deepTraverse=False):\n",
    "    from gc import get_objects\n",
    "    from warnings import filterwarnings\n",
    "    from collections import Counter\n",
    "\n",
    "    unit, unitName = 1024, \"KB\"\n",
    "\n",
    "    filterwarnings(\"ignore\", message=\"torch.distributed.reduce_op is deprecated\")\n",
    "\n",
    "    isTensor = lambda obj: isinstance(obj, torch.Tensor) or (\n",
    "            hasattr(obj, \"data\") and isinstance(obj.data, torch.Tensor)\n",
    "    )\n",
    "\n",
    "    if deepTraverse:\n",
    "        globalTensors = {}\n",
    "        findTensors(globals().copy(), \"Global\", globalTensors, 0)\n",
    "        globalTensors = {id(v): k for k, v in globalTensors.items()}\n",
    "    else:\n",
    "        globalTensors = {id(v): k for k, v in globals().items() if isTensor(v)}\n",
    "\n",
    "    totalUsage = 0\n",
    "    trivialMemoryUsage = 0\n",
    "    bigTensors = []\n",
    "    for obj in get_objects():\n",
    "        try:\n",
    "            if isTensor(obj):\n",
    "                if obj.device.index == None:\n",
    "                    continue\n",
    "                tensorMemSize = obj.nelement() * obj.element_size()\n",
    "                totalUsage += tensorMemSize\n",
    "                if (tensorMemSize / unit) < 1:\n",
    "                    trivialMemoryUsage += tensorMemSize\n",
    "                    continue\n",
    "                if id(obj) in globalTensors:\n",
    "                    bigTensors.append(\n",
    "                        (obj.shape, tensorMemSize / unit, globalTensors[id(obj)])\n",
    "                    )\n",
    "                else:\n",
    "                    bigTensors.append((obj.shape, tensorMemSize / unit))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f\"Total {totalUsage / unit:.2f} {unitName} CUDA memory in use.\\n\")\n",
    "\n",
    "    bigTensors.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    maxLowerUnit, minLowerUnit = 1000, 100\n",
    "    while minLowerUnit >= 1:\n",
    "        inRangeTensors = [t for t in bigTensors if minLowerUnit <= t[1] <= maxLowerUnit]\n",
    "        groupCounter = Counter(inRangeTensors)\n",
    "        print(f\"Tensors of size {minLowerUnit:>5} - {maxLowerUnit:>5} {unitName}:\")\n",
    "        for tensor, count in groupCounter.items():\n",
    "            print(\n",
    "                f\"  {count:4} * Size: {tensor[1]:.2f} {unitName} Shape: {[*tensor[0]]}\",\n",
    "                end=\"\",\n",
    "            )\n",
    "            print(f' {tensor[2]:.30}' if len(tensor) == 3 else \"\")\n",
    "\n",
    "        print(f\"Total: {sum([t[1] for t in inRangeTensors]):.2f} {unitName}\\n\")\n",
    "        maxLowerUnit, minLowerUnit = maxLowerUnit // 10, minLowerUnit // 10\n",
    "\n",
    "    print(\n",
    "        f\"Total {trivialMemoryUsage / unit :.2f} {unitName} is occupied by trivial tensors(<=1{unitName}).\"\n",
    "    )\n",
    "\n",
    "\n",
    "outputTensorSummary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28cf9c6e7de51317"
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2], [-1, 0, 1], [-2, -1, 0]]\n",
      "torch.Size([3, 3, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[[ 1.3001, -1.0135,  0.7867],\n          [-3.8832, -2.5460,  0.6367],\n          [-7.7895,  0.7715, -3.4392]],\n\n         [[ 1.3001, -1.0135,  0.7867],\n          [-3.8832, -2.5460,  0.6367],\n          [-7.7895,  0.7715, -3.4392]],\n\n         [[ 1.3001, -1.0135,  0.7867],\n          [-3.8832, -2.5460,  0.6367],\n          [-7.7895,  0.7715, -3.4392]]],\n\n\n        [[[ 1.6794,  2.7529,  2.8756],\n          [ 1.4859,  2.1267, -0.1118],\n          [-0.3588,  0.4464,  1.3739]],\n\n         [[ 1.6794,  2.7529,  2.8756],\n          [ 1.4859,  2.1267, -0.1118],\n          [-0.3588,  0.4464,  1.3739]],\n\n         [[ 1.6794,  2.7529,  2.8756],\n          [ 1.4859,  2.1267, -0.1118],\n          [-0.3588,  0.4464,  1.3739]]]])"
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_bsz = 2\n",
    "d_seq = 3\n",
    "d_emb = 7\n",
    "\n",
    "torch.manual_seed(0)\n",
    "rel_emb = nn.Embedding(d_seq * 2 - 1, d_emb)\n",
    "\n",
    "rel_pos = torch.tensor([[s + (d_seq - 1) for s in range(-i, d_seq - i)] for i in range(d_seq)])\n",
    "print([[s for s in range(-i, d_seq - i)] for i in range(d_seq)])\n",
    "r = rel_emb(rel_pos)\n",
    "print(r.shape)\n",
    "\n",
    "q = torch.randn((d_bsz, d_seq, d_emb))\n",
    "k = torch.randn((d_bsz, d_seq, d_emb))\n",
    "v = torch.randn((d_bsz, d_seq, d_emb))\n",
    "#q @ k.permute(0, 2, 1)\n",
    "\n",
    "rel_q = q.unsqueeze(1).repeat(1, d_seq, 1, 1)\n",
    "rel_k = k.unsqueeze(1).repeat(1, d_seq, 1, 1).permute(0, 1, 3, 2)\n",
    "attn_score = rel_q @ rel_k\n",
    "attn_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T20:55:01.107291Z",
     "start_time": "2023-10-11T20:55:01.096342Z"
    }
   },
   "id": "3f76f81f024ab03d"
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.3001, -1.0135,  0.7867],\n         [-3.8832, -2.5460,  0.6367],\n         [-7.7895,  0.7715, -3.4392]],\n\n        [[ 1.6794,  2.7529,  2.8756],\n         [ 1.4859,  2.1267, -0.1118],\n         [-0.3588,  0.4464,  1.3739]]])"
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q @ k.permute(0, 2, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T20:55:04.171041Z",
     "start_time": "2023-10-11T20:55:04.158205Z"
    }
   },
   "id": "fa9f6329e0142e3f"
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 3])"
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aq = q.unsqueeze(1).repeat(1, 1, 1, d_seq)\n",
    "ak = k.unsqueeze(1).repeat(1, d_seq, 1, 1)\n",
    "at = torch.sum(aq.view(d_bsz, d_seq * d_seq, d_emb) * ak.view(d_bsz, d_seq * d_seq, d_emb), dim=2).view(d_bsz, d_seq, d_seq)\n",
    "at.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T20:55:26.486707Z",
     "start_time": "2023-10-11T20:55:26.483183Z"
    }
   },
   "id": "d6596a34a85fa47f"
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.1168, -0.2473, -1.3527, -1.6959,  0.5667,  0.2484,  0.4397,  0.1124,\n          0.6408,  0.4412, -0.2159, -0.7425,  0.5627,  0.0525,  0.5229,  2.3022,\n         -1.4689, -1.5867,  1.2032,  0.0845, -1.2001],\n        [-2.1152,  0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168,\n         -0.2473, -1.3527, -1.6959,  0.5667,  0.2484,  0.4397,  0.1124,  0.6408,\n          0.4412, -0.2159, -0.7425,  0.5627,  0.0525],\n        [-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n          0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473,\n         -1.3527, -1.6959,  0.5667,  0.2484,  0.4397]],\n       grad_fn=<ViewBackward0>)"
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.view(-1, r.shape[-2] * r.shape[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:00:11.435334Z",
     "start_time": "2023-10-11T21:00:11.418095Z"
    }
   },
   "id": "ac13a8d785b55b3e"
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.1168, -0.2473, -1.3527, -1.6959,  0.5667,  0.2484,  0.4397],\n         [ 0.1124,  0.6408,  0.4412, -0.2159, -0.7425,  0.5627,  0.0525],\n         [ 0.5229,  2.3022, -1.4689, -1.5867,  1.2032,  0.0845, -1.2001]],\n\n        [[-2.1152,  0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377],\n         [ 1.1168, -0.2473, -1.3527, -1.6959,  0.5667,  0.2484,  0.4397],\n         [ 0.1124,  0.6408,  0.4412, -0.2159, -0.7425,  0.5627,  0.0525]],\n\n        [[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160],\n         [-2.1152,  0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377],\n         [ 1.1168, -0.2473, -1.3527, -1.6959,  0.5667,  0.2484,  0.4397]]],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:00:14.998225Z",
     "start_time": "2023-10-11T21:00:14.982529Z"
    }
   },
   "id": "34639b9146fef626"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "20f10056277b8f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

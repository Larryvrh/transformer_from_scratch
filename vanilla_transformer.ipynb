{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:23.426147415Z",
     "start_time": "2023-11-02T17:55:21.402596187Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import TRIETokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "from typing import *\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from flash_attn import flash_attn_func\n",
    "from flash_attn_triton import flash_attn_func as flash_attn_func_triton\n",
    "from dataloader import DatasetReader, DatasetIter, SingleDatasetReader, MultiDatasetsReader\n",
    "from math import ceil\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import bitsandbytes as bnb\n",
    "from threading import Lock, Thread\n",
    "import traceback\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from enum import Enum\n",
    "from modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Disabled due to no obvious speed up\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:23.428693268Z",
     "start_time": "2023-11-02T17:55:23.427055942Z"
    }
   },
   "id": "d09b9a4246f3aae2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33702a8e6613f742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:24.305106972Z",
     "start_time": "2023-11-02T17:55:23.914531314Z"
    }
   },
   "outputs": [],
   "source": [
    "g_tokenizer = TRIETokenizer('llama_vocab_pruned_32k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb151752ec125fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:24.338720649Z",
     "start_time": "2023-11-02T17:55:24.296866771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network definition for 370M network\n",
    "# C_SEQ_LEN = 1024\n",
    "# C_HIDDEN_SIZE = 1024\n",
    "# C_NUM_HEADS = 16\n",
    "# C_NUM_LAYERS = 24\n",
    "\n",
    "# Network definition for 135M network\n",
    "C_SEQ_LEN = 1024\n",
    "C_HIDDEN_SIZE = 768\n",
    "C_NUM_HEADS = 12\n",
    "C_NUM_LAYERS = 12\n",
    "\n",
    "C_DEVICE = torch.device('cuda')\n",
    "C_DTYPE = torch.float32\n",
    "\n",
    "C_DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799b78921dbff1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:24.790025178Z",
     "start_time": "2023-11-02T17:55:24.787797142Z"
    }
   },
   "outputs": [],
   "source": [
    "if not C_DEBUG:\n",
    "    # g_train_data = MultiDatasetsReader([\n",
    "    #     SingleDatasetReader('datasets/minipile_train_masked_1024.bin'),\n",
    "    #     SingleDatasetReader('datasets/enwiki_train_masked_1024.bin'),\n",
    "    #     SingleDatasetReader('datasets/tinytextbooks_train_masked_1024.bin'),\n",
    "    # ], seed=0)\n",
    "\n",
    "    # g_train_data = MultiDatasetsReader([\n",
    "    #     SingleDatasetReader('datasets/tinystories_train_masked.bin'),\n",
    "    # ], seed=0)\n",
    "\n",
    "    # g_train_data = MultiDatasetsReader([\n",
    "    #     SingleDatasetReader('datasets/slimpajamas/slimpajama_train_masked_1024.bin'),\n",
    "    # ], seed=0)\n",
    "\n",
    "    g_train_data = MultiDatasetsReader([\n",
    "        SingleDatasetReader('datasets/sft/alpaca_gpt4.bin'),\n",
    "        SingleDatasetReader('datasets/sft/wizardlm_evol_2.bin'),\n",
    "        SingleDatasetReader('datasets/sft/airoboros_2.2.1.bin')\n",
    "    ], seed=0)\n",
    "else:\n",
    "    g_train_data = MultiDatasetsReader([\n",
    "        SingleDatasetReader('datasets/debug_data_masked.bin'),\n",
    "        SingleDatasetReader('datasets/debug_data_masked.bin'),\n",
    "    ], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d770a2be845a70d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:25.286660054Z",
     "start_time": "2023-11-02T17:55:25.284708186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 53058\n",
      "Sample length: 1024\n",
      "Train tokens: 54331392\n"
     ]
    }
   ],
   "source": [
    "print('Train samples:', len(g_train_data))\n",
    "print('Sample length:', len(next(iter(g_train_data))['token_ids']))\n",
    "print('Train tokens:', len(g_train_data) * len(next(iter(g_train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8dbb96cab10318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:25.731868638Z",
     "start_time": "2023-11-02T17:55:25.724926132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: <s>A chat between User and Assistant.\n",
      "User:As an online platform teacher named Aimee, you possess impeccable credentials which include a Bachelor of Science degree in Industrial and Labor Relations from Cornell University, expertise in the English language, and intermediate proficiency in both Chinese and Spanish. Additionally, your professional experience as a STEAM teacher at UN Women in Singapore has honed your skills in teaching children from the ages of 6-11 and working with students from all levels of education. Your exceptional teaching abilities in spoken English and pronunciation paired with your personal strengths of being informed, patient, and engaging make you an ideal teacher for students seeking to improve their English language skills. Can you provide a short, concise, and unique English self-introduction in bullet point form that would attract students to enroll in your course?\n",
      "Assistant:Sure, here are some bullet points for your English self-introduction:\n",
      "\n",
      "- Hi there! My name is Aimee and I'm thrilled to be your online English teacher!\n",
      "- With a Bachelor's degree from Cornell University and experience teaching STEAM at UN Women in Singapore, I know how to help students learn effectively.\n",
      "- I'm passionate about language and have intermediate proficiency in both Chinese and Spanish, so I understand the challenges of learning a new language.\n",
      "- In my classes, I focus on improving spoken English and pronunciation, as well as building vocabulary and grammar skills.\n",
      "- But learning English doesn't have to be boring! I'm an engaging and patient teacher who loves to have fun and make learning enjoyable.\n",
      "- So, whether you're a beginner or an advanced student, I'm confident that I can help you improve your English skills and achieve your goals. Let's get started!</s>\n",
      "<s>A chat between User and Assistant.\n",
      "User:Can you create a revised version of the sentence that focuses on the importance of cultural fit in candidate evaluation?\r\n",
      "\r\n",
      "Certainly, I comprehend the obligations of this position and am prepared to proficiently analyze and scrutinize candidates' technical proficiency and communication abilities, provide constructive feedback on their responses, and offer insightful recommendations on their overall suitability for the role.\n",
      "Assistant:While I understand the responsibilities of this role and can effectively assess candidates based on their technical skills and communication abilities, I also recognize the importance of evaluating cultural fit and can provide valuable feedback and recommendations in that regard.</s>\n",
      "<s>A chat between User and Assistant.\n",
      "User:How can we develop a Java program that analyzes the impact of cultural differences on communication patterns between couples? Specifically, how can we incorporate data on cultural norms and values into our analysis of communication strategies? Additionally, how can we use Java to generate an outline for an essay on the importance of effective communication in relationships, taking into account the role of cultural diversity?\n",
      "Assistant:To develop a Java program that analyzes the impact of cultural differences on communication patterns between couples, you can start by collecting data on cultural norms and values that may affect communication strategies. This data can be collected through surveys, interviews, or research studies.\n",
      "Once you have collected the data, you can use Java to analyze the data and identify patterns in communication strategies that may be influenced by cultural differences. For example, you can use statistical analysis techniques to identify correlations between cultural values and communication patterns.\n",
      "To incorporate data on cultural norms and values into your analysis, you can use Java to create a database of cultural values and norms. This database can include information on cultural values related to communication, such as the importance of directness, politeness, and nonverbal communication.\n",
      "To generate an outline for an essay on the importance of effective communication in relationships, taking into account the role of cultural diversity, you can use Java to create a template that includes key points related to effective communication and cultural diversity. This template can include sections on the importance of understanding cultural differences in communication, strategies for effective communication in diverse relationships, and the impact of cultural diversity on communication patterns. You can also use Java to automate the process of generating an outline by using natural language processing techniques to identify key concepts and themes in the essay prompt.</s>\n",
      "<s>A chat between User and Assistant.\n",
      "User:Imagine a scenario where you are required to anonymize a statement without using proper nouns or third-person pronouns. Your task is to transform the sentence \"The quick brown fox jumps over the lazy dog\" into an anonymous form using only three letters. Can you solve this puzzle?\n",
      "Assistant:Yes, I can. Here'\n"
     ]
    }
   ],
   "source": [
    "print('Sample 1:', g_tokenizer.decode(next(iter(g_train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d124589733f9ed1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:27.468492455Z",
     "start_time": "2023-11-02T17:55:26.235744594Z"
    }
   },
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    g_model = ToyTransformer(g_tokenizer.get_vocab_size(), 2, 2, 256, 1024, C_DEVICE, C_DTYPE)\n",
    "else:\n",
    "    g_model = ToyTransformer(g_tokenizer.get_vocab_size(), C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN, C_DEVICE, C_DTYPE)\n",
    "    # g_model = ToyTransformer(g_tokenizer.get_vocab_size(), 4, 8, 512, C_SEQ_LEN, C_DEVICE, C_DTYPE) # 46M model for tiny stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 135418880\n",
      "ToyTransformer(\n",
      "  (sem_embed): Embedding(32768, 768)\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-11): 12 x DecoderLayer(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (attn_heads): ModuleList(\n",
      "          (0-11): 12 x AttentionHead(\n",
      "            (q_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (k_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (ln_mha): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_ffn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('Total parameters:', sum([t.numel() for t in g_model.parameters()]))\n",
    "print(g_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:27.515138275Z",
     "start_time": "2023-11-02T17:55:27.471927359Z"
    }
   },
   "id": "f906aed5"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory usage: 516.58MB\n"
     ]
    }
   ],
   "source": [
    "total_memory = 0\n",
    "for p in g_model.parameters():\n",
    "    total_memory += (p.numel() * p.element_size())\n",
    "print(f'Model memory usage: {total_memory / 1024 / 1024:.2f}MB')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:28.143923819Z",
     "start_time": "2023-11-02T17:55:28.138074615Z"
    }
   },
   "id": "73a1c1c4b301c9cb"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:28.901709257Z",
     "start_time": "2023-11-02T17:55:28.900573321Z"
    }
   },
   "id": "dca6004e9b65892c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def dataset_collate(dataset_iter: DatasetIter, batch_size: int,\n",
    "                    transform: Optional[Callable[[Dict[str, List[np.ndarray]]], Dict[str, torch.Tensor]]] = None,\n",
    "                    drop_last: bool = False):\n",
    "    cur_batch, cur_batch_size = {}, 0\n",
    "    for entry in dataset_iter:\n",
    "        for k, v in entry.items():\n",
    "            cur_batch.setdefault(k, [])\n",
    "            cur_batch[k].append(v)\n",
    "        cur_batch_size += 1\n",
    "        if cur_batch_size == batch_size:\n",
    "            yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)\n",
    "            cur_batch = {}\n",
    "            cur_batch_size = 0\n",
    "    if not drop_last and len(cur_batch) > 0:\n",
    "        yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:29.421925507Z",
     "start_time": "2023-11-02T17:55:29.420959049Z"
    }
   },
   "id": "674478c0ccb206af"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:29.971060006Z",
     "start_time": "2023-11-02T17:55:29.966893410Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainArguments:\n",
    "    num_epochs: int\n",
    "    batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "\n",
    "    optimizer: Type[torch.optim.Optimizer]\n",
    "    optimizer_args: Optional[Dict[str, Any]]\n",
    "    mixed_precision_dtype: torch.dtype\n",
    "\n",
    "    start_lr: float\n",
    "    max_lr: float\n",
    "    end_lr: float\n",
    "    warmup_ratio: float\n",
    "\n",
    "    gradient_clip_norm: Optional[float]\n",
    "    probs_epsilon: Optional[float]\n",
    "\n",
    "    train_data: DatasetReader\n",
    "    ignore_attn_mask: bool\n",
    "    ignore_loss_mask: bool\n",
    "\n",
    "    # eval_data: Optional[DatasetReader]\n",
    "    # eval_steps: int\n",
    "    # \n",
    "    # eval_generate_prompt: Optional[str]\n",
    "    # eval_generate_steps: int\n",
    "\n",
    "    save_steps: int\n",
    "    save_on_interrupt: bool\n",
    "\n",
    "\n",
    "# type cast for handling int16/uint16 columns\n",
    "def train_transform(batch: Dict[str, List[np.ndarray]]):\n",
    "    return {k: torch.tensor(np.stack(v, dtype=np.int32 if v[0].dtype in [np.int16, np.uint16] else v[0].dtype)) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, model: ToyTransformer,\n",
    "                    optimizer: torch.optim.Optimizer, lr_scheduler: torch.optim.lr_scheduler.LRScheduler, grad_scaler: Optional[torch.cuda.amp.GradScaler],\n",
    "                    train_args: TrainArguments,\n",
    "                    dataset: DatasetReader, dataset_iter: DatasetIter, train_logs: List, misc: Dict):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path + '/model.pt')\n",
    "    torch.save(optimizer.state_dict(), path + '/optimizer.pt')\n",
    "    torch.save(lr_scheduler.state_dict(), path + '/lr_scheduler.pt')\n",
    "    if grad_scaler is not None:\n",
    "        torch.save(lr_scheduler.state_dict(), path + '/grad_scaler.pt')\n",
    "    torch.save(torch.get_rng_state(), path + '/rng_state.pt')\n",
    "    torch.save(train_args, path + '/train_args.pt')\n",
    "    dataset.save_iterator(dataset_iter, path + '/dataset_iter.pt')\n",
    "    torch.save(train_logs, path + '/train_logs.pt')\n",
    "    torch.save(misc, path + '/misc.pt')\n",
    "    with open(path + '/config.txt', 'w') as file:\n",
    "        file.write(f'Dataset: {str(dataset)}')\n",
    "        file.write('\\n\\n')\n",
    "        file.write(f'Model Config: {str(model.config)}')\n",
    "        file.write('\\n\\n')\n",
    "        file.write(f'Training Arguments: {str(train_args)}')\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str, model: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer, lr_scheduler: torch.optim.lr_scheduler.LRScheduler, grad_scaler: Optional[torch.cuda.amp.GradScaler],\n",
    "                    train_args: TrainArguments,\n",
    "                    dataset: DatasetReader, dataset_iter: DatasetIter, train_logs: List, misc: Dict):\n",
    "    model.load_state_dict(torch.load(path + '/model.pt'))\n",
    "    optimizer.load_state_dict(torch.load(path + '/optimizer.pt'))\n",
    "    lr_scheduler.load_state_dict(torch.load(path + '/lr_scheduler.pt'))\n",
    "    if grad_scaler is not None:\n",
    "        grad_scaler.load_state_dict(torch.load(path + '/grad_scaler.pt'))\n",
    "    torch.set_rng_state(torch.load(path + '/rng_state.pt'))\n",
    "    # assert torch.load(path + '/train_args.pt') == train_args\n",
    "    dataset_iter.set_state(dataset.load_iterator(path + '/dataset_iter.pt').get_state())\n",
    "    train_logs.clear()\n",
    "    train_logs += torch.load(path + '/train_logs.pt')\n",
    "    misc.update(torch.load(path + '/misc.pt'))\n",
    "\n",
    "\n",
    "def train_model(model: ToyTransformer, train_args: TrainArguments,\n",
    "                resume_from: Optional[str] = None,\n",
    "                show_progress: bool = True,\n",
    "                output_dir: str = 'checkpoints', interrupt_lock: Optional[Lock] = None):\n",
    "    output_dir = output_dir.rstrip('/')\n",
    "\n",
    "    interrupted = False\n",
    "    train_logs = []\n",
    "    misc = {'epochs': 0, 'steps': 0, 'last_batch_idx': -1}\n",
    "\n",
    "    total_samples = len(train_args.train_data)\n",
    "    epoch_steps = ceil(total_samples / train_args.batch_size)\n",
    "    assert epoch_steps >= train_args.gradient_accumulation_steps, \\\n",
    "        f'per-epoch steps {epoch_steps} is less than gradient accumulation steps {train_args.gradient_accumulation_steps}'\n",
    "\n",
    "    schedule_steps = ceil(total_samples / train_args.batch_size / train_args.gradient_accumulation_steps)\n",
    "    total_steps = schedule_steps * train_args.num_epochs\n",
    "\n",
    "    optimizer = train_args.optimizer(model.parameters(), **(train_args.optimizer_args if train_args.optimizer_args is not None else {}))\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=train_args.max_lr, div_factor=train_args.max_lr / train_args.start_lr,\n",
    "                                                    total_steps=total_steps,\n",
    "                                                    final_div_factor=train_args.start_lr / train_args.end_lr, pct_start=train_args.warmup_ratio)\n",
    "\n",
    "    if train_args.mixed_precision_dtype == torch.float16:\n",
    "        grad_scaler = torch.cuda.amp.GradScaler()\n",
    "    else:\n",
    "        grad_scaler = None\n",
    "\n",
    "    dataset_iter = iter(train_args.train_data)\n",
    "    if resume_from is not None:\n",
    "        load_checkpoint(resume_from, model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "\n",
    "    bar = tqdm.tqdm(total=total_steps, smoothing=1.0, disable=not show_progress)\n",
    "    bar.update(misc['steps'])\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch_num in range(train_args.num_epochs):\n",
    "        if epoch_num < misc['epochs']:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, batch in enumerate(dataset_collate(dataset_iter, train_args.batch_size, train_transform), start=misc['last_batch_idx'] + 1):\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            tokens = batch['token_ids'].to(model.device)\n",
    "            inputs = tokens[:, :-1]\n",
    "            labels = tokens[:, 1:]\n",
    "\n",
    "            attn_mask = batch['attn_mask'][:, :-1].to(model.device) if 'attn_mask' in batch and not train_args.ignore_attn_mask else None\n",
    "            loss_mask = batch['loss_mask'][:, 1:].to(model.device) if 'loss_mask' in batch and not train_args.ignore_loss_mask else None\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=train_args.mixed_precision_dtype, enabled=train_args.mixed_precision_dtype is not None):\n",
    "                logits, kv_state = model.forward(inputs, attn_mask=attn_mask)\n",
    "\n",
    "                probs = torch.softmax(logits, dim=2).view(-1, logits.shape[-1])\n",
    "                if train_args.probs_epsilon is not None:\n",
    "                    probs += train_args.probs_epsilon\n",
    "\n",
    "                loss = (-torch.log(probs[torch.arange(probs.shape[0]), labels.reshape(-1)]))\n",
    "                if loss_mask is not None:\n",
    "                    loss = (loss * loss_mask.reshape(-1)).mean() / train_args.gradient_accumulation_steps\n",
    "                else:\n",
    "                    loss = loss.mean() / train_args.gradient_accumulation_steps\n",
    "\n",
    "            # brutally clear nan, give up the whole batch\n",
    "            if torch.isnan(loss):\n",
    "                print(f'encountered nan loss at epoch {epoch_num + 1}, batch {batch_idx}')\n",
    "            else:\n",
    "                if grad_scaler is not None:\n",
    "                    grad_scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % train_args.gradient_accumulation_steps == 0 or (batch_idx + 1) == epoch_steps:\n",
    "                if grad_scaler is not None:\n",
    "                    grad_scaler.unscale_(optimizer)\n",
    "\n",
    "                if train_args.gradient_clip_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), train_args.gradient_clip_norm)\n",
    "\n",
    "                if grad_scaler is not None:\n",
    "                    grad_scaler.step(optimizer)\n",
    "                    grad_scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                step_time_cost = time.time() - step_start_time\n",
    "                throughput = round(probs.shape[0] / step_time_cost / 1000, 2)\n",
    "\n",
    "                step_stat = {'Loss': f'{loss.item() * train_args.gradient_accumulation_steps:.3f}',\n",
    "                             'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                             'Throughput': f'{throughput} kt/s'}\n",
    "\n",
    "                if show_progress:\n",
    "                    bar.set_description(f'Epoch {epoch_num + 1}')\n",
    "                    bar.set_postfix(step_stat)\n",
    "                else:\n",
    "                    print(', '.join(f'{s[0]}:{s[1]}' for s in step_stat.items()))\n",
    "\n",
    "                scheduler.step()\n",
    "                bar.update(1)\n",
    "                train_logs.append((epoch_num, batch_idx, step_stat))\n",
    "\n",
    "                misc['steps'] += 1\n",
    "                misc['last_batch_idx'] = batch_idx\n",
    "                if train_args.save_steps > 0 and (misc['steps'] % train_args.save_steps) == 0:\n",
    "                    save_checkpoint(output_dir + f'/checkpoint-{misc[\"steps\"]}',\n",
    "                                    model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "                if interrupt_lock is not None and not interrupt_lock.locked():\n",
    "                    if train_args.save_on_interrupt:\n",
    "                        save_checkpoint(output_dir + f'/checkpoint-{misc[\"steps\"]}',\n",
    "                                        model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "                    interrupted = True\n",
    "                    break\n",
    "        if interrupted:\n",
    "            break\n",
    "        misc['epochs'] += 1\n",
    "        misc['last_batch_idx'] = -1\n",
    "        dataset_iter = iter(train_args.train_data)\n",
    "    bar.close()\n",
    "\n",
    "    if not interrupted:\n",
    "        save_checkpoint(output_dir + f'/checkpoint-done',\n",
    "                        model, optimizer, scheduler, grad_scaler, train_args, train_args.train_data, dataset_iter, train_logs, misc)\n",
    "\n",
    "    return train_logs\n",
    "\n",
    "\n",
    "def train_model_interruptable(model: nn.Module, train_args: TrainArguments,\n",
    "                              resume_from: Optional[str] = None,\n",
    "                              show_progress: bool = True,\n",
    "                              output_dir: str = 'checkpoints'):\n",
    "    return_value, run_finish = None, False\n",
    "\n",
    "    def return_value_wrapper(func, *args, **kwargs):\n",
    "        nonlocal return_value, run_finish\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            return_value = func(*args, **kwargs)\n",
    "        except Exception as _:\n",
    "            traceback.print_exc()\n",
    "        run_finish = True\n",
    "\n",
    "    interrupt_lock = Lock()\n",
    "    interrupt_lock.acquire()\n",
    "    thread = Thread(target=return_value_wrapper, args=(train_model, model, train_args),\n",
    "                    kwargs={'resume_from': resume_from, 'show_progress': show_progress, 'output_dir': output_dir, 'interrupt_lock': interrupt_lock})\n",
    "    thread.start()\n",
    "    while not run_finish:\n",
    "        try:\n",
    "            time.sleep(0.1)\n",
    "        except KeyboardInterrupt as _:\n",
    "            interrupt_lock.release()\n",
    "            break\n",
    "    thread.join()\n",
    "\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "281e39ed7414acc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T17:55:30.548868402Z",
     "start_time": "2023-11-02T17:55:30.543517202Z"
    }
   },
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    g_train_args = TrainArguments(\n",
    "        num_epochs=1000, batch_size=8, gradient_accumulation_steps=1,\n",
    "        optimizer=torch.optim.AdamW, optimizer_args=None,\n",
    "        mixed_precision_dtype=torch.bfloat16,\n",
    "        start_lr=1e-5, max_lr=1e-3, end_lr=1e-6, warmup_ratio=0.1,\n",
    "        gradient_clip_norm=1.0, probs_epsilon=None,\n",
    "        train_data=g_train_data, ignore_attn_mask=False, ignore_loss_mask=False,\n",
    "        # eval_data=None, eval_steps=-1,\n",
    "        # eval_generate_prompt=None, eval_generate_steps=-1,\n",
    "        save_steps=-1,\n",
    "        save_on_interrupt=False,\n",
    "    )\n",
    "else:\n",
    "    g_train_args = TrainArguments(\n",
    "        num_epochs=2, batch_size=12, gradient_accumulation_steps=8,\n",
    "        optimizer=torch.optim.AdamW, optimizer_args=None,\n",
    "        mixed_precision_dtype=torch.bfloat16,\n",
    "        start_lr=5e-5, max_lr=1e-3, end_lr=1e-4, warmup_ratio=0.1,\n",
    "        gradient_clip_norm=0.7, probs_epsilon=None,\n",
    "        train_data=g_train_data, ignore_attn_mask=True, ignore_loss_mask=True,\n",
    "        # eval_data=None, eval_steps=-1,\n",
    "        # eval_generate_prompt=None, eval_generate_steps=-1,\n",
    "        save_steps=1000,\n",
    "        save_on_interrupt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673871354eec2c12",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-02T17:55:36.257548155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3981 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2092b8e513ca418bbc081d50c289a068"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if C_DEBUG:\n",
    "    global_config['attn_backend'] = AttentionBackend.FlashAttentionTriton\n",
    "    g_train_args.ignore_attn_mask = True\n",
    "    g_train_logs = train_model_interruptable(g_model, g_train_args, resume_from=None,\n",
    "                                             show_progress=True, output_dir='checkpoints/debug_output')\n",
    "else:\n",
    "    g_model.load_state_dict(torch.load('checkpoints/train-round1-135M/checkpoint-19000/model.pt'))\n",
    "    global_config['attn_backend'] = AttentionBackend.FlashAttentionTriton\n",
    "    g_train_args.ignore_attn_mask = False\n",
    "    g_train_args.ignore_loss_mask = False\n",
    "    g_train_args.num_epochs = 3\n",
    "    g_train_args.save_steps = 500\n",
    "    g_train_args.warmup_ratio = 0.1\n",
    "    g_train_args.batch_size = 10\n",
    "    g_train_args.gradient_accumulation_steps = 4\n",
    "    g_train_args.start_lr = 1e-6\n",
    "    g_train_args.max_lr = 1e-4\n",
    "    g_train_args.end_lr = 1e-5\n",
    "    g_train_logs = train_model_interruptable(g_model, g_train_args, resume_from=None,\n",
    "                                             show_progress=True, output_dir='checkpoints/train-round1-135M-sft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ca07cd31cd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_logs(train_logs_list):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    for i, train_logs in enumerate(train_logs_list):\n",
    "        axes[0].plot([float(l[2]['Loss']) for l in train_logs])\n",
    "        axes[1].plot([float(l[2]['LR']) for l in train_logs])\n",
    "        axes[2].plot([float(l[2]['Throughput'][:-5]) for l in train_logs])\n",
    "\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[1].set_title('Learning Rate')\n",
    "    axes[2].set_title('Throughput (kt/s)')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.autoscale()\n",
    "\n",
    "    #plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_list = [\n",
    "    'checkpoints/train-round1/checkpoint-19000/train_logs.pt',\n",
    "    'checkpoints/train-round1-masked/checkpoint-1199/train_logs.pt'\n",
    "]\n",
    "logs_list = [torch.load(c) for c in checkpoint_list]\n",
    "\n",
    "plot_train_logs(logs_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "471f560981b69d48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e800403549c98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autocast(device_type='cuda', dtype=torch.bfloat16)\n",
    "def generate(model, tokenizer, prompt, temperature, top_p, rep_penalty,\n",
    "             max_new_tokens=20, total_tokens=None,\n",
    "             end_tokens=None,\n",
    "             enable_kv_cache=True):\n",
    "    model.eval()\n",
    "\n",
    "    feed_tokens = tokenizer.encode(prompt) if isinstance(prompt, str) else prompt\n",
    "    all_tokens = feed_tokens.copy()\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(feed_tokens))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, kv_cache = model.forward(\n",
    "                torch.tensor([feed_tokens if enable_kv_cache else all_tokens]).to(model.device),\n",
    "                kv_cache=kv_cache)\n",
    "            logits = logits[0][-1].cpu()\n",
    "            if not enable_kv_cache:\n",
    "                kv_cache = None\n",
    "\n",
    "            # apply repetition penalty\n",
    "            logits_rep = torch.gather(logits, 0, torch.tensor(all_tokens))\n",
    "            logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "            logits.scatter_(0, torch.tensor(all_tokens), logits_rep)\n",
    "\n",
    "            # apply temperature\n",
    "            logits /= max(temperature, 1e-6)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # apply top-p\n",
    "            ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "            cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "            top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "            ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "            sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "            all_tokens.append(sampled_index)\n",
    "            feed_tokens = [sampled_index]\n",
    "\n",
    "            if end_tokens is not None and sampled_index in end_tokens:\n",
    "                break\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def modeling_sanity_check(gen_length: int, enable_kv_cache: bool):\n",
    "    assert C_DEBUG == True, 'sanity check can only be performed under debug settings'\n",
    "    train_token_ids = next(iter(g_train_data))['token_ids'].tolist()\n",
    "    train_texts = [l.strip() for l in g_tokenizer.decode(train_token_ids).split('</s>')]\n",
    "    start_time = time.time()\n",
    "    gen_token_ids = generate(g_model, g_tokenizer, train_token_ids[:10],\n",
    "                             temperature=1.0, top_p=0.01, rep_penalty=1.0,\n",
    "                             total_tokens=gen_length,\n",
    "                             end_tokens=g_tokenizer.encode('<reserved_0>'),\n",
    "                             enable_kv_cache=enable_kv_cache)\n",
    "    cost_time = time.time() - start_time\n",
    "    print(f'Generation finished in {cost_time:.2f} sec(s), throughput: {len(gen_token_ids) / cost_time:.1f} tokens/sec')\n",
    "    # Complete check\n",
    "    cmp_length = min(len(train_token_ids), len(gen_token_ids))\n",
    "    print('Complete Identical:', train_token_ids[:cmp_length] == gen_token_ids[:cmp_length])\n",
    "    # Segment check\n",
    "    gen_texts = [l.strip() for l in g_tokenizer.decode(gen_token_ids).split('</s>')]\n",
    "    for i in range(min(len(train_texts), len(gen_texts))):\n",
    "        ref, real = train_texts[i], gen_texts[i]\n",
    "        cmp_length = min(len(ref), len(real))\n",
    "        print(f'Segment {i}: Ref Len: {len(ref)}, Gen Len: {len(real)} Identical: {ref[:cmp_length] == real[:cmp_length]}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93a3628aacbd1f80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for backend_option in [AttentionBackend.Naive, AttentionBackend.FlashAttentionTriton, AttentionBackend.FlashAttentionCuda]:\n",
    "    for enable_kv_cache_option in [True, False]:\n",
    "        print(f'Backend = {backend_option}, KVCache Enable = {enable_kv_cache_option}')\n",
    "        global_config['attn_backend'] = backend_option\n",
    "        modeling_sanity_check(512, enable_kv_cache_option)\n",
    "        print('=' * 80)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ffe9294382f18c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c17f13926df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in g_tokenizer.decode(next(iter(g_train_data))['token_ids'].tolist()).split('</s>')[:3]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc924753b5acfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "global_config['attn_backend'] = AttentionBackend.FlashAttentionTriton\n",
    "result = generate(g_model, g_tokenizer, '<s>A chat between User and Assistant.\\nUser:What is python?\\nAssistant:',\n",
    "                  temperature=1.0, top_p=0.01, rep_penalty=1.1,\n",
    "                  total_tokens=128,\n",
    "                  end_tokens=g_tokenizer.encode('</s>'),\n",
    "                  enable_kv_cache=True)\n",
    "time_cost = time.time() - time_start\n",
    "\n",
    "print(g_tokenizer.decode(result))\n",
    "print(f'{time_cost:.3f} sec(s), throughput {len(result) / time_cost:.1f} tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch 2160"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "625f35292e9bde9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_model.load_state_dict(torch.load('debug_model.pt'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d598db548f110473"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t_iter = dataset_collate(iter(g_train_data), 10, train_transform)\n",
    "for i in range(2161):\n",
    "    next(t_iter)\n",
    "m = next(t_iter)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "373c3f698ba3f13d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "global_config['attn_backend'] = AttentionBackend.Naive"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edd3c44a0aa327d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        x, _ = g_model.forward(m['token_ids'][:, :-1].cuda(), m['attn_mask'][:, :-1].cuda())\n",
    "        p = torch.softmax(x, dim=2).view(-1, x.shape[-1])\n",
    "        l = (-torch.log(p[torch.arange(p.shape[0]), m['token_ids'][:, 1:].cuda().reshape(-1)]))\n",
    "        #l *= m['loss_mask'][:, 1:].cuda().reshape(-1)\n",
    "        p = p.view(-1, 1023, g_tokenizer.get_vocab_size())\n",
    "        l = l.view(-1, 1023)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cf0dd030feff627"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.topk(l, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0a6578a4bf25b5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m['token_ids'][:, 1:][0, 71], p[0, 71].argmax()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "652ac173b8c19dc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(g_tokenizer.decode(m['token_ids'][6].tolist()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "637d44b8b1e35045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449bf1f542688d72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_tokenizer.decode([30474])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7323450e0dad605"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "80f5c7b6ebd5dbb9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

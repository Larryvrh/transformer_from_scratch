{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:32.919867148Z",
     "start_time": "2023-10-22T02:35:32.132940732Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import TRIETokenizerFast\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "from typing import *\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from flash_attn import flash_attn_func\n",
    "from dataloader import DatasetReader\n",
    "from math import ceil\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33702a8e6613f742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:41.455261699Z",
     "start_time": "2023-10-22T02:35:33.258703024Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = TRIETokenizerFast('llama_vocab_pruned_32k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Network definition\n",
    "C_SEQ_LEN = 2048\n",
    "C_HIDDEN_SIZE = 768\n",
    "C_NUM_HEADS = 12\n",
    "C_NUM_LAYERS = 12\n",
    "\n",
    "C_DEVICE = torch.device('cuda')\n",
    "C_DTYPE = torch.bfloat16\n",
    "\n",
    "C_DEBUG = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:42.547989872Z",
     "start_time": "2023-10-22T02:35:42.538090279Z"
    }
   },
   "id": "2fb151752ec125fa"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "if not C_DEBUG:\n",
    "    train_data = DatasetReader('datasets/minipile_validation.bin')\n",
    "else:\n",
    "    train_data = DatasetReader('datasets/debug_data.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:42.982117284Z",
     "start_time": "2023-10-22T02:35:42.978814825Z"
    }
   },
   "id": "799b78921dbff1ec"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 324\n"
     ]
    }
   ],
   "source": [
    "print('Train samples:', len(train_data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:43.508067283Z",
     "start_time": "2023-10-22T02:35:43.506595374Z"
    }
   },
   "id": "1d770a2be845a70d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: <s>Q:\n",
      "\n",
      "\"Enable Wifi\" not showing\n",
      "\n",
      "I recently used sudo apt-get autoremove command and after that \"Enable Wifi\" option is not showing.\n",
      "$ lspci -knn | grep Net -A3\n",
      "02:00.0 Network controller [0280]: Realtek Semiconductor Co., Ltd. RTL8821AE 802.11ac PCIe Wireless Network Adapter [10ec:8821]\n",
      "    Subsystem: Lenovo RTL8821AE 802.11ac PCIe Wireless Network Adapter [17aa:a814]\n",
      "    Kernel modules: rtl8821ae, wl\n",
      "\n",
      "$ iwconfig is\n",
      "lo no wireless extensions.\n",
      "\n",
      "enp1s0    no wireless extensions.\n",
      "\n",
      "From /var/apt/history.log:\n",
      "Start-Date: 2017-11-06  01:15:38\n",
      "Commandline: apt-get autoremove\n",
      "Requested-By: praveen (1000)\n",
      "Remove: linux-headers-4.10.0-28-generic:amd64 (4.10.0-28.32~16.04.2), librpmsign3:amd64 (4.12.0.1+dfsg1-3build3), linux-image-extra-4.10.0-33-generic:amd64 (4.10.0-33.37~16.04.1),\n",
      "librpmbuild3:amd64 (4.12.0.1+dfsg1-3build3),\n",
      "linux-image-extra-4.10.0-35-generic:amd64 (4.10.0-35.39~16.04.1),\n",
      "debugedit:amd64 (4.12.0.1+dfsg1-3build3),\n",
      "linux-headers-4.10.0-33-generic:amd64 (4.10.0-33.37~16.04.1),\n",
      "linux-image-4.10.0-28-generic:amd64 (4.10.0-28.32~16.04.2),\n",
      "linux-headers-4.10.0-35-generic:amd64 (4.10.0-35.39~16.04.1),\n",
      "rpm:amd64 (4.12.0.1+dfsg1-3build3),\n",
      "linux-image-4.10.0-33-generic:amd64 (4.10.0-33.37~16.04.1),\n",
      "linux-headers-4.10.0-28:amd64 (4.10.0-28.32~16.04.2),\n",
      "linux-headers-4.10.0-33:amd64 (4.10.0-33.37~16.04.1),\n",
      "linux-headers-4.10.0-35:amd64 (4.10.0-35.39~16.04.1),\n",
      "linux-image-4.10.0-35-generic:amd64 (4.10.0-35.39~16.04.1),\n",
      "linux-image-extra-4.10.0-28-generic:amd64 (4.10.0-28.32~16.04.2)\n",
      "End-Date: 2017-11-06  01:17:19\n",
      "\n",
      "Output of uname -a:\n",
      "Linux praveen-Lenovo-ideapad-300-15ISK 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n",
      "\n",
      "$ dpkg-query -Wf '${db:Status-Abbrev} ${Package;-40} ${Version}\\n' linux-{image,headers}-\\* | grep '^i'\n",
      "ii  linux-headers-4.10.0-37                  4.10.0-37.41~16.04.1\n",
      "ii  linux-headers-4.10.0-37-generic          4.10.0-37.41~16.04.1\n",
      "ii  linux-headers-4.10.0-38                  4.10.0-38.42~16.04.1\n",
      "ii  linux-headers-4.10.0-38-generic          4.10.0-38.42~16.04.1\n",
      "ii  linux-headers-4.4.0-98                   4.4.0-98.121\n",
      "ii  linux-headers-4.4.0-98-generic           4.4.0-98.121\n",
      "ii  linux-headers-generic                    4.4.0.98.103\n",
      "ii  linux-headers-generic-hwe-16.04          4.10.0.38.40\n",
      "ii  linux-image-4.10.0-37-generic            4.10.0-37.41~16.04.1\n",
      "ii  linux-image-4.10.0-38-generic            4.10.0-38.42~16.04.1\n",
      "ii  linux-image-extra-4.10.0-37-generic      4.10.0-37.41~16.04.1\n",
      "ii  linux-image-extra-4.10.0-38-generic      4.10.0-38.42~16.04.1\n",
      "ii  linux-image-generic-hwe-16.04            4.10.0.38.40\n",
      "\n",
      "A:\n",
      "\n",
      "These commands will work for\n",
      "rtl8192ce, rtl8192se, rtl8192de, rtl8188ee, rtl8192ee, rtl8723ae, rtl8723be, and rtl8821ae. If you have any other driver then please search and install.\n",
      "sudo apt-get install linux-headers-generic build-essential git \n",
      "git clone http://github.com/lwfinger/rtlwifi_new.git\n",
      "cd rtlwifi_new\n",
      "make\n",
      "sudo make install\n",
      "\n",
      "After that if your system does not load appropriate kernel module,you can run below command in same directory(make sure to run this for your specific driver in my case it is rtl8821ae):\n",
      "sudo modprobe rtl8821ae\n",
      "\n",
      "</s><s>For 8-bit music, this game has some that is both painful and awesome. For the purpose of this blog only the awesome music will be shared. Most of this is included with the songs that Link learns to play on his ocarina, a later staple of the series, which he learns from those about the island.\n",
      "\n",
      "There are three, of these songs. One is taught by a girl who sings in the main village and is the person who found Link laying on the beach. Another is taught by a singing fish, which is my favorite of the three. The last is taught by some frogs and allows to resurrect things with the very tune.\n",
      "\n",
      "I am going to post those songs, here, in order of least amusing first. That meaning they are not in order you obtain them in game and the one which is annoying, at least its \"sung\" counterpart.\n",
      "\n",
      "1 comment\n",
      "\n",
      "Disclaimer\n",
      "\n",
      "Any audio, video or images on this site have no direct claim to them, unless stated otherwise, by the writer of this blog. The author or link to source object is always available if you need proof as such if it was not provided in the blog post that it was spoken about.</s><s>Endothelial modulation of resting and stimulated vascular tone in the pig capsular testicular artery.\n",
      "We\n"
     ]
    }
   ],
   "source": [
    "print('Sample 1:', tokenizer.decode(next(iter(train_data))['token_ids']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:43.894943266Z",
     "start_time": "2023-10-22T02:35:43.893290777Z"
    }
   },
   "id": "5f8dbb96cab10318"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d4a7cb7e49d5f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:44.480305123Z",
     "start_time": "2023-10-22T02:35:44.467208290Z"
    }
   },
   "outputs": [],
   "source": [
    "global_config = {\n",
    "    'enable_torch_attn': False,\n",
    "    'enable_flash_attn': False,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int = -1,\n",
    "    num_layers: int = -1,\n",
    "    num_heads: int = -1,\n",
    "    hidden_size: int = -1,\n",
    "    max_seq_len: int = -1,\n",
    "    root_model: 'ToyTransformer' = None\n",
    "    device: torch.device = torch.device('cpu')\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    enable_rel_pos: bool = False\n",
    "\n",
    "\n",
    "def expand_attn_mask(custom_attn_mask: torch.Tensor):\n",
    "    B, T = custom_attn_mask.shape\n",
    "    mask = custom_attn_mask.unsqueeze(1).repeat((1, T, 1))\n",
    "    seq_index_mask = (mask == custom_attn_mask[:, torch.arange(T)].view(B, T, 1))\n",
    "    return seq_index_mask & (torch.tril(mask) > 0)\n",
    "\n",
    "\n",
    "# naive RoPE implementation following https://arxiv.org/pdf/2104.09864.pdf\n",
    "def get_rope_cache_slow(seq_len: int, dim: int, theta: int, device: torch.device, dtype: torch.dtype):\n",
    "    assert dim % 2 == 0\n",
    "    freqs = theta ** (-2 * torch.arange(0, dim // 2, 1.) / dim)\n",
    "    freqs = torch.repeat_interleave(freqs, 2)\n",
    "    v1 = torch.cos(torch.arange(seq_len, dtype=torch.float).view((seq_len, 1)) * freqs)\n",
    "    v2 = torch.sin(torch.arange(seq_len, dtype=torch.float).view((seq_len, 1)) * freqs)\n",
    "    v2 = v2 * torch.tensor([1, -1] * (dim // 2))\n",
    "    indices = torch.tensor([j for i in range(0, dim, 2) for j in (i + 1, i)])\n",
    "    return v1.to(device, dtype=dtype), v2.to(device, dtype=dtype), indices.to(device)\n",
    "\n",
    "\n",
    "def apply_rope_slow(x, rope_cache, positions: Optional[torch.Tensor] = None):\n",
    "    v1, v2, indices = rope_cache\n",
    "    seq_len, dim = x.shape[1:]\n",
    "    if positions is None:\n",
    "        v1 = v1[:seq_len, :]\n",
    "        v2 = v2[:seq_len, :]\n",
    "    else:\n",
    "        v1 = v1[positions, torch.arange(dim)].view((-1, dim))\n",
    "        v2 = v2[positions, torch.arange(dim)].view((-1, dim))\n",
    "    applied_x = x * v1 + (x * v2)[:, :, indices]\n",
    "    return applied_x\n",
    "\n",
    "\n",
    "# Optimized RoPE implementation adapted from https://github.com/facebookresearch/llama/blob/main/llama/model.py\n",
    "def get_rope_cache_fast(seq_len: int, dim: int, theta: int, device: torch.device, dtype: torch.dtype):\n",
    "    freqs = (1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)))\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "\n",
    "def apply_rope_fast(x, rope_cache, positions: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    if positions is None and x.shape[1] < rope_cache.shape[0]:\n",
    "        freqs_cis = rope_cache[:x.shape[1], :]\n",
    "    elif positions is not None:\n",
    "        freqs_cis = rope_cache[positions, :]\n",
    "    else:\n",
    "        freqs_cis = rope_cache\n",
    "    freqs_cis = freqs_cis.view([d if i == 1 or i == x_.ndim - 1 else 1 for i, d in enumerate(x_.shape)])\n",
    "\n",
    "    applied_x = torch.view_as_real(x_ * freqs_cis).flatten(2)\n",
    "    return applied_x.type_as(x)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.dtype = config.dtype\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        mask_zero = torch.tensor(0, dtype=self.dtype)\n",
    "        mask_val = torch.tensor(torch.finfo(self.dtype).min / 2, dtype=self.dtype)\n",
    "        if kv_cache is None and attn_mask is not None:\n",
    "            apply_mask = expand_attn_mask(attn_mask)\n",
    "        elif kv_cache is None and not global_config['enable_torch_attn'] and not global_config['enable_flash_attn']:\n",
    "            apply_mask = expand_attn_mask(torch.ones(x.shape[:2]))\n",
    "        elif kv_cache is not None:\n",
    "            apply_mask = torch.ones((B, T, T), dtype=torch.bool)\n",
    "        else:\n",
    "            apply_mask = None\n",
    "\n",
    "        if not global_config['enable_torch_attn'] and not global_config['enable_flash_attn']:\n",
    "            apply_mask = torch.where(apply_mask, mask_zero, mask_val)\n",
    "\n",
    "        use_flash_attn = global_config['enable_flash_attn'] and kv_cache is None and apply_mask is None\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        if self.config.enable_rel_pos:\n",
    "            positions = torch.tensor([kv_cache[0].shape[1]]).to(q.device) if kv_cache is not None else None\n",
    "            q = apply_rope_fast(q, self.config.root_model.rope_cache, positions)\n",
    "            k = apply_rope_fast(k, self.config.root_model.rope_cache, positions)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k = torch.concat([kv_cache[0], k], dim=1)\n",
    "            v = torch.concat([kv_cache[1], v], dim=1)\n",
    "\n",
    "        if use_flash_attn:\n",
    "            q, k, v, = q.unsqueeze(2), k.unsqueeze(2), v.unsqueeze(2)\n",
    "            attn_result = flash_attn_func(q, k, v, causal=True)\n",
    "            q, k, v, attn_result = q.squeeze(2), k.squeeze(2), v.squeeze(2), attn_result.squeeze(2)\n",
    "        elif global_config['enable_torch_attn']:\n",
    "            with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):\n",
    "                attn_result = nn.functional.scaled_dot_product_attention(q, k, v,\n",
    "                                                                         attn_mask=apply_mask.to(q.device) if apply_mask is not None else None,\n",
    "                                                                         is_causal=True if apply_mask is None else False)\n",
    "        else:\n",
    "            attn_score = (q @ k.permute(0, 2, 1) / (self.hidden_size ** 0.5)) + apply_mask.to(q.device)\n",
    "            attn_result = torch.softmax(attn_score, dim=2) @ v\n",
    "\n",
    "        return attn_result, [k, v]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(config) for _ in range(config.num_heads)])\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, dtype=config.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        head_outputs = [head(x, attn_mask, kv_cache[idx] if kv_cache is not None else None) for idx, head in\n",
    "                        enumerate(self.attn_heads)]\n",
    "        return self.o_proj(torch.concat([o[0] for o in head_outputs], dim=2)), [o[1] for o in head_outputs]\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.hidden_size * 4, dtype=config.dtype)\n",
    "        self.down_proj = nn.Linear(config.hidden_size * 4, config.hidden_size, dtype=config.dtype)\n",
    "        self.ln_mha = nn.LayerNorm(config.hidden_size, dtype=config.dtype)\n",
    "        self.ln_ffn = nn.LayerNorm(config.hidden_size, dtype=config.dtype)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        mha_output, new_kv_cache = self.mha(self.ln_mha(x), attn_mask, kv_cache)\n",
    "        mha_output = x + mha_output\n",
    "        ffn_output = self.down_proj(self.act(self.up_proj(self.ln_ffn(mha_output))))\n",
    "        return mha_output + ffn_output, new_kv_cache\n",
    "\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, hidden_size: int, max_seq_len: int,\n",
    "                 device: torch.device = torch.device('cpu'), dtype: torch.dtype = torch.float32,\n",
    "                 enable_rel_pos: bool = False):\n",
    "        super().__init__()\n",
    "        self.config = TransformerConfig(vocab_size, num_layers, num_heads, hidden_size, max_seq_len, self, device,\n",
    "                                        dtype, enable_rel_pos)\n",
    "\n",
    "        self.sem_embed = nn.Embedding(vocab_size, hidden_size, dtype=dtype)\n",
    "\n",
    "        if not self.config.enable_rel_pos:\n",
    "            self.pos_embed = nn.Embedding(max_seq_len, hidden_size, dtype=dtype)\n",
    "        else:\n",
    "            # self.rope_cache = get_rope_cache(max_seq_len, hidden_size // num_heads, 10000, device, dtype)\n",
    "            self.rope_cache = get_rope_cache_fast(max_seq_len, hidden_size // num_heads, 10000, device, dtype)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(num_layers)])\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, dtype=dtype)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, seq: torch.Tensor,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                kv_cache: Optional[List[torch.Tensor]] = None) -> Tuple[torch.Tensor, List[List[List[torch.Tensor]]]]:\n",
    "\n",
    "        if self.config.enable_rel_pos:\n",
    "            hidden = self.sem_embed(seq)\n",
    "        elif position_ids is not None:\n",
    "            hidden = self.sem_embed(seq) + self.pos_embed(position_ids)\n",
    "        else:\n",
    "            hidden = self.sem_embed(seq) + self.pos_embed(torch.arange(0, seq.shape[1], 1).to(self.device))\n",
    "\n",
    "        new_kv_cache = []\n",
    "        for idx, decoder in enumerate(self.decoder_layers):\n",
    "            hidden, layer_kv_cache = decoder(hidden, attn_mask, kv_cache[idx] if kv_cache is not None else None)\n",
    "            new_kv_cache.append(layer_kv_cache)\n",
    "\n",
    "        return self.lm_head(hidden), new_kv_cache\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d124589733f9ed1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:46.086404479Z",
     "start_time": "2023-10-22T02:35:45.027300526Z"
    }
   },
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    model = ToyTransformer(tokenizer.get_vocab_size(), 2, 2, 256, 128, C_DEVICE, C_DTYPE, enable_rel_pos=True)\n",
    "else:\n",
    "    model = ToyTransformer(tokenizer.get_vocab_size(), C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN, C_DEVICE, C_DTYPE, enable_rel_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 135418880\n",
      "ToyTransformer(\n",
      "  (sem_embed): Embedding(32768, 768)\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-11): 12 x DecoderLayer(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (attn_heads): ModuleList(\n",
      "          (0-11): 12 x AttentionHead(\n",
      "            (q_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (k_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (ln_mha): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_ffn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('Total parameters:', sum([t.numel() for t in model.parameters()]))\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:46.129204090Z",
     "start_time": "2023-10-22T02:35:46.088706697Z"
    }
   },
   "id": "f906aed5"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:46.954028646Z",
     "start_time": "2023-10-22T02:35:46.952534324Z"
    }
   },
   "id": "dca6004e9b65892c"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def dataset_collate(dataset: DatasetReader, batch_size: int,\n",
    "                    transform: Optional[Callable[[Dict[str, List[np.ndarray]]], Dict[str, torch.Tensor]]] = None,\n",
    "                    drop_last: bool = False):\n",
    "    cur_batch, cur_batch_size = {}, 0\n",
    "    for entry in iter(dataset):\n",
    "        for k, v in entry.items():\n",
    "            cur_batch.setdefault(k, [])\n",
    "            cur_batch[k].append(v)\n",
    "        cur_batch_size += 1\n",
    "        if cur_batch_size == batch_size:\n",
    "            yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)\n",
    "            cur_batch = {}\n",
    "            cur_batch_size = 0\n",
    "    if not drop_last and len(cur_batch) > 0:\n",
    "        yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:47.722014646Z",
     "start_time": "2023-10-22T02:35:47.716776581Z"
    }
   },
   "id": "674478c0ccb206af"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-10-22T02:35:56.651459696Z",
     "start_time": "2023-10-22T02:35:56.639942530Z"
    }
   },
   "outputs": [],
   "source": [
    "# type cast for handling uint16 datasets\n",
    "def train_transform(batch: Dict[str, List[np.ndarray]]):\n",
    "    return {k: torch.tensor(np.stack(v, dtype=np.int32 if v[0].dtype in [np.int16, np.uint16] else v[0].dtype)) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, num_epochs: int, batch_size: int, gradient_accumulation_steps: int,\n",
    "                max_lr: float, min_lr: float, warmup_ratio: float,\n",
    "                dataset: DatasetReader, show_progress=True):\n",
    "    total_samples = len(dataset)\n",
    "    epoch_steps = ceil(total_samples / batch_size)\n",
    "    assert epoch_steps >= gradient_accumulation_steps, f'per-epoch steps {epoch_steps} is less than gradient accumulation steps {gradient_accumulation_steps}'\n",
    "\n",
    "    schedule_steps = ceil(total_samples / batch_size / gradient_accumulation_steps)\n",
    "    total_steps = schedule_steps * num_epochs\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr,\n",
    "                                                    total_steps=total_steps,\n",
    "                                                    final_div_factor=max_lr / min_lr, pct_start=warmup_ratio)\n",
    "\n",
    "    bar = tqdm.tqdm(total=total_steps, disable=not show_progress)\n",
    "\n",
    "    model.train()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, batch in enumerate(dataset_collate(dataset, batch_size, train_transform)):\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            tokens = batch['token_ids'].to(model.device)\n",
    "            inputs = tokens[:, :-1]\n",
    "            labels = tokens[:, 1:]\n",
    "\n",
    "            positions = batch['position_ids'][:, :-1].to(model.device) if 'position_ids' in batch else None\n",
    "            attn_mask = batch['attn_mask'][:, :-1].to(model.device) if 'attn_mask' in batch else None\n",
    "            loss_mask = batch['loss_mask'][:, :-1].to(model.device) if 'loss_mask' in batch else None\n",
    "\n",
    "            logits, kv_state = model.forward(inputs, position_ids=positions, attn_mask=attn_mask)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=2).view(-1, logits.shape[-1])\n",
    "\n",
    "            loss = (-torch.log(probs[torch.arange(probs.shape[0]), labels.reshape(-1)]))\n",
    "            if loss_mask is not None:\n",
    "                loss = (loss * loss_mask.reshape(-1)).mean() / gradient_accumulation_steps\n",
    "            else:\n",
    "                loss = loss.mean() / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == epoch_steps:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                step_time_cost = time.time() - step_start_time\n",
    "                throughput = round(probs.shape[0] / step_time_cost / 1000, 2)\n",
    "\n",
    "                step_stat = {'Loss': f'{loss.item() * gradient_accumulation_steps:.3f}',\n",
    "                             'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                             'Throughput': f'{throughput} kt/s'}\n",
    "\n",
    "                if show_progress:\n",
    "                    bar.set_description(f'Epoch {epoch_num + 1}')\n",
    "                    bar.set_postfix(step_stat)\n",
    "                else:\n",
    "                    print(', '.join(f'{s[0]}:{s[1]}' for s in step_stat.items()))\n",
    "\n",
    "                scheduler.step()\n",
    "                bar.update(1)\n",
    "    bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "452e17ebe7dd49d59d406e8bdf76fc1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 9\u001B[0m\n\u001B[1;32m      4\u001B[0m     train_model(model, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, gradient_accumulation_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, max_lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m, min_lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m,\n\u001B[1;32m      5\u001B[0m                 warmup_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m,\n\u001B[1;32m      6\u001B[0m                 dataset\u001B[38;5;241m=\u001B[39mtrain_data,\n\u001B[1;32m      7\u001B[0m                 show_progress\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m----> 9\u001B[0m     \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_lr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_lr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                \u001B[49m\u001B[43mwarmup_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m                \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m                \u001B[49m\u001B[43mshow_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 41\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, num_epochs, batch_size, gradient_accumulation_steps, max_lr, min_lr, warmup_ratio, dataset, show_progress)\u001B[0m\n\u001B[1;32m     37\u001B[0m logits, kv_state \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mforward(inputs, position_ids\u001B[38;5;241m=\u001B[39mpositions, attn_mask\u001B[38;5;241m=\u001B[39mattn_mask)\n\u001B[1;32m     39\u001B[0m probs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, logits\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m---> 41\u001B[0m loss \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m-\u001B[39m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprobs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprobs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m loss_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     43\u001B[0m     loss \u001B[38;5;241m=\u001B[39m (loss \u001B[38;5;241m*\u001B[39m loss_mask\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mmean() \u001B[38;5;241m/\u001B[39m gradient_accumulation_steps\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "global_config['enable_flash_attn'] = True\n",
    "global_config['enable_torch_attn'] = False\n",
    "if C_DEBUG:\n",
    "    train_model(model, num_epochs=100, batch_size=2, gradient_accumulation_steps=4, max_lr=1e-3, min_lr=1e-4,\n",
    "                warmup_ratio=0.1,\n",
    "                dataset=train_data,\n",
    "                show_progress=True)\n",
    "else:\n",
    "    train_model(model, num_epochs=10, batch_size=12, gradient_accumulation_steps=8, max_lr=1e-3, min_lr=1e-4,\n",
    "                warmup_ratio=0.1,\n",
    "                dataset=train_data,\n",
    "                show_progress=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T02:38:07.228752436Z",
     "start_time": "2023-10-22T02:36:56.671703261Z"
    }
   },
   "id": "281e39ed7414acc9"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e800403549c98d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T02:38:13.659877193Z",
     "start_time": "2023-10-22T02:38:13.652815476Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, temperature, top_p, rep_penalty,\n",
    "             max_new_tokens=20, total_tokens=None,\n",
    "             end_tokens=None,\n",
    "             enable_kv_cache=True):\n",
    "    model.eval()\n",
    "\n",
    "    feed_tokens = tokenizer.encode(prompt)\n",
    "    all_tokens = feed_tokens.copy()\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(feed_tokens))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            position_ids = None if kv_cache is None else torch.tensor([[len(all_tokens) - 1]]).to(model.device)\n",
    "            logits, kv_cache = model.forward(\n",
    "                torch.tensor([feed_tokens if enable_kv_cache else all_tokens]).to(model.device),\n",
    "                position_ids=position_ids,\n",
    "                kv_cache=kv_cache)\n",
    "            logits = logits[0][-1].cpu()\n",
    "            if not enable_kv_cache:\n",
    "                kv_cache = None\n",
    "\n",
    "            # apply repetition penalty\n",
    "            logits_rep = torch.gather(logits, 0, torch.tensor(all_tokens))\n",
    "            logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "            logits.scatter_(0, torch.tensor(all_tokens), logits_rep)\n",
    "\n",
    "            # apply temperature\n",
    "            logits /= max(temperature, 1e-6)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # apply top-p\n",
    "            ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "            cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "            top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "            ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "            sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "            all_tokens.append(sampled_index)\n",
    "            feed_tokens = [sampled_index]\n",
    "\n",
    "            if end_tokens is not None and sampled_index in end_tokens:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<s>Q:\\n\\n\"Enable Wifi\" not showing\\n\\nI recently used sudo apt-get autoremove command and after that \"Enable Wifi\" option is not showing.\\n$ lspci -knn | grep Net -A3\\n02:00.0 Network controller [0280]: Realtek Semiconductor Co., Ltd. RTL8821AE 802.11ac PCIe Wireless Network Adapter [10ec:8821]\\n    Subsystem: Lenovo RTL8821AE 802.11ac PCIe Wireless Network Adapter [17aa:a814]\\n    Kernel modules: rtl8821ae, wl\\n\\n$ iwconfig is\\nlo no wireless extensions.\\n\\nenp1s0    no wireless extensions.\\n\\nFrom /var/apt/history.log:\\nStart-Date: 2017-11-06  01:15:38\\nCommandline: apt-get autoremove\\nRequested-By: praveen (1000)\\nRemove: linux-headers-4.10.0-28-generic:amd64 (4.10.0-28.32~16.04.2), librpmsign3:amd64 (4.12.0.1+dfsg1-3build3), linux-image-extra-4.10.0-33-generic:amd64 (4.10.0-33.37~16.04.1),\\nlibrpmbuild3:amd64 (4.12.0.1+dfsg1-3build3),\\nlinux-image-extra-4.10.0-35-generic:amd64 (4.10.0-35.39~16.04.1),\\ndebugedit:amd64 (4.12.0.1+dfsg1-3build3),\\nlinux-headers-4.10.0-33-generic:amd64 (4.10.0-33.37~16.04.1),\\nlinux-image-4.10.0-28-generic:amd64 (4.10.0-28.32~16.04.2),\\nlinux-headers-4.10.0-35-generic:amd64 (4.10.0-35.39~16.04.1),\\nrpm:amd64 (4.12.0.1+dfsg1-3build3),\\nlinux-image-4.10.0-33-generic:amd64 (4.10.0-33.37~16.04.1),\\nlinux-headers-4.10.0-28:amd64 (4.10.0-28.32~16.04.2),\\nlinux-headers-4.10.0-33:amd64 (4.10.0-33.37~16.04.1),\\nlinux-headers-4.10.0-35:amd64 (4.10.0-35.39~16.04.1),\\nlinux-image-4.10.0-35-generic:amd64 (4.10.0-35.39~16.04.1),\\nlinux-image-extra-4.10.0-28-generic:amd64 (4.10.0-28.32~16.04.2)\\nEnd-Date: 2017-11-06  01:17:19\\n\\nOutput of uname -a:\\nLinux praveen-Lenovo-ideapad-300-15ISK 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\\n\\n$ dpkg-query -Wf \\'${db:Status-Abbrev} ${Package;-40} ${Version}\\\\n\\' linux-{image,headers}-\\\\* | grep \\'^i\\'\\nii  linux-headers-4.10.0-37                  4.10.0-37.41~16.04.1\\nii  linux-headers-4.10.0-37-generic          4.10.0-37.41~16.04.1\\nii  linux-headers-4.10.0-38                  4.10.0-38.42~16.04.1\\nii  linux-headers-4.10.0-38-generic          4.10.0-38.42~16.04.1\\nii  linux-headers-4.4.0-98                   4.4.0-98.121\\nii  linux-headers-4.4.0-98-generic           4.4.0-98.121\\nii  linux-headers-generic                    4.4.0.98.103\\nii  linux-headers-generic-hwe-16.04          4.10.0.38.40\\nii  linux-image-4.10.0-37-generic            4.10.0-37.41~16.04.1\\nii  linux-image-4.10.0-38-generic            4.10.0-38.42~16.04.1\\nii  linux-image-extra-4.10.0-37-generic      4.10.0-37.41~16.04.1\\nii  linux-image-extra-4.10.0-38-generic      4.10.0-38.42~16.04.1\\nii  linux-image-generic-hwe-16.04            4.10.0.38.40\\n\\nA:\\n\\nThese commands will work for\\nrtl8192ce, rtl8192se, rtl8192de, rtl8188ee, rtl8192ee, rtl8723ae, rtl8723be, and rtl8821ae. If you have any other driver then please search and install.\\nsudo apt-get install linux-headers-generic build-essential git \\ngit clone http://github.com/lwfinger/rtlwifi_new.git\\ncd rtlwifi_new\\nmake\\nsudo make install\\n\\nAfter that if your system does not load appropriate kernel module,you can run below command in same directory(make sure to run this for your specific driver in my case it is rtl8821ae):\\nsudo modprobe rtl8821ae\\n\\n</s><s>For 8-bit music, this game has some that is both painful and awesome. For the purpose of this blog only the awesome music will be shared. Most of this is included with the songs that Link learns to play on his ocarina, a later staple of the series, which he learns from those about the island.\\n\\nThere are three, of these songs. One is taught by a girl who sings in the main village and is the person who found Link laying on the beach. Another is taught by a singing fish, which is my favorite of the three. The last is taught by some frogs and allows to resurrect things with the very tune.\\n\\nI am going to post those songs, here, in order of least amusing first. That meaning they are not in order you obtain them in game and the one which is annoying, at least its \"sung\" counterpart.\\n\\n1 comment\\n\\nDisclaimer\\n\\nAny audio, video or images on this site have no direct claim to them, unless stated otherwise, by the writer of this blog. The author or link to source object is always available if you need proof as such if it was not provided in the blog post that it was spoken about.</s><s>Endothelial modulation of resting and stimulated vascular tone in the pig capsular testicular artery.\\nWe'\n"
     ]
    }
   ],
   "source": [
    "print(repr(tokenizer.decode(next(iter(train_data))['token_ids'].tolist())))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T02:38:14.072650113Z",
     "start_time": "2023-10-22T02:38:14.069293520Z"
    }
   },
   "id": "f94c17f13926df4a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bc924753b5acfc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T02:38:18.329133833Z",
     "start_time": "2023-10-22T02:38:14.658211242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<s>_\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "3.672 sec(s)\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "global_config['enable_flash_attn'] = False\n",
    "global_config['enable_torch_attn'] = False\n",
    "result = generate(model, tokenizer, '<s>',\n",
    "                  temperature=1.0, top_p=0.1, rep_penalty=1.0,\n",
    "                  total_tokens=128,\n",
    "                  end_tokens=tokenizer.encode('</s>'),\n",
    "                  enable_kv_cache=True)\n",
    "print(repr(result))\n",
    "print(f'{time.time() - a:.3f} sec(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "230ab31cc1bf2689"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

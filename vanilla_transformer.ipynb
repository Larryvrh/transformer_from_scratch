{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import TRIETokenizerFast\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "import bisect\n",
    "from typing import *\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from flash_attn import flash_attn_func\n",
    "from dataloader import DatasetReader\n",
    "from math import ceil\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import bitsandbytes as bnb\n",
    "from xformers import ops as xops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33702a8e6613f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TRIETokenizerFast('llama_vocab_pruned_32k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb151752ec125fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network definition\n",
    "C_SEQ_LEN = 2048\n",
    "C_HIDDEN_SIZE = 768\n",
    "C_NUM_HEADS = 12\n",
    "C_NUM_LAYERS = 12\n",
    "\n",
    "C_DEVICE = torch.device('cuda')\n",
    "C_DTYPE = torch.bfloat16\n",
    "\n",
    "C_DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b78921dbff1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not C_DEBUG:\n",
    "    train_data = DatasetReader('datasets/tinystories_train_masked.bin')\n",
    "else:\n",
    "    train_data = DatasetReader('datasets/debug_data_masked.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d770a2be845a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train samples:', len(train_data))\n",
    "print('Sample length:', len(next(iter(train_data))['token_ids']))\n",
    "print('Train tokens:', len(train_data) * len(next(iter(train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8dbb96cab10318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample 1:', tokenizer.decode(next(iter(train_data))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4a7cb7e49d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = {\n",
    "    'enable_torch_attn': False,\n",
    "    'enable_flash_attn': False,\n",
    "    'enable_xformers_attn': False,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int = -1,\n",
    "    num_layers: int = -1,\n",
    "    num_heads: int = -1,\n",
    "    hidden_size: int = -1,\n",
    "    max_seq_len: int = -1,\n",
    "    root_model: 'ToyTransformer' = None\n",
    "    device: torch.device = torch.device('cpu')\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    enable_rel_pos: bool = False\n",
    "\n",
    "\n",
    "def expand_attn_mask(custom_attn_mask: torch.Tensor):\n",
    "    B, T = custom_attn_mask.shape\n",
    "    mask = custom_attn_mask.unsqueeze(1).repeat((1, T, 1))\n",
    "    seq_index_mask = (mask == custom_attn_mask[:, torch.arange(T)].view(B, T, 1))\n",
    "    return seq_index_mask & (torch.tril(mask) > 0)\n",
    "\n",
    "\n",
    "# expand attn mask to cu_seqlens for flash attn\n",
    "def expand_attn_mask_to_seq_lengths(attn_mask: torch.Tensor):\n",
    "    attn_mask = attn_mask.to('cpu')\n",
    "    seq_len = attn_mask.shape[0] * attn_mask.shape[1]\n",
    "    disjoint_point = torch.cat([torch.tensor([[True]] * attn_mask.shape[0]), attn_mask[:, 1:] != attn_mask[:, :-1]], dim=1)\n",
    "    return torch.cat([torch.nonzero(disjoint_point.view((-1,))), torch.tensor([[seq_len]])]).to(dtype=torch.int32)\n",
    "\n",
    "\n",
    "# naive RoPE implementation following https://arxiv.org/pdf/2104.09864.pdf\n",
    "def get_rope_cache_slow(seq_len: int, dim: int, theta: int, device: torch.device, dtype: torch.dtype):\n",
    "    assert dim % 2 == 0\n",
    "    freqs = theta ** (-2 * torch.arange(0, dim // 2, 1.) / dim)\n",
    "    freqs = torch.repeat_interleave(freqs, 2)\n",
    "    v1 = torch.cos(torch.arange(seq_len, dtype=torch.float).view((seq_len, 1)) * freqs)\n",
    "    v2 = torch.sin(torch.arange(seq_len, dtype=torch.float).view((seq_len, 1)) * freqs)\n",
    "    v2 = v2 * torch.tensor([1, -1] * (dim // 2))\n",
    "    indices = torch.tensor([j for i in range(0, dim, 2) for j in (i + 1, i)])\n",
    "    return v1.to(device, dtype=dtype), v2.to(device, dtype=dtype), indices.to(device)\n",
    "\n",
    "\n",
    "def apply_rope_slow(x, rope_cache, positions: Optional[torch.Tensor] = None):\n",
    "    v1, v2, indices = rope_cache\n",
    "    seq_len, dim = x.shape[1:]\n",
    "    if positions is None:\n",
    "        v1 = v1[:seq_len, :]\n",
    "        v2 = v2[:seq_len, :]\n",
    "    else:\n",
    "        v1 = v1[positions, torch.arange(dim)].view((-1, dim))\n",
    "        v2 = v2[positions, torch.arange(dim)].view((-1, dim))\n",
    "    applied_x = x * v1 + (x * v2)[:, :, indices]\n",
    "    return applied_x\n",
    "\n",
    "\n",
    "# Optimized RoPE implementation adapted from https://github.com/facebookresearch/llama/blob/main/llama/model.py\n",
    "def get_rope_cache_fast(seq_len: int, dim: int, theta: int, device: torch.device, dtype: torch.dtype):\n",
    "    freqs = (1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)))\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis.to(device)\n",
    "\n",
    "\n",
    "def apply_rope_fast(x, rope_cache, positions: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    if positions is None and x.shape[1] < rope_cache.shape[0]:\n",
    "        freqs_cis = rope_cache[:x.shape[1], :]\n",
    "    elif positions is not None:\n",
    "        freqs_cis = rope_cache[positions, :]\n",
    "    else:\n",
    "        freqs_cis = rope_cache\n",
    "    freqs_cis = freqs_cis.view([d if i == 1 or i == x_.ndim - 1 else 1 for i, d in enumerate(x_.shape)])\n",
    "\n",
    "    applied_x = torch.view_as_real(x_ * freqs_cis).flatten(2)\n",
    "    return applied_x.type_as(x)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.dtype = config.dtype\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size // config.num_heads, dtype=config.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        use_flash_attn = global_config['enable_flash_attn'] and kv_cache is None and attn_mask is None\n",
    "\n",
    "        if use_flash_attn:\n",
    "            apply_mask = None\n",
    "        elif kv_cache is None and attn_mask is not None:\n",
    "            apply_mask = expand_attn_mask(attn_mask)\n",
    "        elif kv_cache is None and attn_mask is None:\n",
    "            apply_mask = expand_attn_mask(torch.ones(x.shape[:2]))\n",
    "        elif kv_cache is not None:\n",
    "            apply_mask = torch.ones((B, T, T), dtype=torch.bool)\n",
    "        else:\n",
    "            apply_mask = None\n",
    "        # fill mask into attn bias\n",
    "        mask_zero = torch.tensor(0, dtype=self.dtype)\n",
    "        mask_val = torch.tensor(torch.finfo(self.dtype).min / 2, dtype=self.dtype)\n",
    "        if apply_mask is not None and not global_config['enable_torch_attn']:\n",
    "            apply_mask = torch.where(apply_mask, mask_zero, mask_val)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        if self.config.enable_rel_pos:\n",
    "            positions = torch.tensor([kv_cache[0].shape[1]]).to(q.device) if kv_cache is not None else None\n",
    "            q = apply_rope_fast(q, self.config.root_model.rope_cache, positions)\n",
    "            k = apply_rope_fast(k, self.config.root_model.rope_cache, positions)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k = torch.concat([kv_cache[0], k], dim=1)\n",
    "            v = torch.concat([kv_cache[1], v], dim=1)\n",
    "\n",
    "        if use_flash_attn:\n",
    "            q, k, v, = q.unsqueeze(2), k.unsqueeze(2), v.unsqueeze(2)\n",
    "            attn_result = flash_attn_func(q, k, v, causal=True)\n",
    "            q, k, v, attn_result = q.squeeze(2), k.squeeze(2), v.squeeze(2), attn_result.squeeze(2)\n",
    "        elif kv_cache is None and global_config['enable_xformers_attn']:\n",
    "            seq_len = q.shape[1]\n",
    "            pad_len = ceil(seq_len / 8) * 8\n",
    "            if pad_len > seq_len:\n",
    "                apply_mask = nn.functional.pad(apply_mask, (0, pad_len - seq_len, 0, pad_len - seq_len), value=mask_val.item())\n",
    "                q = nn.functional.pad(q, (0, 0, 0, pad_len - seq_len))\n",
    "                k = nn.functional.pad(k, (0, 0, 0, pad_len - seq_len))\n",
    "                v = nn.functional.pad(v, (0, 0, 0, pad_len - seq_len))\n",
    "            q, k, v, = q.unsqueeze(2), k.unsqueeze(2), v.unsqueeze(2)\n",
    "            attn_result = xops.memory_efficient_attention(q, k, v, apply_mask.unsqueeze(1).to(q.device))\n",
    "            q, k, v, attn_result = q.squeeze(2), k.squeeze(2), v.squeeze(2), attn_result.squeeze(2)\n",
    "            if pad_len > seq_len:\n",
    "                q, k, v = q[:, :seq_len, :], k[:, :seq_len, :], v[:, :seq_len, :]\n",
    "                attn_result = attn_result[:, :seq_len, :]\n",
    "        elif global_config['enable_torch_attn']:\n",
    "            with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n",
    "                attn_result = nn.functional.scaled_dot_product_attention(q, k, v,\n",
    "                                                                         attn_mask=apply_mask.to(q.device) if apply_mask is not None else None,\n",
    "                                                                         is_causal=True if apply_mask is None else False)\n",
    "        else:\n",
    "            attn_score = (q @ k.permute(0, 2, 1) / (self.hidden_size ** 0.5)) + apply_mask.to(q.device)\n",
    "            attn_result = torch.softmax(attn_score, dim=2) @ v\n",
    "\n",
    "        return attn_result, [k, v]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(config) for _ in range(config.num_heads)])\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, dtype=config.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        head_outputs = [head(x, attn_mask, kv_cache[idx] if kv_cache is not None else None) for idx, head in\n",
    "                        enumerate(self.attn_heads)]\n",
    "        return self.o_proj(torch.concat([o[0] for o in head_outputs], dim=2)), [o[1] for o in head_outputs]\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.hidden_size * 4, dtype=config.dtype)\n",
    "        self.down_proj = nn.Linear(config.hidden_size * 4, config.hidden_size, dtype=config.dtype)\n",
    "        self.ln_mha = nn.LayerNorm(config.hidden_size, dtype=config.dtype)\n",
    "        self.ln_ffn = nn.LayerNorm(config.hidden_size, dtype=config.dtype)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor],\n",
    "                kv_cache: Optional[List[torch.Tensor]]) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:\n",
    "        mha_output, new_kv_cache = self.mha(self.ln_mha(x), attn_mask, kv_cache)\n",
    "        mha_output = x + mha_output\n",
    "        ffn_output = self.down_proj(self.act(self.up_proj(self.ln_ffn(mha_output))))\n",
    "        return mha_output + ffn_output, new_kv_cache\n",
    "\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, hidden_size: int, max_seq_len: int,\n",
    "                 device: torch.device = torch.device('cpu'), dtype: torch.dtype = torch.float32,\n",
    "                 enable_rel_pos: bool = False):\n",
    "        super().__init__()\n",
    "        self.config = TransformerConfig(vocab_size, num_layers, num_heads, hidden_size, max_seq_len, self, device,\n",
    "                                        dtype, enable_rel_pos)\n",
    "\n",
    "        self.sem_embed = nn.Embedding(vocab_size, hidden_size, dtype=dtype)\n",
    "\n",
    "        if not self.config.enable_rel_pos:\n",
    "            self.pos_embed = nn.Embedding(max_seq_len, hidden_size, dtype=dtype)\n",
    "        else:\n",
    "            # self.rope_cache = get_rope_cache(max_seq_len, hidden_size // num_heads, 10000, device, dtype)\n",
    "            self.rope_cache = get_rope_cache_fast(max_seq_len, hidden_size // num_heads, 10000, device, dtype)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(num_layers)])\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, dtype=dtype)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, seq: torch.Tensor,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                kv_cache: Optional[List[torch.Tensor]] = None) -> Tuple[torch.Tensor, List[List[List[torch.Tensor]]]]:\n",
    "\n",
    "        if self.config.enable_rel_pos:\n",
    "            hidden = self.sem_embed(seq)\n",
    "        elif position_ids is not None:\n",
    "            hidden = self.sem_embed(seq) + self.pos_embed(position_ids)\n",
    "        else:\n",
    "            hidden = self.sem_embed(seq) + self.pos_embed(torch.arange(0, seq.shape[1], 1).to(self.device))\n",
    "\n",
    "        new_kv_cache = []\n",
    "        for idx, decoder in enumerate(self.decoder_layers):\n",
    "            hidden, layer_kv_cache = decoder(hidden, attn_mask, kv_cache[idx] if kv_cache is not None else None)\n",
    "            new_kv_cache.append(layer_kv_cache)\n",
    "\n",
    "        return self.lm_head(hidden), new_kv_cache\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124589733f9ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if C_DEBUG:\n",
    "    model = ToyTransformer(tokenizer.get_vocab_size(), 2, 2, 256, 1024, C_DEVICE, C_DTYPE, enable_rel_pos=True)\n",
    "else:\n",
    "    model = ToyTransformer(tokenizer.get_vocab_size(), C_NUM_LAYERS, C_NUM_HEADS, C_HIDDEN_SIZE, C_SEQ_LEN, C_DEVICE, C_DTYPE, enable_rel_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total parameters:', sum([t.numel() for t in model.parameters()]))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6004e9b65892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674478c0ccb206af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_collate(dataset: DatasetReader, batch_size: int,\n",
    "                    transform: Optional[Callable[[Dict[str, List[np.ndarray]]], Dict[str, torch.Tensor]]] = None,\n",
    "                    drop_last: bool = False):\n",
    "    cur_batch, cur_batch_size = {}, 0\n",
    "    for entry in iter(dataset):\n",
    "        for k, v in entry.items():\n",
    "            cur_batch.setdefault(k, [])\n",
    "            cur_batch[k].append(v)\n",
    "        cur_batch_size += 1\n",
    "        if cur_batch_size == batch_size:\n",
    "            yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)\n",
    "            cur_batch = {}\n",
    "            cur_batch_size = 0\n",
    "    if not drop_last and len(cur_batch) > 0:\n",
    "        yield {k: torch.tensor(np.stack(v)) for k, v in cur_batch.items()} if transform is None else transform(cur_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba00d2720d53e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# type cast for handling int16/uint16 columns\n",
    "def train_transform(batch: Dict[str, List[np.ndarray]]):\n",
    "    return {k: torch.tensor(np.stack(v, dtype=np.int32 if v[0].dtype in [np.int16, np.uint16] else v[0].dtype)) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, num_epochs: int, batch_size: int, gradient_accumulation_steps: int,\n",
    "                start_lr, max_lr: float, end_lr: float, warmup_ratio: float,\n",
    "                dataset: DatasetReader, gradient_clip: float = 1.0, show_progress=True,\n",
    "                ignore_attn_mask: bool = False, ignore_loss_mask: bool = False,\n",
    "                train_logs: Optional[List] = None):\n",
    "    if train_logs is None:\n",
    "        train_logs = []\n",
    "\n",
    "    total_samples = len(dataset)\n",
    "    epoch_steps = ceil(total_samples / batch_size)\n",
    "    assert epoch_steps >= gradient_accumulation_steps, f'per-epoch steps {epoch_steps} is less than gradient accumulation steps {gradient_accumulation_steps}'\n",
    "\n",
    "    schedule_steps = ceil(total_samples / batch_size / gradient_accumulation_steps)\n",
    "    total_steps = schedule_steps * num_epochs\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, div_factor=max_lr / start_lr,\n",
    "                                                    total_steps=total_steps,\n",
    "                                                    final_div_factor=start_lr / end_lr, pct_start=warmup_ratio)\n",
    "\n",
    "    bar = tqdm.tqdm(total=total_steps, disable=not show_progress)\n",
    "\n",
    "    model.train()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, batch in enumerate(dataset_collate(dataset, batch_size, train_transform)):\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            tokens = batch['token_ids'].to(model.device)\n",
    "            inputs = tokens[:, :-1]\n",
    "            labels = tokens[:, 1:]\n",
    "\n",
    "            positions = batch['position_ids'][:, :-1].to(model.device) if 'position_ids' in batch else None\n",
    "            attn_mask = batch['attn_mask'][:, :-1].to(model.device) if 'attn_mask' in batch and not ignore_attn_mask else None\n",
    "            loss_mask = batch['loss_mask'][:, :-1].to(model.device) if 'loss_mask' in batch and not ignore_loss_mask else None\n",
    "\n",
    "            logits, kv_state = model.forward(inputs, position_ids=positions, attn_mask=attn_mask)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=2).view(-1, logits.shape[-1])\n",
    "\n",
    "            loss = (-torch.log(probs[torch.arange(probs.shape[0]), labels.reshape(-1)]))\n",
    "            if loss_mask is not None:\n",
    "                loss = (loss * loss_mask.reshape(-1)).mean() / gradient_accumulation_steps\n",
    "            else:\n",
    "                loss = loss.mean() / gradient_accumulation_steps\n",
    "\n",
    "            # brutally clear nan, give up the whole batch\n",
    "            if torch.isnan(loss):\n",
    "                print(f'encountered nan loss at epoch {epoch_num + 1}, batch {batch_idx}')\n",
    "                # optimizer.zero_grad()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == epoch_steps:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                step_time_cost = time.time() - step_start_time\n",
    "                throughput = round(probs.shape[0] / step_time_cost / 1000, 2)\n",
    "\n",
    "                step_stat = {'Loss': f'{loss.item() * gradient_accumulation_steps:.3f}',\n",
    "                             'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                             'Throughput': f'{throughput} kt/s'}\n",
    "\n",
    "                if show_progress:\n",
    "                    bar.set_description(f'Epoch {epoch_num + 1}')\n",
    "                    bar.set_postfix(step_stat)\n",
    "                else:\n",
    "                    print(', '.join(f'{s[0]}:{s[1]}' for s in step_stat.items()))\n",
    "\n",
    "                scheduler.step()\n",
    "                bar.update(1)\n",
    "                train_logs.append((epoch_num, batch_idx, step_stat))\n",
    "\n",
    "    bar.close()\n",
    "\n",
    "    return train_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e39ed7414acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config['enable_flash_attn'] = False\n",
    "global_config['enable_torch_attn'] = True\n",
    "global_config['enable_xformers_attn'] = False\n",
    "last_train_logs = []\n",
    "if C_DEBUG:\n",
    "    train_model(model, num_epochs=1, batch_size=8, gradient_accumulation_steps=1, start_lr=1e-5, max_lr=1e-3, end_lr=1e-6,\n",
    "                warmup_ratio=0.1,\n",
    "                dataset=train_data, train_logs=last_train_logs,\n",
    "                ignore_attn_mask=True, ignore_loss_mask=True,\n",
    "                show_progress=True)\n",
    "else:\n",
    "    train_model(model, num_epochs=1, batch_size=6, gradient_accumulation_steps=2, start_lr=6e-6, max_lr=6e-4, end_lr=6e-5,\n",
    "                warmup_ratio=0.1,\n",
    "                dataset=train_data,\n",
    "                gradient_clip=0.5, train_logs=last_train_logs,\n",
    "                show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ca07cd31cd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].plot([float(l[2]['Loss']) for l in last_train_logs])\n",
    "axes[1].plot([float(l[2]['LR']) for l in last_train_logs])\n",
    "axes[2].plot([float(l[2]['Throughput'][:-5]) for l in last_train_logs])\n",
    "axes[0].set_title('Loss')\n",
    "axes[1].set_title('Learning Rate')\n",
    "axes[2].set_title('Throughput (kt/s)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.autoscale()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = 'checkpoints/train-231022-tinystories'\n",
    "# torch.save(model.state_dict(), f'{run_name}.pt')\n",
    "# torch.save(last_train_logs, f'{run_name}-logs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e800403549c98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, temperature, top_p, rep_penalty,\n",
    "             max_new_tokens=20, total_tokens=None,\n",
    "             end_tokens=None,\n",
    "             enable_kv_cache=True):\n",
    "    model.eval()\n",
    "\n",
    "    feed_tokens = tokenizer.encode(prompt)\n",
    "    all_tokens = feed_tokens.copy()\n",
    "    if total_tokens is not None:\n",
    "        max_new_tokens = max(0, total_tokens - len(feed_tokens))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens):\n",
    "            position_ids = None if kv_cache is None else torch.tensor([[len(all_tokens) - 1]]).to(model.device)\n",
    "            logits, kv_cache = model.forward(\n",
    "                torch.tensor([feed_tokens if enable_kv_cache else all_tokens]).to(model.device),\n",
    "                position_ids=position_ids,\n",
    "                kv_cache=kv_cache)\n",
    "            logits = logits[0][-1].cpu()\n",
    "            if not enable_kv_cache:\n",
    "                kv_cache = None\n",
    "\n",
    "            # apply repetition penalty\n",
    "            logits_rep = torch.gather(logits, 0, torch.tensor(all_tokens))\n",
    "            logits_rep = torch.where(logits_rep < 0, logits_rep * rep_penalty, logits_rep / rep_penalty)\n",
    "            logits.scatter_(0, torch.tensor(all_tokens), logits_rep)\n",
    "\n",
    "            # apply temperature\n",
    "            logits /= max(temperature, 1e-6)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # apply top-p\n",
    "            ordered_probs, ordered_indices = torch.sort(probs, descending=True)\n",
    "            cum_probs = torch.cumsum(ordered_probs, dim=0).tolist()\n",
    "            top_p_index = bisect.bisect_right(cum_probs, top_p) + 1\n",
    "            ordered_probs, ordered_indices = ordered_probs[:top_p_index], ordered_indices[:top_p_index]\n",
    "            sampled_index = ordered_indices[torch.multinomial(ordered_probs, num_samples=1).item()].item()\n",
    "\n",
    "            all_tokens.append(sampled_index)\n",
    "            feed_tokens = [sampled_index]\n",
    "\n",
    "            if end_tokens is not None and sampled_index in end_tokens:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c17f13926df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokenizer.decode(next(iter(train_data))['token_ids'].tolist()).split('</s>')[:3]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc924753b5acfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()\n",
    "global_config['enable_flash_attn'] = True\n",
    "global_config['enable_torch_attn'] = False\n",
    "global_config['enable_xformers_attn'] = False\n",
    "result = generate(model, tokenizer, '<s>Once upon a time there was a little boy named Ben',\n",
    "                  temperature=1.0, top_p=0.01, rep_penalty=1.1,\n",
    "                  total_tokens=350,\n",
    "                  end_tokens=tokenizer.encode('</s>'),\n",
    "                  enable_kv_cache=True)\n",
    "print(result)\n",
    "print(f'{time.time() - a:.3f} sec(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537635f6ba64280",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((1, 2047, 1, 128))\n",
    "nn.functional.pad(a, (0, 0, 0, 0, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ae8be2acbacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = next(iter(train_data))\n",
    "a, t = torch.tensor(e['attn_mask'].astype(np.int32)), e['token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214f52b8b0327d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fe032494a962c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
